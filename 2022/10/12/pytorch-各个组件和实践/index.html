<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>pytorch - 各个组件和实践 - 🐏&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="🐏&#039;s Blog"><meta name="msapplication-TileImage" content="/img/logo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="🐏&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本章介绍pytorch进行深度学习模型训练的各个组件和实践，层层搭建神经网络模型。 神经网络学习机制        数据预处理完成一项机器学习任务时的步骤，首先需要对数据进行预处理，其中重要的步骤包括数据格式的统一和必要的数据变换，同时划分训练集和测试集。 模型设计选择模型。 损失函数和优化方案设计设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失"><meta property="og:type" content="blog"><meta property="og:title" content="pytorch - 各个组件和实践"><meta property="og:url" content="http://example.com/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/"><meta property="og:site_name" content="🐏&#039;s Blog"><meta property="og:description" content="本章介绍pytorch进行深度学习模型训练的各个组件和实践，层层搭建神经网络模型。 神经网络学习机制        数据预处理完成一项机器学习任务时的步骤，首先需要对数据进行预处理，其中重要的步骤包括数据格式的统一和必要的数据变换，同时划分训练集和测试集。 模型设计选择模型。 损失函数和优化方案设计设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665563909211/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665566323623/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.1.png"><meta property="og:image" content="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.2.png"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580819001/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580768639/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://datawhalechina.github.io/thorough-pytorch/_images/3.5.2.png"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582006862/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582222120/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582284050/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582351089/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="og:image" content="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.1.png"><meta property="og:image" content="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.2.png"><meta property="og:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665568130565/D2B5CA33BD970F64A6301FA75AE2EB22"><meta property="article:published_time" content="2022-10-12T06:43:54.000Z"><meta property="article:modified_time" content="2022-10-12T15:33:48.906Z"><meta property="article:author" content="Yang"><meta property="article:tag" content="Pytorch"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665563909211/D2B5CA33BD970F64A6301FA75AE2EB22"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/"},"headline":"pytorch - 各个组件和实践","image":["https://datawhalechina.github.io/thorough-pytorch/_images/3.4.1.png","https://datawhalechina.github.io/thorough-pytorch/_images/3.4.2.png","https://datawhalechina.github.io/thorough-pytorch/_images/3.5.2.png","https://datawhalechina.github.io/thorough-pytorch/_images/3.6.1.png","https://datawhalechina.github.io/thorough-pytorch/_images/3.6.2.png"],"datePublished":"2022-10-12T06:43:54.000Z","dateModified":"2022-10-12T15:33:48.906Z","author":{"@type":"Person","name":"Yang"},"publisher":{"@type":"Organization","name":"🐏's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"本章介绍pytorch进行深度学习模型训练的各个组件和实践，层层搭建神经网络模型。 神经网络学习机制        数据预处理完成一项机器学习任务时的步骤，首先需要对数据进行预处理，其中重要的步骤包括数据格式的统一和必要的数据变换，同时划分训练集和测试集。 模型设计选择模型。 损失函数和优化方案设计设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失"}</script><link rel="canonical" href="http://example.com/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/"><link rel="icon" href="/img/logo.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 6.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="🐏&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Yang-Emily/Yang-Emily.github.io"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-12T06:43:54.000Z" title="2022/10/12 下午2:43:54">2022-10-12</time>发表</span><span class="level-item"><time dateTime="2022-10-12T15:33:48.906Z" title="2022/10/12 下午11:33:48">2022-10-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">1 小时读完 (大约11579个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">pytorch - 各个组件和实践</h1><div class="content"><p>本章介绍pytorch进行深度学习模型训练的各个组件和实践，层层搭建神经网络模型。</p>
<h1 id="神经网络学习机制"><a href="#神经网络学习机制" class="headerlink" title="神经网络学习机制"></a>神经网络学习机制</h1><center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665563909211/D2B5CA33BD970F64A6301FA75AE2EB22" width="250">  
</center>

<ul>
<li>数据预处理<br>完成一项机器学习任务时的步骤，首先需要对数据进行预处理，其中重要的步骤包括数据格式的统一和必要的数据变换，同时划分训练集和测试集。</li>
<li>模型设计<br>选择模型。</li>
<li>损失函数和优化方案设计<br>设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失函数和优化器）。</li>
<li>前向传播<br>用模型去拟合训练集数据，</li>
<li>反向传播</li>
<li>更新参数</li>
<li>模型表现<br>在验证集&#x2F;测试集上计算模型表现。</li>
</ul>
<h1 id="深度学习在实现上的特殊性"><a href="#深度学习在实现上的特殊性" class="headerlink" title="深度学习在实现上的特殊性"></a>深度学习在实现上的特殊性</h1><ul>
<li>样本量大，需要分批加载<br>由于深度学习所需的样本量很大，一次加载全部数据运行可能会超出内存容量而无法实现；同时还有批（batch）训练等提高模型表现的策略，需要每次训练读取固定数量的样本送入模型中训练。</li>
<li>逐层、模块化搭建网络（卷积层、全连接层、LSTM等）<br>深度神经网络往往需要“逐层”搭建，或者预先定义好可以实现特定功能的模块，再把这些模块组装起来。</li>
<li>多样化的损失函数和优化器设计<br>由于模型设定的灵活性，因此损失函数和优化器要能够保证反向传播能够在用户自行定义的模型结构上实现</li>
<li>GPU的使用<br>需要把模型和数据“放到”GPU上去做运算，同时还需要保证损失函数和优化器能够在GPU上工作。如果使用多张GPU进行训练，还需要考虑模型和数据分配、整合的问题。</li>
<li>各个模块之间的配合<br>深度学习中训练和验证过程最大的特点在于读入数据是按批的，每次读入一个批次的数据，放入GPU中训练，然后将损失函数反向传播回网络最前面的层，同时使用优化器调整网络参数。这里会涉及到各个模块配合的问题。训练&#x2F;验证后还需要根据设定好的指标计算模型表现。</li>
</ul>
<h1 id="pytorch深度学习模块"><a href="#pytorch深度学习模块" class="headerlink" title="pytorch深度学习模块"></a>pytorch深度学习模块</h1><p>将PyTorch完成深度学习的步骤拆解为几个主要模块，实际使用根据自身需求修改对应模块即可，深度学习-&gt;搭积木。</p>
<h2 id="一、-基本配置"><a href="#一、-基本配置" class="headerlink" title="一、 基本配置"></a>一、 基本配置</h2><p>首先导入必要的包</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optimizer</span><br></pre></td></tr></table></figure>
<p>配置训练环境和超参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置GPU，这里有两种方式</span></span><br><span class="line"><span class="comment">## 方案一：使用os.environ，后续用.cuda()</span></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0&#x27;</span></span><br><span class="line"><span class="comment"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:1&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置其他超参数，如batch_size, num_workers, learning rate, 以及总的epochs</span></span><br><span class="line">batch_size = <span class="number">256</span> <span class="comment"># 每次训练读入的数据量</span></span><br><span class="line">num_workers = <span class="number">4</span>   <span class="comment"># 有多少线程来读入数据，对于Windows用户，这里应设置为0，否则会出现多线程错误</span></span><br><span class="line">lr = <span class="number">1e-4</span> <span class="comment"># 参数更新的步长</span></span><br><span class="line">epochs = <span class="number">20</span> <span class="comment"># 训练多少轮</span></span><br></pre></td></tr></table></figure>
<h2 id="二、-数据读入"><a href="#二、-数据读入" class="headerlink" title="二、 数据读入"></a>二、 数据读入</h2><p>有两种方式：</p>
<ul>
<li>下载并使用PyTorch提供的内置数据集<br>只适用于常见的数据集，如MNIST，CIFAR10等，PyTorch官方提供了数据下载。这种方式往往适用于快速测试方法（比如测试下某个idea在MNIST数据集上是否有效）<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先设置数据变换</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">28</span> </span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),   <span class="comment"># 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要</span></span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">train_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br><span class="line">test_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br></pre></td></tr></table></figure></li>
<li>从网站下载以csv格式存储的数据，读入并转成预期的格式<br>需要自己构建Dataset，这对于PyTorch应用于自己的工作中十分重要,同时，还需要对数据进行必要的变换，比如说需要将图片统一为一致的大小，以便后续能够输入网络训练；需要将数据格式转为Tensor类，等等。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式二：读入csv格式的数据，自行构建Dataset类</span></span><br><span class="line"><span class="comment"># csv数据下载链接：https://www.kaggle.com/zalando-research/fashionmnist</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FMDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.df = df</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.images = df.iloc[:,<span class="number">1</span>:].values.astype(np.uint8)</span><br><span class="line">        self.labels = df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.images)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        image = self.images[idx].reshape(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 1是单一通道</span></span><br><span class="line">        label = <span class="built_in">int</span>(self.labels[idx])</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image = torch.tensor(image/<span class="number">255.</span>, dtype=torch.<span class="built_in">float</span>) <span class="comment"># image/255 把数值归一化</span></span><br><span class="line">        label = torch.tensor(label, dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">train_df = pd.read_csv(<span class="string">&quot;./fashion-mnist_train.csv&quot;</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">&quot;./fashion-mnist_test.csv&quot;</span>)</span><br><span class="line">train_data = FMDataset(train_df, data_transform)</span><br><span class="line">test_data = FMDataset(test_df, data_transform)</span><br></pre></td></tr></table></figure>
PyTorch数据读入是通过Dataset+DataLoader的方式完成的，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：</li>
<li>_<em>init</em>_: 用于向类中传入外部参数，同时定义样本集</li>
<li>_<em>getitem</em>_: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练&#x2F;验证所需的数据</li>
<li>_<em>len</em>_: 用于返回数据集的样本数<br>在构建训练和测试数据集完成后，需要定义DataLoader类，以便在训练和测试时加载数据:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
读入后，我们可以做一些数据可视化操作，主要是验证我们读入的数据是否正确:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image, label = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="built_in">print</span>(image.shape, label.shape)</span><br><span class="line">plt.imshow(image[<span class="number">0</span>][<span class="number">0</span>], cmap=<span class="string">&quot;gray&quot;</span>)</span><br></pre></td></tr></table></figure>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665566323623/D2B5CA33BD970F64A6301FA75AE2EB22" width="250">  
</center></li>
</ul>
<h2 id="三、-模型构建"><a href="#三、-模型构建" class="headerlink" title="三、 模型构建"></a>三、 模型构建</h2><p>我们这里的任务是对10个类别的“时装”图像进行分类，FashionMNIST数据集中包含已经预先划分好的训练集和测试集，其中训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为32*32pixel，分属10个类别。由于任务较为简单，这里我们<strong>手搭一个CNN</strong>，而不考虑当下各种模型的复杂结构，模型构建完成后，将模型放到GPU上用于训练。<br>Module 类是nn模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__() </span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># x = nn.functional.normalize(x)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">model = model.cuda()</span><br><span class="line"><span class="comment"># model = nn.DataParallel(model).cuda()   # 多卡训练时的写法，之后的课程中会进一步讲解</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.nn.Conv2d(
  in_channels, 
  out_channels, 
  kernel_size, 
  stride=1, 
  padding=0, 
  dilation=1, 
  groups=1, 
  bias=True, 
  padding_mode=&#39;zeros&#39;, 
  device=None, 
  dtype=None)

torch.nn.MaxPool2d(
  kernel_size, 
  stride=None, 
  padding=0, 
  dilation=1, 
  return_indices=False, 
  ceil_mode=False)
</code></pre>
<p>$d_{out} &#x3D;(d_{in}−dilation∗(kernelsize−1)−1+2∗padding)&#x2F;stride+1)$<br><strong>下面再举一个其他模型MLP：</strong><br>继承Module类构造多层感知机,这里定义的MLP类重载了Module类的init函数和forward函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 backward 函数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)   </span><br></pre></td></tr></table></figure>
<p>我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 call 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">2</span>,<span class="number">784</span>)</span><br><span class="line">net = MLP()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p>注意，这里并没有将 Module 类命名为 Layer (层)或者 Model (模型)之类的名字，这是因为该类是一个可供⾃由组建的部件。它的子类既可以是⼀个层(如PyTorch提供的 Linear 类)，⼜可以是一个模型(如这里定义的 MLP 类)，或者是模型的⼀个部分。<br><strong>下面介绍一些神经网络中常见的层：</strong><br>深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层、卷积层、池化层与循环层等等。虽然PyTorch提供了⼤量常用的层，但有时候我们依然希望⾃定义层。</p>
<ol>
<li>不含模型参数的层<br>下⾯构造的 <strong>MyLayer</strong> 类通过继承 Module 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 forward 函数里。这个层里不含模型参数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyLayer, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()  </span><br><span class="line"><span class="comment"># 测试，实例化该层，然后做前向计算        </span></span><br><span class="line">layer = MyLayer()</span><br><span class="line">layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br></pre></td></tr></table></figure></li>
<li>含模型参数的层<br>自定义含模型参数的自定义层，其中的模型参数可以通过训练学出。<strong>Parameter</strong> 类其实是 Tensor 的子类，如果一个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ <strong>ParameterList</strong> 和 <strong>ParameterDict</strong> 分别定义参数的列表和字典。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyListDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i]) <span class="comment"># torch.mm矩阵相乘，两个二维张量相乘</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = MyListDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line">net = MyDictDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。</li>
<li>二维卷积层<br>二维卷积层将输入和<strong>卷积核</strong>做互相关运算，并加上一个<strong>标量偏差</strong>来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积运算（二维互相关）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span> </span><br><span class="line">    h, w = K.shape</span><br><span class="line">    X, K = X.<span class="built_in">float</span>(), K.<span class="built_in">float</span>()</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。给定一个高和宽为8的输入，我们发现输出的高和宽也是8。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span></span><br><span class="line">    <span class="comment"># (1, 1)代表批量大小和通道数</span></span><br><span class="line">    X = X.view((<span class="number">1</span>, <span class="number">1</span>) + X.shape) <span class="comment"># 加上两个维度</span></span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.view(Y.shape[<span class="number">2</span>:]) <span class="comment"># 排除不关心的前两维:批量和通道</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 ( 为大于1的整数)。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1，-5+2*2=-3+2*1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></li>
<li>池化层<br>池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。下面把池化层的前向计算实现在pool2d函数里。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
我们可以使用torch.nn包来构建神经网络。我们已经介绍了autograd包，nn包则依赖于autograd包来定义模型并对它们求导。一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。</li>
<li>LeNet模型示例<center> 
<img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.1.png" width="480">  
</center>
这是一个简单的前馈神经网络 (feed-forward network）（LeNet）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。一个神经网络的典型训练过程如下：
定义包含一些可学习参数(或者叫权重）的神经网络
在输入数据集上迭代
通过网络处理输入
计算 loss (输出和正确答案的距离）
将梯度反向传播给网络的参数
更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入图像channel：1；输出channel：6；5x5卷积核</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 2x2 Max pooling</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是方阵,则可以只使用一个数字进行定义</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 除去批处理维度的其他所有维度</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<pre><code> Net(
   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
   (fc1): Linear(in_features=400, out_features=120, bias=True)
   (fc2): Linear(in_features=120, out_features=84, bias=True)
   (fc3): Linear(in_features=84, out_features=10, bias=True)
 )
</code></pre>
</li>
</ol>
<p>我们只需要定义 forward 函数，backward函数会在使用autograd时自动定义，backward函数用来计算导数。我们可以在 forward 函数中使用任何针对张量的操作和计算。一个模型的可学习参数可以通过net.parameters()返回。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1的权重</span></span><br></pre></td></tr></table></figure>
<p>尝试随机输入</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="comment"># 清零所有参数的梯度缓存，然后进行随机梯度的反向传播：</span></span><br><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>torch.nn只支持小批量处理 (mini-batches）。整个 torch.nn 包只支持小批量样本的输入，不支持单个样本的输入。比如，nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width 如果是一个单独的样本，只需要使用input.unsqueeze(0) 来添加一个“假的”批大小维度。<br>torch.Tensor - 一个多维数组，支持诸如backward()等的自动求导操作，同时也保存了张量的梯度。<br>nn.Module - 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。<br>nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。<br>autograd.Function - 实现了自动求导前向和反向传播的定义，每个Tensor至少创建一个Function节点，该节点连接到创建Tensor的函数并对其历史进行编码。</p>
<ol>
<li>AlexNet模型示例<center> 
<img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.2.png" width="480">  
</center></li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">net = AlexNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<pre><code>AlexNet(
  (conv): Sequential(
    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU()
    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU()
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc): Sequential(
    (0): Linear(in_features=6400, out_features=4096, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
</code></pre>
<h2 id="四、-模型初始化"><a href="#四、-模型初始化" class="headerlink" title="四、 模型初始化"></a>四、 模型初始化</h2><ol>
<li>torch.nn.init使用<br>通常使用isinstance来进行判断模块<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">linear = nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">isinstance</span>(conv,nn.Conv2d)</span><br><span class="line"><span class="built_in">isinstance</span>(linear,nn.Conv2d)</span><br></pre></td></tr></table></figure>
查看不同初始化参数<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看随机初始化的conv参数</span></span><br><span class="line">conv.weight.data</span><br><span class="line"><span class="comment"># 查看linear的参数</span></span><br><span class="line">linear.weight.data</span><br></pre></td></tr></table></figure>
对不同类型层进行初始化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对conv进行kaiming初始化</span></span><br><span class="line">torch.nn.init.kaiming_normal_(conv.weight.data)</span><br><span class="line">conv.weight.data</span><br><span class="line"><span class="comment"># 对linear进行常数初始化</span></span><br><span class="line">torch.nn.init.constant_(linear.weight.data,<span class="number">0.3</span>)</span><br><span class="line">linear.weight.data</span><br></pre></td></tr></table></figure></li>
<li>初始化函数的封装<br>人们常常将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">		<span class="comment"># 判断是否属于Conv2d</span></span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">			torch.nn.init.xavier_normal_(m.weight.data)</span><br><span class="line">			<span class="comment"># 判断是否有偏置</span></span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				torch.nn.init.constant_(m.bias.data,<span class="number">0.3</span>)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">			torch.nn.init.normal_(m.weight.data, <span class="number">0.1</span>)</span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				torch.nn.init.zeros_(m.bias.data)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">			m.weight.data.fill_(<span class="number">1</span>) 		 </span><br><span class="line">			m.bias.data.zeros_()	</span><br></pre></td></tr></table></figure>
这段代码流程是遍历当前模型的每一层，然后判断各层属于什么类型，然后根据不同类型层，设定不同的权值初始化方法。我们可以通过下面的例程进行一个简短的演示：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型的定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)</span><br><span class="line"></span><br><span class="line">mlp = MLP()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(mlp.parameters()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------初始化-------&quot;</span>)</span><br><span class="line"></span><br><span class="line">initialize_weights(mlp)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(mlp.parameters()))</span><br></pre></td></tr></table></figure>

<pre><code> [Parameter containing:
 tensor([[[[ 0.2103, -0.1679,  0.1757],
           [-0.0647, -0.0136, -0.0410],
           [ 0.1371, -0.1738, -0.0850]]]], requires_grad=True), Parameter containing:
 tensor([0.2507], requires_grad=True), Parameter containing:
 tensor([[ 0.2790, -0.1247,  0.2762,  0.1149, -0.2121, -0.3022, -0.1859,  0.2983,
         -0.0757, -0.2868]], requires_grad=True), Parameter containing:
 tensor([-0.0905], requires_grad=True)]
 &quot;-------初始化-------&quot;
 [Parameter containing:
 tensor([[[[-0.3196, -0.0204, -0.5784],
           [ 0.2660,  0.2242, -0.4198],
           [-0.0952,  0.6033, -0.8108]]]], requires_grad=True),
 Parameter containing:
 tensor([0.3000], requires_grad=True),
 Parameter containing:
 tensor([[ 0.7542,  0.5796,  2.2963, -0.1814, -0.9627,  1.9044,  0.4763,  1.2077,
           0.8583,  1.9494]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]
</code></pre>
</li>
</ol>
<h2 id="五、-损失函数"><a href="#五、-损失函数" class="headerlink" title="五、 损失函数"></a>五、 损失函数</h2><p>这里使用torch.nn模块自带的CrossEntropy损失，PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss，这里需要确保label是从0开始的，同时模型不加softmax层（使用logits计算）,这也说明了PyTorch训练中各个部分不是独立的，需要通盘考虑。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="1-二分类交叉熵损失函数"><a href="#1-二分类交叉熵损失函数" class="headerlink" title="1. 二分类交叉熵损失函数"></a>1. 二分类交叉熵损失函数</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BCELoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。<br><strong>主要参数</strong>：<br><code>weight</code>:每个类别的loss设置权值<br><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。<br><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。<br>计算公式如下：</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580819001/D2B5CA33BD970F64A6301FA75AE2EB22" width="290">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;BCELoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>BCELoss损失函数的计算结果为 tensor(0.5732, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
</code></pre>
<h3 id="2-交叉熵损失函数"><a href="#2-交叉熵损失函数" class="headerlink" title="2. 交叉熵损失函数"></a>2. 交叉熵损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=-<span class="number">100</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算交叉熵函数<br><strong>主要参数</strong>：<br><code>weight</code>:每个类别的loss设置权值。<br><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。<br><code>ignore_index</code>:忽略某个类的损失函数。<br><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。<br>计算公式如下：<br>$<br>\operatorname{loss}(x, \text { class })&#x3D;-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)&#x3D;-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.0115, grad_fn=&lt;NllLossBackward&gt;)
</code></pre>
<h3 id="3-L1损失函数"><a href="#3-L1损失函数" class="headerlink" title="3. L1损失函数"></a>3. L1损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.L1Loss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算输出<code>y</code>和真实标签<code>target</code>之间的差值的绝对值。<code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。mean：加权平均，返回标量。如果选择<code>none</code>，那么返回的结果是和输入元素相同尺寸的。默认计算方式是求平均。<br><strong>计算公式如下：</strong><br>$<br>L_{n} &#x3D; |x_{n}-y_{n}|<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.L1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;L1损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>L1损失函数的计算结果为 tensor(1.5729, grad_fn=&lt;L1LossBackward&gt;)
</code></pre>
<h3 id="4-MSE损失函数"><a href="#4-MSE损失函数" class="headerlink" title="4. MSE损失函数"></a>4. MSE损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MSELoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算输出<code>y</code>和真实标签<code>target</code>之差的平方。</p>
<p>和<code>L1Loss</code>一样，<code>MSELoss</code>损失函数中，<code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。默认计算方式是求平均。</p>
<p><strong>计算公式如下：</strong></p>
<p>$<br>l_{n}&#x3D;\left(x_{n}-y_{n}\right)^{2}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MSE损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MSE损失函数的计算结果为 tensor(1.6968, grad_fn=&lt;MseLossBackward&gt;)
</code></pre>
<h3 id="5-平滑L1-Smooth-L1-损失函数"><a href="#5-平滑L1-Smooth-L1-损失函数" class="headerlink" title="5. 平滑L1 (Smooth L1)损失函数"></a>5. 平滑L1 (Smooth L1)损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.SmoothL1Loss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, beta=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> L1的平滑输出，其功能是减轻离群点带来的影响</p>
<p><code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。默认计算方式是求平均。</p>
<p><strong>提醒：</strong> 之后的损失函数中，关于<code>reduction</code> 这个参数依旧会存在。所以，之后就不再单独说明。</p>
<p><strong>计算公式如下：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\frac{1}{n} \sum_{i&#x3D;1}^{n} z_{i}<br>$<br>其中，</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580768639/D2B5CA33BD970F64A6301FA75AE2EB22" width="290">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.SmoothL1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SmoothL1Loss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>SmoothL1Loss损失函数的计算结果为 tensor(0.7808, grad_fn=&lt;SmoothL1LossBackward&gt;)
</code></pre>
<p><strong>平滑L1与L1的对比</strong></p>
<p>这里我们通过可视化两种损失函数曲线来对比平滑L1和L1两种损失函数的区别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.linspace(-<span class="number">10</span>, <span class="number">10</span>, steps=<span class="number">5000</span>)</span><br><span class="line">target = torch.zeros_like(inputs)</span><br><span class="line"></span><br><span class="line">loss_f_smooth = nn.SmoothL1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_smooth = loss_f_smooth(inputs, target)</span><br><span class="line">loss_f_l1 = nn.L1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_l1 = loss_f_l1(inputs,target)</span><br><span class="line"></span><br><span class="line">plt.plot(inputs.numpy(), loss_smooth.numpy(), label=<span class="string">&#x27;Smooth L1 Loss&#x27;</span>)</span><br><span class="line">plt.plot(inputs.numpy(), loss_l1, label=<span class="string">&#x27;L1 loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x_i - y_i&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss value&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.5.2.png"><br>可以看出，对于<code>smoothL1</code>来说，在 0 这个尖端处，过渡更为平滑。</p>
<h3 id="6-目标泊松分布的负对数似然损失"><a href="#6-目标泊松分布的负对数似然损失" class="headerlink" title="6. 目标泊松分布的负对数似然损失"></a>6. 目标泊松分布的负对数似然损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.PoissonNLLLoss(log_input=<span class="literal">True</span>, full=<span class="literal">False</span>, size_average=<span class="literal">None</span>, eps=<span class="number">1e-08</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 泊松分布的负对数似然损失函数<br><strong>主要参数：</strong><br><code>log_input</code>：输入是否为对数形式，决定计算公式。<br><code>full</code>：计算所有 loss，默认为 False。<br><code>eps</code>：修正项，避免 input 为 0 时，log(input) 为 nan 的情况。<br><strong>数学公式：</strong></p>
<ul>
<li>当参数<code>log_input=True</code>：<br>$<br>\operatorname{loss}\left(x_{n}, y_{n}\right)&#x3D;e^{x_{n}}-x_{n} \cdot y_{n}<br>$</li>
<li>当参数<code>log_input=False</code>：<br>  $<br>  \operatorname{loss}\left(x_{n}, y_{n}\right)&#x3D;x_{n}-y_{n} \cdot \log \left(x_{n}+\text { eps }\right)<br>  $</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.PoissonNLLLoss()</span><br><span class="line">log_input = torch.randn(<span class="number">5</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">output = loss(log_input, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;PoissonNLLLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>PoissonNLLLoss损失函数的计算结果为 tensor(0.7358, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="7-KL散度"><a href="#7-KL散度" class="headerlink" title="7. KL散度"></a>7. KL散度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.KLDivLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, log_target=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 <code>none</code>&#x2F;<code>sum</code>&#x2F;<code>mean</code>&#x2F;<code>batchmean</code>。</p>
<pre><code>none：逐个元素计算。
sum：所有元素求和，返回标量。
mean：加权平均，返回标量。
batchmean：batchsize 维度求平均值。
</code></pre>
<p><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582006862/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.05</span>], [<span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.2</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">loss = nn.KLDivLoss()</span><br><span class="line">output = loss(inputs,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;KLDivLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>KLDivLoss损失函数的计算结果为 tensor(-0.3335)
</code></pre>
<h3 id="8-MarginRankingLoss"><a href="#8-MarginRankingLoss" class="headerlink" title="8. MarginRankingLoss"></a>8. MarginRankingLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MarginRankingLoss(margin=<span class="number">0.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算两个向量之间的相似度，用于排序任务。该方法用于计算两组数据之间的差异。<br><strong>主要参数:</strong><br><code>margin</code>：边界值，$x_{1}$ 与$x_{2}$ 之间的差异值。<br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x 1, x 2, y)&#x3D;\max (0,-y *(x 1-x 2)+\operatorname{margin})<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MarginRankingLoss()</span><br><span class="line">input1 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">input2 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>).sign()</span><br><span class="line">output = loss(input1, input2, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MarginRankingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MarginRankingLoss损失函数的计算结果为 tensor(0.7740, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="9-多标签边界损失函数"><a href="#9-多标签边界损失函数" class="headerlink" title="9. 多标签边界损失函数"></a>9. 多标签边界损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiLabelMarginLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对于多标签分类问题计算损失函数。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i j} \frac{\max (0,1-x[y[j]]-x[i])}{x \cdot \operatorname{size}(0)}<br>$<br>$<br>\begin{array}{l}<br>\text { 其中, } i&#x3D;0, \ldots, x \cdot \operatorname{size}(0), j&#x3D;0, \ldots, y \cdot \operatorname{size}(0), \text { 对于所有的 } i \text { 和 } j \text {, 都有 } y[j] \geq 0 \text { 并且 }\<br>i \neq y[j]<br>\end{array}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MultiLabelMarginLoss()</span><br><span class="line">x = torch.FloatTensor([[<span class="number">0.9</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]])</span><br><span class="line"><span class="comment"># for target y, only consider labels 3 and 0, not after label -1</span></span><br><span class="line">y = torch.LongTensor([[<span class="number">3</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]])<span class="comment"># 真实的分类是，第3类和第0类</span></span><br><span class="line">output = loss(x, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MultiLabelMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MultiLabelMarginLoss损失函数的计算结果为 tensor(0.4500)
</code></pre>
<h3 id="10-二分类损失函数"><a href="#10-二分类损失函数" class="headerlink" title="10. 二分类损失函数"></a>10. 二分类损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.SoftMarginLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)torch.nn.(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算二分类的 logistic 损失。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i} \frac{\log (1+\exp (-y[i] \cdot x[i]))}{x \cdot \operatorname{nelement}()}<br>$<br>$<br><br>\text { 其中, } x . \text { nelement() 为输入 } x \text { 中的样本个数。注意这里 } y \text { 也有 } 1 \text { 和 }-1 \text { 两种模式。 }<br><br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]])  <span class="comment"># 两个样本，两个神经元</span></span><br><span class="line">target = torch.tensor([[-<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签</span></span><br><span class="line"></span><br><span class="line">loss_f = nn.SoftMarginLoss()</span><br><span class="line">output = loss_f(inputs, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SoftMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>SoftMarginLoss损失函数的计算结果为 tensor(0.6764)
</code></pre>
<h3 id="11-多分类的折页损失"><a href="#11-多分类的折页损失" class="headerlink" title="11. 多分类的折页损失"></a>11. 多分类的折页损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiMarginLoss(p=<span class="number">1</span>, margin=<span class="number">1.0</span>, weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算多分类的折页损失<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>p：</code>可选 1 或 2。<br><code>weight</code>：各类别的 loss 设置权值。<br><code>margin</code>：边界值<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\frac{\sum_{i} \max (0, \operatorname{margin}-x[y]+x[i])^{p}}{x \cdot \operatorname{size}(0)}<br>$<br>$<br>\begin{array}{l}<br>\text { 其中, } x \in{0, \ldots, x \cdot \operatorname{size}(0)-1}, y \in{0, \ldots, y \cdot \operatorname{size}(0)-1} \text {, 并且对于所有的 } i \text { 和 } j \text {, }\<br>\text { 都有 } 0 \leq y[j] \leq x \cdot \operatorname{size}(0)-1, \text { 以及 } i \neq y[j] \text { 。 }<br>\end{array}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]]) </span><br><span class="line">target = torch.tensor([<span class="number">0</span>, <span class="number">1</span>], dtype=torch.long) </span><br><span class="line"></span><br><span class="line">loss_f = nn.MultiMarginLoss()</span><br><span class="line">output = loss_f(inputs, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MultiMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MultiMarginLoss损失函数的计算结果为 tensor(0.6000)
</code></pre>
<h3 id="12-三元组损失"><a href="#12-三元组损失" class="headerlink" title="12. 三元组损失"></a>12. 三元组损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">2.0</span>, eps=<span class="number">1e-06</span>, swap=<span class="literal">False</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算三元组损失。<br><strong>三元组:</strong> 这是一种数据的存储或者使用格式。&lt;实体1，关系，实体2&gt;。在项目中，也可以表示为&lt; <code>anchor</code>, <code>positive examples</code> , <code>negative examples</code>&gt;<br>在这个损失函数中，我们希望去<code>anchor</code>的距离更接近<code>positive examples</code>，而远离<code>negative examples </code><br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>p：</code>可选 1 或 2。<br><code>margin</code>：边界值<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582222120/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">triplet_loss = nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">2</span>)</span><br><span class="line">anchor = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">positive = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">negative = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">output = triplet_loss(anchor, positive, negative)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TripletMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>TripletMarginLoss损失函数的计算结果为 tensor(1.1667, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="13-HingEmbeddingLoss"><a href="#13-HingEmbeddingLoss" class="headerlink" title="13. HingEmbeddingLoss"></a>13. HingEmbeddingLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.HingeEmbeddingLoss(margin=<span class="number">1.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对输出的embedding结果做Hing损失计算<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>margin</code>：边界值<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582284050/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<p><strong>注意事项：</strong> 输入x应为两个输入之差的绝对值。<br>可以这样理解，让个输出的是正例yn&#x3D;1,那么loss就是x，如果输出的是负例y&#x3D;-1，那么输出的loss就是要做一个比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_f = nn.HingeEmbeddingLoss()</span><br><span class="line">inputs = torch.tensor([[<span class="number">1.</span>, <span class="number">0.8</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line">output = loss_f(inputs,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;HingEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>HingEmbeddingLoss损失函数的计算结果为 tensor(0.7667)
</code></pre>
<h3 id="14-余弦相似度"><a href="#14-余弦相似度" class="headerlink" title="14. 余弦相似度"></a>14. 余弦相似度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CosineEmbeddingLoss(margin=<span class="number">0.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对两个向量做余弦相似度<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>margin</code>：可取值[-1,1] ，推荐为[0,0.5] 。<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582351089/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<p>这个损失函数应该是最广为人知的。对于两个向量，做余弦相似度。将余弦相似度作为一个距离的计算方式，如果两个向量的距离近，则损失函数值小，反之亦然。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss_f = nn.CosineEmbeddingLoss()</span><br><span class="line">inputs_1 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>], [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>]])</span><br><span class="line">inputs_2 = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">output = loss_f(inputs_1,inputs_2,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;CosineEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>CosineEmbeddingLoss损失函数的计算结果为 tensor(0.5000)
</code></pre>
<h3 id="15-CTC损失函数"><a href="#15-CTC损失函数" class="headerlink" title="15.CTC损失函数"></a>15.CTC损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CTCLoss(blank=<span class="number">0</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, zero_infinity=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 用于解决时序类数据的分类<br>计算连续时间序列和目标序列之间的损失。CTCLoss对输入和目标的可能排列的概率进行求和，产生一个损失值，这个损失值对每个输入节点来说是可分的。输入与目标的对齐方式被假定为 “多对一”，这就限制了目标序列的长度，使其必须是≤输入长度。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>blank</code>：blank label。<br><code>zero_infinity</code>：无穷大的值或梯度值为 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Target are to be padded</span></span><br><span class="line">T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line">S = <span class="number">30</span>      <span class="comment"># Target sequence length of longest target in batch (padding length)</span></span><br><span class="line">S_min = <span class="number">10</span>  <span class="comment"># Minimum target length, for demonstration purposes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">target = torch.randint(low=<span class="number">1</span>, high=C, size=(N, S), dtype=torch.long)</span><br><span class="line"></span><br><span class="line">input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line">target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span><br><span class="line">ctc_loss = nn.CTCLoss()</span><br><span class="line">loss = ctc_loss(<span class="built_in">input</span>, target, input_lengths, target_lengths)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target are to be un-padded</span></span><br><span class="line">T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line">input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">target_lengths = torch.randint(low=<span class="number">1</span>, high=T, size=(N,), dtype=torch.long)</span><br><span class="line">target = torch.randint(low=<span class="number">1</span>, high=C, size=(<span class="built_in">sum</span>(target_lengths),), dtype=torch.long)</span><br><span class="line">ctc_loss = nn.CTCLoss()</span><br><span class="line">loss = ctc_loss(<span class="built_in">input</span>, target, input_lengths, target_lengths)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;CTCLoss损失函数的计算结果为&#x27;</span>,loss)</span><br></pre></td></tr></table></figure>

<pre><code>CTCLoss损失函数的计算结果为 tensor(16.0885, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h2 id="六、-优化器"><a href="#六、-优化器" class="headerlink" title="六、 优化器"></a>六、 优化器</h2><p>这里使用Adam优化器</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<p>Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面提供了十种优化器。</p>
<ul>
<li>torch.optim.ASGD</li>
<li>torch.optim.Adadelta</li>
<li>torch.optim.Adagrad</li>
<li>torch.optim.Adam</li>
<li>torch.optim.AdamW</li>
<li>torch.optim.Adamax</li>
<li>torch.optim.LBFGS</li>
<li>torch.optim.RMSprop</li>
<li>torch.optim.Rprop</li>
<li>torch.optim.SGD</li>
<li>torch.optim.SparseAdam</li>
</ul>
<p>而以上这些优化算法均继承于<code>Optimizer</code>，下面我们先来看下所有优化器的基类<code>Optimizer</code>。定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, defaults</span>):</span>        </span><br><span class="line">        self.defaults = defaults</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">        self.param_groups = []</span><br></pre></td></tr></table></figure>

<p><strong><code>Optimizer</code>有三个属性：</strong></p>
<ul>
<li><code>defaults</code>：存储的是优化器的超参数，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>state</code>：参数的缓存，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;, &#123;<span class="title">tensor</span>(<span class="params">[[ <span class="number">0.3864</span>, -<span class="number">0.0131</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">        [-<span class="number">0.1911</span>, -<span class="number">0.4511</span>]], requires_grad=<span class="literal">True</span></span>):</span> &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>param_groups</code>：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.1022</span>, -<span class="number">1.6890</span>],[-<span class="number">1.5116</span>, -<span class="number">1.7846</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;]</span><br></pre></td></tr></table></figure>

<p><strong><code>Optimizer</code>还有以下的方法：</strong></p>
<ul>
<li><code>zero_grad()</code>：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self, set_to_none: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment">#梯度不为空</span></span><br><span class="line">                <span class="keyword">if</span> set_to_none: </span><br><span class="line">                    p.grad = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> p.grad.grad_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        p.grad.detach_()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        p.grad.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">                    p.grad.zero_()<span class="comment"># 梯度设置为0</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>step()</code>：执行一步梯度更新，参数更新</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure</span>):</span> </span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<ul>
<li><code>add_param_group()</code>：添加参数组</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_param_group</span>(<span class="params">self, param_group</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(param_group, <span class="built_in">dict</span>), <span class="string">&quot;param group must be a dict&quot;</span></span><br><span class="line"><span class="comment"># 检查类型是否为tensor</span></span><br><span class="line">    params = param_group[<span class="string">&#x27;params&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(params, torch.Tensor):</span><br><span class="line">        param_group[<span class="string">&#x27;params&#x27;</span>] = [params]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(params, <span class="built_in">set</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;optimizer parameters need to be organized in ordered collections, but &#x27;</span></span><br><span class="line">                        <span class="string">&#x27;the ordering of tensors in sets will change between runs. Please use a list instead.&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        param_group[<span class="string">&#x27;params&#x27;</span>] = <span class="built_in">list</span>(params)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> param_group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(param, torch.Tensor):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;optimizer can only optimize Tensors, &quot;</span></span><br><span class="line">                            <span class="string">&quot;but one of the params is &quot;</span> + torch.typename(param))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> param.is_leaf:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;can&#x27;t optimize a non-leaf Tensor&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, default <span class="keyword">in</span> self.defaults.items():</span><br><span class="line">        <span class="keyword">if</span> default <span class="keyword">is</span> required <span class="keyword">and</span> name <span class="keyword">not</span> <span class="keyword">in</span> param_group:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;parameter group didn&#x27;t specify a value of required optimization parameter &quot;</span> +</span><br><span class="line">                             name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            param_group.setdefault(name, default)</span><br><span class="line"></span><br><span class="line">    params = param_group[<span class="string">&#x27;params&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(params) != <span class="built_in">len</span>(<span class="built_in">set</span>(params)):</span><br><span class="line">        warnings.warn(<span class="string">&quot;optimizer contains a parameter group with duplicate parameters; &quot;</span></span><br><span class="line">                      <span class="string">&quot;in future, this will cause an error; &quot;</span></span><br><span class="line">                      <span class="string">&quot;see github.com/pytorch/pytorch/issues/40967 for more information&quot;</span>, stacklevel=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 上面好像都在进行一些类的检测，报Warning和Error</span></span><br><span class="line">    param_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        param_set.update(<span class="built_in">set</span>(group[<span class="string">&#x27;params&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> param_set.isdisjoint(<span class="built_in">set</span>(param_group[<span class="string">&#x27;params&#x27;</span>])):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;some parameters appear in more than one parameter group&quot;</span>)</span><br><span class="line"><span class="comment"># 添加参数</span></span><br><span class="line">    self.param_groups.append(param_group)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>load_state_dict()</code> ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_state_dict</span>(<span class="params">self, state_dict</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Loads the optimizer state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        state_dict (dict): optimizer state. Should be an object returned</span></span><br><span class="line"><span class="string">            from a call to :meth:`state_dict`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># deepcopy, to be consistent with module API</span></span><br><span class="line">    state_dict = deepcopy(state_dict)</span><br><span class="line">    <span class="comment"># Validate the state_dict</span></span><br><span class="line">    groups = self.param_groups</span><br><span class="line">    saved_groups = state_dict[<span class="string">&#x27;param_groups&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(groups) != <span class="built_in">len</span>(saved_groups):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;loaded state dict has a different number of &quot;</span></span><br><span class="line">                         <span class="string">&quot;parameter groups&quot;</span>)</span><br><span class="line">    param_lens = (<span class="built_in">len</span>(g[<span class="string">&#x27;params&#x27;</span>]) <span class="keyword">for</span> g <span class="keyword">in</span> groups)</span><br><span class="line">    saved_lens = (<span class="built_in">len</span>(g[<span class="string">&#x27;params&#x27;</span>]) <span class="keyword">for</span> g <span class="keyword">in</span> saved_groups)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">any</span>(p_len != s_len <span class="keyword">for</span> p_len, s_len <span class="keyword">in</span> <span class="built_in">zip</span>(param_lens, saved_lens)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;loaded state dict contains a parameter group &quot;</span></span><br><span class="line">                         <span class="string">&quot;that doesn&#x27;t match the size of optimizer&#x27;s group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the state</span></span><br><span class="line">    id_map = &#123;old_id: p <span class="keyword">for</span> old_id, p <span class="keyword">in</span></span><br><span class="line">              <span class="built_in">zip</span>(chain.from_iterable((g[<span class="string">&#x27;params&#x27;</span>] <span class="keyword">for</span> g <span class="keyword">in</span> saved_groups)),</span><br><span class="line">                  chain.from_iterable((g[<span class="string">&#x27;params&#x27;</span>] <span class="keyword">for</span> g <span class="keyword">in</span> groups)))&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cast</span>(<span class="params">param, value</span>):</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Make a deep copy of value, casting all tensors to device of param.&quot;&quot;&quot;</span></span><br><span class="line">   		.....</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Copy state assigned to params (and cast tensors to appropriate types).</span></span><br><span class="line">    <span class="comment"># State that is not assigned to params is copied as is (needed for</span></span><br><span class="line">    <span class="comment"># backward compatibility).</span></span><br><span class="line">    state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict[<span class="string">&#x27;state&#x27;</span>].items():</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> id_map:</span><br><span class="line">            param = id_map[k]</span><br><span class="line">            state[param] = cast(param, v)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            state[k] = v</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameter groups, setting their &#x27;params&#x27; value</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_group</span>(<span class="params">group, new_group</span>):</span></span><br><span class="line">       ...</span><br><span class="line">    param_groups = [</span><br><span class="line">        update_group(g, ng) <span class="keyword">for</span> g, ng <span class="keyword">in</span> <span class="built_in">zip</span>(groups, saved_groups)]</span><br><span class="line">    self.__setstate__(&#123;<span class="string">&#x27;state&#x27;</span>: state, <span class="string">&#x27;param_groups&#x27;</span>: param_groups&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>state_dict()</code>：获取优化器当前状态信息字典</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Returns the state of the optimizer as a :class:`dict`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It contains two entries:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * state - a dict holding current optimization state. Its content</span></span><br><span class="line"><span class="string">        differs between optimizer classes.</span></span><br><span class="line"><span class="string">    * param_groups - a dict containing all parameter groups</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Save order indices instead of Tensors</span></span><br><span class="line">    param_mappings = &#123;&#125;</span><br><span class="line">    start_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pack_group</span>(<span class="params">group</span>):</span></span><br><span class="line">		......</span><br><span class="line">    param_groups = [pack_group(g) <span class="keyword">for</span> g <span class="keyword">in</span> self.param_groups]</span><br><span class="line">    <span class="comment"># Remap state to use order indices as keys</span></span><br><span class="line">    packed_state = &#123;(param_mappings[<span class="built_in">id</span>(k)] <span class="keyword">if</span> <span class="built_in">isinstance</span>(k, torch.Tensor) <span class="keyword">else</span> k): v</span><br><span class="line">                    <span class="keyword">for</span> k, v <span class="keyword">in</span> self.state.items()&#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;state&#x27;</span>: packed_state,</span><br><span class="line">        <span class="string">&#x27;param_groups&#x27;</span>: param_groups,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置权重，服从正态分布  --&gt; 2 x 2</span></span><br><span class="line">weight = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 设置梯度为全1矩阵  --&gt; 2 x 2</span></span><br><span class="line">weight.grad = torch.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 输出现有的weight和data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The data of weight before step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight before step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 实例化优化器</span></span><br><span class="line">optimizer = torch.optim.SGD([weight], lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># 进行一步操作</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment"># 查看进行一步后的值，梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The data of weight after step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight after step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 权重清零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="comment"># 检验权重是否为0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight after optimizer.zero_grad():\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 输出参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;optimizer.params_group is \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br><span class="line"><span class="comment"># 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight in optimizer:&#123;&#125;\nweight in weight:&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;params&#x27;</span>][<span class="number">0</span>]), <span class="built_in">id</span>(weight)))</span><br><span class="line"><span class="comment"># 添加参数：weight2</span></span><br><span class="line">weight2 = torch.randn((<span class="number">3</span>, <span class="number">3</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer.add_param_group(&#123;<span class="string">&quot;params&quot;</span>: weight2, <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>&#125;)</span><br><span class="line"><span class="comment"># 查看现有的参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;optimizer.param_groups is\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br><span class="line"><span class="comment"># 查看当前状态信息</span></span><br><span class="line">opt_state_dict = optimizer.state_dict()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;state_dict before step:\n&quot;</span>, opt_state_dict)</span><br><span class="line"><span class="comment"># 进行5次step操作</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="comment"># 输出现有状态信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;state_dict after step:\n&quot;</span>, optimizer.state_dict())</span><br><span class="line"><span class="comment"># 保存参数信息</span></span><br><span class="line">torch.save(optimizer.state_dict(),os.path.join(<span class="string">r&quot;D:\pythonProject\Attention_Unet&quot;</span>, <span class="string">&quot;optimizer_state_dict.pkl&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;----------done-----------&quot;</span>)</span><br><span class="line"><span class="comment"># 加载参数信息</span></span><br><span class="line">state_dict = torch.load(<span class="string">r&quot;D:\pythonProject\Attention_Unet\optimizer_state_dict.pkl&quot;</span>) <span class="comment"># 需要修改为你自己的路径</span></span><br><span class="line">optimizer.load_state_dict(state_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;load state_dict successfully\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(state_dict))</span><br><span class="line"><span class="comment"># 输出最后属性信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.defaults))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.state))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br></pre></td></tr></table></figure>

<h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行更新前的数据，梯度</span></span><br><span class="line">The data of weight before step:</span><br><span class="line">tensor([[-<span class="number">0.3077</span>, -<span class="number">0.1808</span>],</span><br><span class="line">        [-<span class="number">0.7462</span>, -<span class="number">1.5556</span>]])</span><br><span class="line">The grad of weight before step:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 进行更新后的数据，梯度</span></span><br><span class="line">The data of weight after step:</span><br><span class="line">tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]])</span><br><span class="line">The grad of weight after step:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 进行梯度清零的梯度</span></span><br><span class="line">The grad of weight after optimizer.zero_grad():</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"><span class="comment"># 输出信息</span></span><br><span class="line">optimizer.params_group <span class="keyword">is</span> </span><br><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 证明了优化器的和weight的储存是在一个地方，Python基于值管理</span></span><br><span class="line">weight <span class="keyword">in</span> optimizer:<span class="number">1841923407424</span></span><br><span class="line">weight <span class="keyword">in</span> weight:<span class="number">1841923407424</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出参数</span></span><br><span class="line">optimizer.param_groups <span class="keyword">is</span></span><br><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;, &#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[ <span class="number">0.4539</span>, -<span class="number">2.1901</span>, -<span class="number">0.6662</span>],</span><br><span class="line">        [ <span class="number">0.6630</span>, -<span class="number">1.5178</span>, -<span class="number">0.8708</span>],</span><br><span class="line">        [-<span class="number">2.0222</span>,  <span class="number">1.4573</span>,  <span class="number">0.8657</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行更新前的参数查看，用state_dict</span></span><br><span class="line">state_dict before step:</span><br><span class="line"> &#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"><span class="comment"># 进行更新后的参数查看，用state_dict</span></span><br><span class="line">state_dict after step:</span><br><span class="line"> &#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储信息完毕</span></span><br><span class="line">----------done-----------</span><br><span class="line"><span class="comment"># 加载参数信息成功</span></span><br><span class="line">load state_dict successfully</span><br><span class="line"><span class="comment"># 加载参数信息</span></span><br><span class="line">&#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># defaults的属性输出</span></span><br><span class="line">&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># state属性输出</span></span><br><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;, &#123;<span class="title">tensor</span>(<span class="params">[[-<span class="number">1.3031</span>, -<span class="number">1.1761</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">        [-<span class="number">1.7415</span>, -<span class="number">2.5510</span>]], requires_grad=<span class="literal">True</span></span>):</span> &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># param_groups属性输出</span></span><br><span class="line">[&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">1.3031</span>, -<span class="number">1.1761</span>],</span><br><span class="line">        [-<span class="number">1.7415</span>, -<span class="number">2.5510</span>]], requires_grad=<span class="literal">True</span>)]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [tensor([[ <span class="number">0.4539</span>, -<span class="number">2.1901</span>, -<span class="number">0.6662</span>],</span><br><span class="line">        [ <span class="number">0.6630</span>, -<span class="number">1.5178</span>, -<span class="number">0.8708</span>],</span><br><span class="line">        [-<span class="number">2.0222</span>,  <span class="number">1.4573</span>,  <span class="number">0.8657</span>]], requires_grad=<span class="literal">True</span>)]&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ol>
<li><p>每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Moddule</span>):</span></span><br><span class="line">    ···</span><br><span class="line">net = Net()</span><br><span class="line">optim = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">optim.step()</span><br></pre></td></tr></table></figure>
</li>
<li><p>optimizer在一个神经网络的epoch中需要实现下面两个步骤：</p>
<ol>
<li>梯度置零</li>
<li>梯度更新<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-5</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">	...</span><br><span class="line">	optimizer.zero_grad()  <span class="comment">#梯度置零</span></span><br><span class="line">	loss = ...             <span class="comment">#计算loss</span></span><br><span class="line">	loss.backward()        <span class="comment">#BP反向传播</span></span><br><span class="line">	optimizer.step()       <span class="comment">#梯度更新</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>给网络不同的层赋予不同的优化器参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"></span><br><span class="line">net = resnet18()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>:net.fc.parameters()&#125;,<span class="comment">#fc的lr使用默认的1e-5</span></span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>:net.layer4[<span class="number">0</span>].conv1.parameters(),<span class="string">&#x27;lr&#x27;</span>:<span class="number">1e-2</span>&#125;],lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用param_groups查看属性</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>为了更好的了解优化器，对PyTorch中的优化器进行了一个小测试</p>
<p><strong>数据生成</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 升维操作</span></span><br><span class="line">x = torch.unsqueeze(a, dim=<span class="number">1</span>)</span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * torch.normal(torch.zeros(x.size()))</span><br></pre></td></tr></table></figure>

<p><strong>数据分布曲线</strong>：<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.1.png"></p>
<p><strong>网络结构</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line">        self.predict = nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.hidden(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下面这部分是测试图，纵坐标代表Loss，横坐标代表的是Step：<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.2.png"><br>在上面的图片上，曲线下降的趋势和对应的steps代表了在这轮数据，模型下的收敛速度</p>
<p><strong>注意:</strong></p>
<p>优化器的选择是需要根据模型进行改变的，不存在绝对的好坏之分，我们需要多进行一些测试。<br>后续会添加SparseAdam，LBFGS这两个优化器的可视化结果</p>
<h2 id="七、-训练与评估"><a href="#七、-训练与评估" class="headerlink" title="七、 训练与评估"></a>七、 训练与评估</h2><p>关注两者的主要区别：<br>模型状态设置<br>是否需要初始化优化器<br>是否需要将loss传回到网络<br>是否需要每步更新optimizer<br>此外，对于测试或验证过程，可以计算分类准确率</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        data, label = data.cuda(), label.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data) <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(output, label)</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 优化器更新权重</span></span><br><span class="line">        train_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    train_loss = train_loss/<span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span>(<span class="params">epoch</span>):</span>       </span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 测试和训练不一样</span></span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    gt_labels = []</span><br><span class="line">    pred_labels = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 不计算梯度</span></span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, label = data.cuda(), label.cuda()</span><br><span class="line">            output = model(data)</span><br><span class="line">            preds = torch.argmax(output, <span class="number">1</span>) <span class="comment"># 得到预测的结果是哪一类</span></span><br><span class="line">            gt_labels.append(label.cpu().data.numpy()) <span class="comment"># 拼接起来</span></span><br><span class="line">            pred_labels.append(preds.cpu().data.numpy())</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            val_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    val_loss = val_loss/<span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)</span><br><span class="line">    acc = np.<span class="built_in">sum</span>(gt_labels==pred_labels)/<span class="built_in">len</span>(pred_labels) <span class="comment"># pre和label相等的次数除上总数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tValidation Loss: &#123;:.6f&#125;, Accuracy: &#123;:6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, val_loss, acc))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    train(epoch)</span><br><span class="line">    val(epoch)</span><br></pre></td></tr></table></figure>
<p><img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665568130565/D2B5CA33BD970F64A6301FA75AE2EB22"></p>
<h2 id="八、-可视化"><a href="#八、-可视化" class="headerlink" title="八、 可视化"></a>八、 可视化</h2><p>见后续专题</p>
<h2 id="九、-保存模型"><a href="#九、-保存模型" class="headerlink" title="九、 保存模型"></a>九、 保存模型</h2><p>训练完成后，可以使用torch.save保存模型参数或者整个模型，也可以在训练过程中保存模型:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">save_path = <span class="string">&quot;./FahionModel.pkl&quot;</span></span><br><span class="line">torch.save(model, save_path)</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%89%E7%AB%A0/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html">深入浅出PyTorch</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>pytorch - 各个组件和实践</p><p><a href="http://example.com/2022/10/12/pytorch-各个组件和实践/">http://example.com/2022/10/12/pytorch-各个组件和实践/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Yang</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2022-10-12</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-10-12</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Pytorch/">Pytorch</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechat.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/10/16/pytorch-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">pytorch - 模型定义</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/10/10/pytorch-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="level-item">pytorch - 基础知识</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="valine-thread"></div><script src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread',
            appId: "5g2Ywva8lFU3cYbP8uFyl0s7-gzGzoHsz",
            appKey: "6X4LgAqvfozBP061RrUMebFP",
            
            avatar: "mm",
            avatarForce: false,
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "zh-CN",
            visitor: false,
            highlight: true,
            recordIP: false,
            
            
            
            enableQQ: false,
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="🐏"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">🐏</p><p class="is-size-6 is-block">DubistmeinAugenstern</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>WuHan,China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Yang-Emily" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Yang-Emily"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/wuyangemily/"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Youtube" href="https://www.youtube.com/channel/UCSnojGpH9om-xzl-og2_AZQ/featured"><i class="fab fa-youtube"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="twitter" href="https://twitter.com/YangWu00506105"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="http://wpa.qq.com/msgrd?v=3&amp;uin=1047772929&amp;site=qq&amp;menu=yes"><i class="fab fa-qq"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#神经网络学习机制"><span class="level-left"><span class="level-item">1</span><span class="level-item">神经网络学习机制</span></span></a></li><li><a class="level is-mobile" href="#深度学习在实现上的特殊性"><span class="level-left"><span class="level-item">2</span><span class="level-item">深度学习在实现上的特殊性</span></span></a></li><li><a class="level is-mobile" href="#pytorch深度学习模块"><span class="level-left"><span class="level-item">3</span><span class="level-item">pytorch深度学习模块</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#一、-基本配置"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">一、 基本配置</span></span></a></li><li><a class="level is-mobile" href="#二、-数据读入"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">二、 数据读入</span></span></a></li><li><a class="level is-mobile" href="#三、-模型构建"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">三、 模型构建</span></span></a></li><li><a class="level is-mobile" href="#四、-模型初始化"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">四、 模型初始化</span></span></a></li><li><a class="level is-mobile" href="#五、-损失函数"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">五、 损失函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-二分类交叉熵损失函数"><span class="level-left"><span class="level-item">3.5.1</span><span class="level-item">1. 二分类交叉熵损失函数</span></span></a></li><li><a class="level is-mobile" href="#2-交叉熵损失函数"><span class="level-left"><span class="level-item">3.5.2</span><span class="level-item">2. 交叉熵损失函数</span></span></a></li><li><a class="level is-mobile" href="#3-L1损失函数"><span class="level-left"><span class="level-item">3.5.3</span><span class="level-item">3. L1损失函数</span></span></a></li><li><a class="level is-mobile" href="#4-MSE损失函数"><span class="level-left"><span class="level-item">3.5.4</span><span class="level-item">4. MSE损失函数</span></span></a></li><li><a class="level is-mobile" href="#5-平滑L1-Smooth-L1-损失函数"><span class="level-left"><span class="level-item">3.5.5</span><span class="level-item">5. 平滑L1 (Smooth L1)损失函数</span></span></a></li><li><a class="level is-mobile" href="#6-目标泊松分布的负对数似然损失"><span class="level-left"><span class="level-item">3.5.6</span><span class="level-item">6. 目标泊松分布的负对数似然损失</span></span></a></li><li><a class="level is-mobile" href="#7-KL散度"><span class="level-left"><span class="level-item">3.5.7</span><span class="level-item">7. KL散度</span></span></a></li><li><a class="level is-mobile" href="#8-MarginRankingLoss"><span class="level-left"><span class="level-item">3.5.8</span><span class="level-item">8. MarginRankingLoss</span></span></a></li><li><a class="level is-mobile" href="#9-多标签边界损失函数"><span class="level-left"><span class="level-item">3.5.9</span><span class="level-item">9. 多标签边界损失函数</span></span></a></li><li><a class="level is-mobile" href="#10-二分类损失函数"><span class="level-left"><span class="level-item">3.5.10</span><span class="level-item">10. 二分类损失函数</span></span></a></li><li><a class="level is-mobile" href="#11-多分类的折页损失"><span class="level-left"><span class="level-item">3.5.11</span><span class="level-item">11. 多分类的折页损失</span></span></a></li><li><a class="level is-mobile" href="#12-三元组损失"><span class="level-left"><span class="level-item">3.5.12</span><span class="level-item">12. 三元组损失</span></span></a></li><li><a class="level is-mobile" href="#13-HingEmbeddingLoss"><span class="level-left"><span class="level-item">3.5.13</span><span class="level-item">13. HingEmbeddingLoss</span></span></a></li><li><a class="level is-mobile" href="#14-余弦相似度"><span class="level-left"><span class="level-item">3.5.14</span><span class="level-item">14. 余弦相似度</span></span></a></li><li><a class="level is-mobile" href="#15-CTC损失函数"><span class="level-left"><span class="level-item">3.5.15</span><span class="level-item">15.CTC损失函数</span></span></a></li></ul></li><li><a class="level is-mobile" href="#六、-优化器"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">六、 优化器</span></span></a></li><li><a class="level is-mobile" href="#实际操作"><span class="level-left"><span class="level-item">3.7</span><span class="level-item">实际操作</span></span></a></li><li><a class="level is-mobile" href="#输出结果"><span class="level-left"><span class="level-item">3.8</span><span class="level-item">输出结果</span></span></a></li><li><a class="level is-mobile" href="#实验"><span class="level-left"><span class="level-item">3.9</span><span class="level-item">实验</span></span></a></li><li><a class="level is-mobile" href="#七、-训练与评估"><span class="level-left"><span class="level-item">3.10</span><span class="level-item">七、 训练与评估</span></span></a></li><li><a class="level is-mobile" href="#八、-可视化"><span class="level-left"><span class="level-item">3.11</span><span class="level-item">八、 可视化</span></span></a></li><li><a class="level is-mobile" href="#九、-保存模型"><span class="level-left"><span class="level-item">3.12</span><span class="level-item">九、 保存模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#参考资料"><span class="level-left"><span class="level-item">4</span><span class="level-item">参考资料</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://leetcode-cn.com/u/wuyangemily/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Leetcode</span></span><span class="level-right"><span class="level-item tag">leetcode-cn.com</span></span></a></li><li><a class="level is-mobile" href="https://blog.csdn.net/qq_44729001" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">CSDN</span></span><span class="level-right"><span class="level-item tag">blog.csdn.net</span></span></a></li><li><a class="level is-mobile" href="https://www.kaggle.com/wuyangemily" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Kaggle</span></span><span class="level-right"><span class="level-item tag">www.kaggle.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%82/"><span class="level-start"><span class="level-item">杂</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度强化学习</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-23T06:43:54.000Z">2022-10-23</time></p><p class="title"><a href="/2022/10/23/pytorch-%E7%94%9F%E6%80%81%E9%83%A8%E7%BD%B2/">pytorch - 生态和部署</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-22T06:43:54.000Z">2022-10-22</time></p><p class="title"><a href="/2022/10/22/pytorch-%E5%8F%AF%E8%A7%86%E5%8C%96/">pytorch - 可视化</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-20T06:43:54.000Z">2022-10-20</time></p><p class="title"><a href="/2022/10/20/pytorch-%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/">pytorch - 训练技巧</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-16T06:43:54.000Z">2022-10-16</time></p><p class="title"><a href="/2022/10/16/pytorch-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/">pytorch - 模型定义</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-12T06:43:54.000Z">2022-10-12</time></p><p class="title"><a href="/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/">pytorch - 各个组件和实践</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">十月 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">九月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">八月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">三月 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">十月 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DRL/"><span class="tag">DRL</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Draft/"><span class="tag">Draft</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Intelligence-Code/"><span class="tag">Intelligence Code</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">6</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="🐏&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Yang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Yang-Emily/Yang-Emily.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>