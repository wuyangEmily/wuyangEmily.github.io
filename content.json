{"pages":[{"title":"About","text":"Nice to meet you ^^欢迎留言~","link":"/about/index.html"}],"posts":[{"title":"test_my_site","text":"","link":"/2020/01/27/test-my-site/"},{"title":"线性代数的本质 - 02线性组合、张成的空间与基","text":"所有可以表示为给定向量线性组合的向量集合，被称为给定向量张成的空间 两个向量张成的空间就是它们所有可能的线性组合，也就是缩放再相加之后可能得到的向量 一组向量中至少有一个是多余的，没有对张成空间做出任何贡献，有多个向量，并且可以移除其中一个而不减小张成的空间，当这种情况发生时，它们是线性相关的。其中一个向量可以表示为其他向量的线性组合，因为这个向量已经落在其他向量张成的空间之中 如果所有向量都给张成的空间增添了新维度，它们是线性无关的 基的严格定义：张成该空间的一个线性无关向量的集合","link":"/2021/10/06/02%20%E7%BA%BF%E6%80%A7%E7%BB%84%E5%90%88%E3%80%81%E5%BC%A0%E6%88%90%E7%9A%84%E7%A9%BA%E9%97%B4%E4%B8%8E%E5%9F%BA/"},{"title":"线性代数的本质 - 03矩阵与线性变换","text":"变换是在暗示以特定方式来可视化这一输入-输出关系，一种理解“向量的函数”的方法是使用运动。 线性代数限制在一种特殊类型的变换上，“线性变换”：一是直线在变换之后仍然保持为直线，不能有所弯曲；二是原点必须保持固定。总的来说，保持网格线平行并等距分布。 一个二维线性变换仅由四个数字完全确定，2X2矩阵，可以把列理解为两个特殊的向量，即$\\vec{i}$和$\\vec{j}$分别落脚的位置。如果有一个描述线性变换的2x2矩阵，以及一个给定向量，线性变换对这个向量的作用：只需取出向量的坐标，将它们分别与矩阵的特定列相乘，然后将结果相加即可。我们完全可以把矩阵的列看作变换后的基向量，把矩阵向量乘法看作一个线性组合。 如果变换后的$\\vec{i}$和$\\vec{j}$是线性相关的，意味着一个向量是另一个的倍数，那么这个线性变换将整个二维空间挤压到它们所在的一条直线上，也就是这两个线性相关向量所张成的一维空间。 每次当你看到一个矩阵时，你都可以把它解读为对空间的一种特定变换。","link":"/2021/10/06/03%20%E7%9F%A9%E9%98%B5%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/"},{"title":"线性代数的本质 - 04矩阵乘法与线性变换复合","text":"一个变换之后再进行另一个变换，这个新的线性变换被称为前两个独立变换的“复合变换”。eg：先旋转再剪切：通常将函数写在变量的左侧，所以函数复合时从右向左读: 矩阵乘法满足结合律，交换律不满足","link":"/2021/10/06/04%20%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%A4%8D%E5%90%88/"},{"title":"线性代数的本质 - 05行列式","text":"只要知道单位正方形面积变化的比例，就能知道其他任意区域的面积变化比例。由网格线保持平行且等距分布推出，无论一个方格如何变化，对其他大小的方格来说，都会有相同变化。 这个缩放比例，即线性变换对面积产生改变的比例，被称为这个变换的行列式。 当空间取向被反转时，行列式为负值，绝对值依然是区域面积的缩放比例： 三维空间中是体积的缩放，用“右手定则”描述三维空间的取向。如果只能用左手描述时，说明空间取向发生翻转，行列式为负。 计算行列式","link":"/2021/10/06/05%20%E8%A1%8C%E5%88%97%E5%BC%8F/"},{"title":"线性代数的本质 - 06逆矩阵、列空间与零空间","text":"求解$A\\vec{x}=\\vec{v}$，意味着寻找一个$\\vec{x}$，使得它在变换后与$\\vec{v}$重合。解在于A的变换是将空间挤压到一条线或一个点的低维空间，还是完整的二维空间。即A的行列式为0，或不为0。 不为0时：有且只有一个向量与$\\vec{v}$重合，可以通过逆向进行变换并跟踪$\\vec{v}$的动向来找到这个向量。A的逆，即$A^{-1}$。什么都不做的变换，称为“恒等变换”。求解$\\vec{x}$： $det(A)\\neq 0$ -&gt; $A^{-1}$ 为0时：变换将空间压缩到更低的维度上，此时没有逆变换，$det(A)=0$，解仍然可能存在： 当变换结果为一条直线时，结果是一维的，这个变换的秩为1；当变换后的向量落在一个二维平面上，这个变换的秩为2。秩代表着变换后空间的维数。 不管是一条直线、一个平面还是三维空间等，所有可能的变换结果的集合，被称为矩阵的“列空间”。 矩阵的列为基向量变换后的位置，这些变换后的基向量张成的空间就是所有可能的变换结果。列空间就是矩阵的列所张成的空间，秩更精确的定义是列空间的维数。 零向量一定被包含在列空间中，因为线性变换必须保持原点位置不变。对一个满秩变换来说唯一能在变换后落在原点的就是零向量自身，但对一个非满秩矩阵来说，它将空间压缩至一个更低维度上，会有一系列向量在变换后成为零向量。 在变换后落在原点的向量集合，被称为矩阵的“零空间”或“核”，变换后一些向量落在零向量上，“零空间”就是这些向量所构成的空间。 对线性方程组来说，当向量$\\vec{v}$恰好为零向量时，零空间给出的就是这个向量方程的所有可能解。 补充说明 - 非方阵 3x2矩阵的几何意义是将二维空间映射到三维空间上 因为矩阵由两列表明输入空间有两个基向量，有三行表明每一个基向量在变换后都用三个独立的坐标来表示 2x3矩阵的几何意义是将三维空间映射到二维空间上 二维空间到一维空间的变换","link":"/2021/10/06/06%20%E9%80%86%E7%9F%A9%E9%98%B5%E3%80%81%E5%88%97%E7%A9%BA%E9%97%B4%E4%B8%8E%E9%9B%B6%E7%A9%BA%E9%97%B4/"},{"title":"线性代数的本质 - 07点积与对偶性","text":"点积的标准观点：两个相同维数的向量，或两个相同长度的数组，求他们的点积，就是将相应坐标配对，求出每一对的乘积，然后将结果相加 点积与顺序无关 对偶性 多维空间到一维空间（数轴）的线性变换 单位向量的点积可以看成将向量投影到单位向量所在直线上所得到的投影值 非单位向量：投影后缩放 总言之即，向量与给定非单位向量的点积可以解读为，首先将朝给定向量上投影，然后将投影的值与给定向量长度相乘 二维空间到数轴的线性变换通过将空间投影到给定数轴上来定义，因为变换是线性的，则必然可以用某个1x2矩阵描述，又因为1x2矩阵与二维向量相乘的计算过程和转置矩阵并求点积的计算过程相同，所以这个投影变换必然会与某个二维向量相关。 无论何时看到一个二维到一维的线性变换，他的输出空间是一维数轴，空间中会存在唯一的向量v与之相关，就这意义而言，应用变换和与向量v做点积是一样的 两个向量点乘，就是将其中一个向量转化为线性变换","link":"/2021/10/06/07%20%E7%82%B9%E7%A7%AF%E4%B8%8E%E5%AF%B9%E5%81%B6%E6%80%A7/"},{"title":"线性代数的本质 - 08叉积的标准介绍、以线性变换的眼光看叉积","text":"叉积的标准介绍 顺序对叉积有影响，$\\vec{v}$在$\\vec{w}$的右边，那么叉乘为正 $\\vec{v}$在$\\vec{w}$的左边，那么叉乘为负 05行列式计算叉积： 以线性变换的眼光看叉积 真正的叉积是通过两个三维向量生成一个新的三维向量，这个叉积的结果是一个向量，长度是平行四边形的面积，方向与平行四边形所在的面垂直，使用右手定则确定方向。 why？-对偶性07 点积与对偶性找到的线性函数对于给定向量的作用：将向量投影到垂直于v和w的直线上，然后将投影长度与v和w张成的平行四边形的面积相乘。这意味着我们找到了一个向量p，使得p与某个向量（x，y，z）点乘时，所得结果等于一个3x3矩阵的行列式，这个矩阵的三列分别为（x，y，z）、v的坐标和w的坐标，定义了一个线性变换：应用这个变换与对偶向量点乘等价","link":"/2021/10/06/08%20%E5%8F%89%E7%A7%AF%E7%9A%84%E6%A0%87%E5%87%86%E4%BB%8B%E7%BB%8D%E3%80%81%E4%BB%A5%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E7%9A%84%E7%9C%BC%E5%85%89%E7%9C%8B%E5%8F%89%E7%A7%AF/"},{"title":"线性代数的本质 - 09基变换","text":"发生在向量与一组数之间的任意一种转化，都被称为一个坐标系 03 矩阵与线性变换将矩阵乘法理解为应用一个特定的线性变换 变换后的向量仍旧是相同的线性组合，不过使用的是新的基向量 坐标系之间的转换：","link":"/2021/10/06/09%20%E5%9F%BA%E5%8F%98%E6%8D%A2/"},{"title":"线性代数的本质 - 10特征向量与特征值","text":"考虑一个线性变换意味着矩阵对它的作用仅仅是拉伸或者压缩而已，如同一个标量，这些特殊向量被称为变换的“特征向量”，每个特征向量都有一个所属的值，被称为“特征值” 三维旋转中特征值为1时，相当于找到了一个旋转轴 计算特征值和特征向量：寻找一个向量V，使得这个新矩阵与V相乘结果为零向量06 逆矩阵、列空间与零空间空间压缩对应的就是矩阵的行列式为0 二维线性变换不一定有特征向量 可能出现只有一个特征值，但特征向量不止在一条直线上 特征基：09 基变换如果基向量恰好是特征向量 变换坐标系使得这些特征向量为基向量 当需要计算次幂的时候，将坐标系换成特征基会方便很多，然后转换回标准坐标系","link":"/2021/10/06/10%20%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E4%B8%8E%E7%89%B9%E5%BE%81%E5%80%BC/"},{"title":"线性代数的本质 - 11抽象向量空间","text":"行列式和特征向量与所选坐标系无关，行列式告诉的是一个变换对面积的缩放比例，特征向量则是在变换中留在它所张成的空间中的向量，这二者都是暗含于空间中的性质，自由选取坐标系不会改变它们最根本的值 既不是一个箭头也不是一组数字，但是同样具有向量特性的东西：函数函数的线性变换有一个完全合理的解释：这个变换接收一个函数并把它变成另一个函数，从微积分中可以找到一个常见的例子——导数。“算子”和“变换”的意思是一样的一个函数变换是线性的：03 矩阵与线性变换抽象性带来一般性的结论，不仅适用于箭头也适用于函数，满足以下两条性质的变换是线性的：“可加性”和“成比例”eg：求导是线性运算 多项式空间上，整个空间包含任意高次的多项式，首先我们要给这个空间赋予坐标的含义，需要选取一个基由于多项式的次数可以任意高，所以这个基函数集也是无穷大的这些类似向量的事物，比如箭头、一组数、函数等，它们构成的集合被称为“向量空间”只要定义满足公理，就能顺利的应用结论","link":"/2021/10/06/11%20%E6%8A%BD%E8%B1%A1%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4/"},{"title":"线性代数的本质 - 12克莱姆法则、几何解释","text":"05 行列式、07 点积与对偶性、06 逆矩阵、列空间与零空间高斯消元比克莱姆计算得更快 $det（A）= 0$降维，要么存在无数解要么没有解 $det（A）\\neq 0$维数依然相同，一个输入对应一个输出，一个输出也对应一个输入 对大多数线性变换来说，点积会随着变换而改变，不改变点积的矩阵变换是正交的变换，基向量在变换后依然保持单位长度且相互垂直 用正交矩阵求解线性系统非常简单，因为点积保持不变，所以已知的输出向量和矩阵的列向量的点积，分别等同于未知输入向量和各个基向量的点积\\ 对大多数线性方程组：这个解法就是克莱姆法则","link":"/2021/10/06/12%20%E5%85%8B%E8%8E%B1%E5%A7%86%E6%B3%95%E5%88%99%E3%80%81%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A/"},{"title":"机器学习 - 01概述","text":"工作流程机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测。 获取数据 数据基本处理 特征工程 机器学习（模型训练） 模型评估 结果达到要求，上线服务 没有达到要求，重新上面步骤 算法分类 监督学习：输入数据由特征值和目标值组成，输出可以是连续的值（回归），或者是有限个离散值（分类）。 无监督学习：输入数据由特征值组成，没有目标值。输入数据没有被标记，也没有确定的结果。样本数据类别未知； 需要根据样本间的相似性对样本集进行类别划分。 半监督学习：训练集同时包含标记样本数据和未标记样本数据 强化学习：实质是make decisions 问题，即自动进行决策，并且可以做连续决策。主要包含五个元素：agent, action, reward, environment, observation；强化学习的目标就是获得最多的累计奖励。 模型评估 分类模型评估 准确率 精确率 召回率 F1-score AUC指标 回归模型评估 均方根误差（Root Mean Squared Error，RMSE） 相对平方根误差（Relative Squared Error，RSE） 平均决定误差（Mean Absolute Error，MAE) 相对绝对误差（Relative Absolute Error，RAE) 拟合 欠拟合（under-fitting）：模型学习的太过粗糙，连训练集中的样本数据特征关系都没有学出来。 过拟合（over-fitting）：所建的机器学习模型或者是深度学习模型在训练样本中表现得过于优越，导致在测试数据集中表现不佳。","link":"/2020/08/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%2001%E6%A6%82%E8%BF%B0/"},{"title":"论文阅读 - DeepFix Fixing Common C Language Errors by Deep Learning","text":"program repair ≈ grammar correction in nlp 作者提出了一个端到端的，带 attention 的多层 seq2seq neural network，包括 RNN 编码器，和带 attention 的 RNN 解码器。该网络可以预测程序出错的位置并附上正确的修复。对比其他修复特定编程任务的工作，DeepFix 可以用在任何未预见的任务上。不易解决的错误需要考虑到程序文本中的长期依赖。DeepFix 通过带注意力机制的 seq2seq 模型来捕捉长期依赖。它的优势在于：(1) 利用端对端的基于深度学习网络解决普遍的编程问题；(2) 可以迭代地解决一个程序中的多个错误；(3) 在上千 C 程序评估中得到很好的结果。 Program Representation程序文本由不同类型的标记组成，如类型、关键字、特殊字符(如分号)、函数、文字和变量。其中，类型、关键字、特殊字符和库函数构成了跨不同程序的共享词汇表。作者在表示程序时保留它们，对其他类型的 token 进行如下建模。首先定义一个固定大小的名称池，然后为每个程序构造一个单独的编码映射 encoding map ，方法是将程序中每个不同的标识符(变量名或函数名)随机映射到池中唯一的名称，并选择一个足够大的池来为数据集中的任何程序创建上述映射。这种转换不会改变程序的语义，且是可逆的。字面量的确切值对学习任务无关紧要，因此，根据字面值的类型将其映射为特殊的 token ，例如，将所有整数字面值映射为 NUM ，将所有字符串字面值映射为 STR 。用 &lt; eos &gt; 表示标记序列的结束。 由于一个程序用 seq 的 tokens 表示时，要产生类似长度的准确修复后的 seq 非常困难，且一个程序通常包含上百个 token。所以作者把代码行号也进行标记，将一个 K 行的程序 P 表示为 $(l_1 , s_1 ),…,( l_k ,s_k)&lt; eos &gt;$，l 和 s 分别是对行号和语句的标记。一个 fix 包括 $l_i , s_i$ ，这比用全部的 token 作为输出简单得多。 Neural Network Architecture基于 Neural Machine Translation by Jointly Learning to Align and Translate ，NLP 中 encoder-decoder 中第一个使用 attention 机制的工作，将 attention 机制用到了神经网络机器翻译(NMT)。https://arxiv.org/pdf/1409.0473.pdf 这里编码器和解码器 rnn 都由 N 个堆叠的门控循环单元(GRUs)组成。编码器将输入序列中的每个 token 映射到一个称为 annotation 的实向量。对于输入序列$x_1，…，x_{T_x}$, t 时刻的隐藏单元激活计算如下: $h_t^{(1)} = GRU(h_{t-1}^{(1)},x_t)$ $h_t^{(n)} = GRU(h_{t-1}^{(n)},h_t^{(n-1)}),\\forall n \\in { 2,…,N }$ 解码器网络的隐藏状态被初始化为用编码器网络的最终状态，然后更新： $d_t^{(n)} = GRU(d_{t-1}^{(n)},d_t^{(n-1)}),\\forall n \\in { 2,…,N }$ $d_t^{(1)} = GRU(d_{t-1}^{(1)},z_t)$ 其中 $z_t$ 是输出 $\\hat{y}_{t-1}$在 $t-1$ 和上下文向量 $c_t$ 的连接，定义如下: $c_t = \\sum_{j=1}^{T_x} a_{tj}h_j^{(N)}$ $a_{tj} = \\frac{exp(e_{tj})}{\\sum_{k=1}^{T_x}exp(e_{tk})}$ $e_{tk} = \\Phi(d_{t-1},h_k^{(N)})$ c 就是全部隐状态的一个加权和，用到归一化权重 a 。a 就是一个对齐模型，用来评估当前预测词，与输入词每一个词的相关度。将上一个输出序列隐状态 $d_{t-1}$ 和输入序列隐状态 $h$ 输入网络，然后做 softmax 归一化，计算出权重。 Iterative Repair DeepFix 使用简单而有效的迭代策略来修复程序中的多个错误。oracle 的工作是通过检查更新后的程序是否比输入的程序好来决定是否接受修复。如果更新后的程序不会比输入程序产生更多的错误消息，则使用编译器并接受修复。我们还使用一些启发式方法来防止对输入程序的任意更改。例如，如果 oracle 不保留原始语句 $s_i$ 中的标识符和关键字，则它拒绝 fix $s_i$。一旦修复程序被接受，DeepFix 将再次向网络显示更新后的程序。 这种迭代策略停止的条件：(1) oracle 确定更新程序没有任何错误；或 (2) 网络视输入程序是正确的，发出一种特殊 token “fix”；或 (3) oracle拒绝修复,或 (4)达到预定的迭代的数量上限。 除了决定是否替换语句，网络还会决定新的一行是否要插入在行前或行后，用 $l_i^-,l_i^+$ 代替 $l_i$，如果要删除行，则将 $l_i$ 带上空字符 $\\epsilon$。oracle 使用程序特殊的编码映射，将修复好的 token sequence 重构回原来的标识符。它使用修复中的行号，并用输入程序中相应行中的字面值替换诸如 NUM 和 STR 等特殊标记。如果 oracle 不能重建程序文本，那么它拒绝修复。 作者提出的修复策略有几个优点：(1) 程序完整地呈现在网络上。识别和修复编程错误通常需要能够推断长期依赖关系的全局分析。网络架构能够有选择地参与程序的任何部分，可以推理结构和语法约束，以预测错误的位置和需要的修复。(2) 在输入和输出中都包含行号，降低了粒度，从而降低了预测任务的复杂性。(3) DeepFix可以迭代地修复程序中的多个错误。(4) oracle用于跟踪进度，防止无用的或任意的更改。(5) DeepFix的修复策略比较泛化。例如，如果我们试图修复逻辑错误，我们可以使用测试引擎和测试套件作为oracle。如果修复程序通过了更多的测试，那么它将被接受。 Experimentshttps://bitbucket.org/iiscseal/deepfix 数据集：两类程序，一种是编译的程序(正确的程序)，另一种是不编译的程序(错误的程序)。一个学生可能会提交几个错误的程序，作者在每个学生的每个编程任务中随机选择一个错误的程序，以避免测试结果的偏差。采用五折交叉验证","link":"/2022/03/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2001/"},{"title":"论文阅读 - Deep Reinforcement Learning for Programming Language Correction","text":"新手编程者经常饱受程序语言正规语法的折磨，为了协助他们，作者设计了一个基于强化学习的新型程序语言纠正框架，这个框架允许一个智能体 agent 模仿人类行为进行文本导航和编辑，展示了一个 agent 可以通过直接从原始的输入自我探索来进行训练，自我探索就是说，程序文本它自己，不利用程序语言正规语法的任何经验。我们充分利用专家案例作为 1/10 的训练数据，来加速训练。 程序语言规定程序文本的语法规则检验。一个不遵守规则的程序文本不会被编译执行，这给新手编程者带来了障碍。在目前大量的线上编程课程中，从指导者获得个性化反馈是十分不可实行的。因此，作者的工作旨在利用技术帮助新手编程者，通过自动化修正程序中的通用语法错误。 作者通过强化学习提出这个问题。当面对一个错误，一个程序员根据程序文本找到错误的位置，然后修正编辑来修复错误。作者提出了一个新型程序语言修正框架，在这里，一个 agent 可以模仿这些行为。一个 agent 可以访问和修改一个程序文本，对它来说，检查程序文本语法有效性的编译器是一个黑盒子。编译器通常不会精确地指明错误的位置。所以，不能依赖编译器来找到错误的位置和进行修正。作者利用编译器生成的错误信息的数量设计了一个 reward function。agent 的目标是将程序成功编译所必要的编辑行为表现的最好。 框架的挑战是为程序文本的 agent 学习一个 control policy，不通过任何带有程序语言正规语法知识的 agent 。通过深度学习，agents 可以被训练到专家水平来玩视觉文字游戏，有趣的是，这些技术直接从原始输入，如像素、文本等。在这项工作中，第一次展示了它在程序空间中的可能性。 如图1，修复了税收算法，有两个语法错误：第四行的 scanf 使用错误， 第12行的 “}” 缺失。程序以被标记化的形式展现给 agent，agent 的指针位置被初始化为程序的第一个 token。在程序中 agent 的导向性的动作用箭头表示，系列动作如图所示。agent 准确定位和修复所有错误。首先，agent 导向错误的位置行4，用逗号替代不正确的分号，在错误2中插入丢失的 “}” 。这些编辑操作完成后，程序成功编译，agent 停止。这比蛮力列举编译修正高效很多。 通过长短期记忆网络 LSTM 网络进行编码，agent 被允许执行一系列的导航和编辑行为来修复程序。每一步修复错误的编辑都获得一些小的奖励，最大化达到目标状态的奖励，即程序的无错误状态。agent 的 control policy 就是学习使用 A3C 算法。 训练一个agent的难点有两个：（1）agent 要同时定位错误并在此做出精确的编辑来修复程序。错误的编辑会导致引入更多的错误，使得任务更加困难。为了克服这个问题，我们设定环境来拒绝这样的编辑。这显著的削减了 state space。（2）随着时间增长，任务的 state space 越来越大，state 探索收集的信息越来越多，的强化学习趋向越来越慢。一个方法是利用专家表示来引导 agent。在我们的工作中，这些表示是自动生成的而不是人类干预，我们将此称为 RLAssist。 DeepFix 是目前修复程序错误中表现最好的工具。作者在需要修复的 C 程序上对比该工具和 RLAssist 。作者证明了RLAssist可以通过只使用错误程序自我探索来训练，同时仍然可以达到 DeepFix 的性能。通过专家演示加速训练，为 10% 训练集生成专家演示，90% 的数据集没有演示。RLAssist完全修复了测试集中26.6%的程序，并解决了39.7%的错误消息。与DeepFix相比，这两个版本分别提高了14%和29%。因此，RLAssist在一小部分训练数据上使用专家演示，表现优于DeepFix。 此工作的主要贡献如下: 设计了一种新颖的程序设计语言校正框架，可用于强化学习。 我们使用 A3C 来纠正编程语言，并通过专家演示加速培训。 我们的实验表明，我们的技术只使用了十分之一的训练数据，其性能超过了最先进的工具 DeepFix 。此外，我们的技术也可以在没有任何演示的情况下工作，但仍然可以匹配 DeepFix 的性能。 RLAssist 的实现将是开源的。 a framework for programming language correction tasks当面对一个错误时，一个程序员会定位程序中错误的位置并编辑操作来修复错误。出现大量错误时，程序员会重复如上步骤，此文提出程序语言修正框架，一个 agent 可以模仿以上的行为。 states一个 state 用一个 &lt; string , cursor &gt; 表示，string 表示程序文本，cursor $\\in$ { 1,…,len(string) }，len(string) 表示 string 中 token 的数量。env 跟踪 string 中错误的数量。这些错误可以从 ground truth 确定(当 ground truth可获取时)，也可以从编辑器编译 string 时产生的错误消息来估计。对于编译器，我们使用 GNU C 编译器。 编码 state 到 a sequence of tokens。首先，将程序字符串转换成词汇序列，这些词汇是不同的类型，例如关键字、操作符、类型、函数、字面量和变量。此外，我们还保留换行符作为词汇，以允许在程序文本上进行二维导航操作。state 中 cursor 部分由一个特殊的 token 表示，插入到序列中，刚好在游标持有索引的 token 之后。 接下来，在所有程序中构建一个共享词汇表。除了一些常见的库函数(如 printf 和 scanf )外，所有其他函数和变量标识符都映射到一个特殊的 token ID。类似地，所有的字面值都根据其类型映射到特殊的 token ，例如，数字映射到 NUM ，字符串映射到 STR 。所有剩余的 tokens 都包含在词汇表中，无需任何修改。这种映射减少了 agent 看到的词汇表的大小。请注意，此编码仅在将 state 反馈给 agent 时才需要。基于这样的编码，agent 预测到的 actions，在原始程序 string 上被 env 执行。 actions and transitionsagent 的动作可以分为两类，一类是更新 cursor，一类是修改 string。我们将第一类称为导航操作，第二类为编辑操作。导航操作允许 agent 在 string 中导航。这些操作只改变一个 state 的游标，而不是 string 。另一方面，编辑操作用于纠错，只修改 string 而不修改游标。错误的编辑操作会在 string 中引入更多错误，而不是修复它们。我们将环境配置为拒绝所有此类编辑，以删除使修复程序变得更加困难的状态空间。此外，拒绝错误的编辑可以防止对程序的任意更改。 对于我们的任务，我们只允许两个导航操作，右移和下移。它们分别将光标设置为右侧的下一个 token 或下一行的第一个 token 。如果光标已经设置为一行的最后一个 token ，则右移动作没有效果；或者光标是最后一行的任何 token 时，向下移动没有效果。注意，向下移动操作是可能的，因为我们在状态编码中保留了换行符。 基于对程序员新手常见的排版错误的研究，我们设计了三种类型的编辑操作。第一个参数化插入 token 操作，插入参数 token 到光标位置之前。参数可以是一个修复的 token 集合中的任何 token ，称之为可变 tokens。第二个是删除操作，删除光标上的token，只有在来自可变 tokens 时才能删除。我们限制可变 tokens 为以下五个类型的 token：分号、括号、大括号、句号和逗号。第三个是是参数化的将 token1 替代为 token2 操作，将光标位置上的 token1 替换为 token2。在这个类中有四个操作：（1）“；”替换为“，”，（2）“，”替换为“；”，（3）“.”替换为“；”，（4）“；）”替换为“）；”。尽管可以用一系列的删除和插入操作替换原子替换操作，但使用它们可以防止组成的删除和/或插入操作被环境拒绝的情况。 episode，termination，and rewards 一个 episode 的 开始 string：一个错误的程序文本 cursor：string 的第一个 token 结束 到达 goal state ：编辑后的程序被编译器成功编译 termination 终止 在一个 episode 中，agent 被允许到达最大数目的 time steps，即 max_episode_len 时终止 在一个 episode 中，agent 只允许通过整个程序一次，即 agent 一旦经过程序的最后一个 token，这个 episode 就终止了 reward 奖励 在每一步，agent 会受到一个小的步长惩罚，一个较高的编辑惩罚 step_penalty 步长惩罚：鼓励 agent 学会在最小步长中来修复程序 edit_penalty 编辑惩罚：编辑操作开销较大，需要调用编译器来验证，所以不鼓励 agent 做不必要的编辑 maximum_reward 最大奖励：agent 达到 goal state intermediate_reward 中间奖励：纠正至少一个错误的编辑操作 model首先，使用 LSTM 网络将 state 的 token 嵌入到实向量中， 最终的 state 是输出向量的各个元素均值。考虑到 state 的嵌入，作者采用两个独立的全连接线性层，来生成策略函数 $\\pi$(a|s; $\\theta$) 和值函数 V(s;w) 。更新网络参数前，计算累积梯度： $H$ 是熵，$\\beta$ 是它的正则化项超参数。 expert demonstrationsRLAssist可以不使用任何演示进行训练，但会耗费更长的训练时间。原因是在A3C算法，每一个 episode 中，一个 agent 从一个随机 state 开始，然后使用它的 policy function 与环境进行交互。由于 policy network 是随机初始化的，因此在训练开始时，这种交互随机进行探索。在只进行随机探索的情况下，agent 会发现很难到达目标状态，也很难获得奖励。因此，训练速度减慢。 作者使用专家演示来加速训练。专家演示是指向事件目标状态的一系列动作。给定一对 (p, p’) ，其中 p 是一个不正确的程序， p’ 是它的正确版本，将自动生成如下的演示。从 p’ 中的第一个 token 开始，每个未修改的行 (w.r.t. p) 都会通过一个向下移动操作跳过。在第一个错误行，光标通过向右移动操作向右移动，直到到达错误位置并生成适当的编辑操作。这个过程一直重复，直到程序中的最后一个错误被解决。我们将 agent 设定为使用以下专家演示。对于可以进行演示的 episodes ，agent 遵循所提供的预定动作序列，而不是由 policy 驱动的 sampling 。对 policy network 参数的更新就像预先确定的动作 sampled 一样。对于其他 episode ，代理将按照标准的 A3C 算法，使用 policy 来 sample 的动作。请注意，演示是在 episode 级别提供的，而不是在更细粒度的转换级别上提供的。因为 agent 需要在整个事件中采取正确的行动来达到目标状态并获得奖励。如果它采取断断续续的指导，那么它仍然无法达到目标状态。 experimentshttps://bitbucket.org/iiscseal/rlassist/src/master/","link":"/2022/03/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2002/"},{"title":"DRL - 01导论","text":"ML 23-1 deep reinforcement learningscenario of deep reinforcement learning learning to play GO Supervised vs Reinforcement applications Gym: https://gym.openai.com/ Universe: https://openai.com/blog/universe/ difficulties of reinforcement learning reward delay 一些没有奖励的动作在当前看起来没有用，但对未来会产生影响，帮助在未来得到奖励。 agent’s actions affect the subsequent data it recevives，agent 需要去探索，不管是好的行为还是坏的。 outline Policy-based Approach - Learning an Actor machine learning $\\approx$ looking for a function 找function 的三大步骤 DRL neural network as actor input: vector、matrix，eg: pixels output: action 采取行动的几率，stochastic goodness of function supervised learning vs DRL pick the best gradient ascent add a baseline critics评估observation Actor-Critic ML 23-2 policy gradient (Supplementary Explanation) ML 23-3 RLinteract with environments 机器学到的行为会影响下一步的发展，所有的action 当成整体看待 componentsenv、reward function不能控制，只能调整actor的行为 critic 评估critic： Monre-Carlo： Temporal defference： Q actor 如果⽆法穷举则会爆炸，采用PDPG pathwise derivative policy gradient Asynchronous A3C imitation learning 类似GAN:","link":"/2022/03/19/DRL%20-%2001/"},{"title":"DRL - 02Proximal Policy Optimization (PPO)","text":"policy gradient on-policy and off-policy add constraint","link":"/2022/03/20/DRL%20-%2002/"},{"title":"DRL - 03Q-learning","text":"introduction of Q-learning Tips of Q-learning Q-learning for Continuous Actions","link":"/2022/03/21/DRL%20-%2003/"},{"title":"DRL - 04Actor-critic","text":"AC A2C A3C pathwise derivative policy gradient","link":"/2022/03/22/DRL%20-%2004/"},{"title":"DRL - 05Sparse Reward","text":"reward shaping curriculum learning hierarchical RL","link":"/2022/03/23/DRL%20-%2005/"},{"title":"DRL - 06Imitation Learning","text":"behavior cloning inverse reinforcement learning","link":"/2022/03/24/DRL%20-%2006/"},{"title":"Review - Transformer","text":"回顾《 attention is all your need 》，transformer 的结构如下图所示，Inputs 包括 embedding 和 positional encodeing，将词嵌入结合位置信息；Encoder 包括 N 个堆叠的层，每个层中的多头注意力机制和前馈神经网络后都进行了残差和归一化连接；Decoder 包括 N 个堆叠的层，与 Encoder 不同的是，它还多了一层 masked multi-head attention；Output 包括简单的线性层和 softmax 。 Inputsembedding 将文本处理为向量，如word embedding。 1234567891011class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): embedds = self.lut(x) return embedds * math.sqrt(self.d_model) # 这里在给词向量添加位置编码之前，扩大词向量的数值目的是让位置编码相对较小。 # 这意味着向词向量添加位置编码时，词向量的原始含义不会丢失。 positional ecoding 添加位置信息，采用正余弦可以避免句子长短不一时对位置带来的影响。 123456789101112131415161718class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # 为防止当1000的幂作为分母导致的float溢出，对公式进行转换 pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # 奇数 pe[:, 1::2] = torch.cos(position * div_term) # 偶数 pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) # self.pe[:, :x.size(1)]取到x的实际长度 return self.dropout(x) 可视化位置信息 Encoderencoder由 N 层堆叠，将层复制 N 次。 12345678910111213def clones(module, N): return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])class Encoder(nn.Module): def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): for layer in self.layers: x = layer(x, mask) return self.norm(x) 构造掩码，这里掩码的作用是屏蔽空白区域，decoder中掩码还有屏蔽未来信息的作用。 1234def subsequent_mask(size): attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0 encoder层包括多头注意力层和前馈全连接层这两个子层，每个子层后面都用归一和残差连接。 12345678910class SublayerConnection(nn.Module): def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): x_norm = self.norm(x + self.dropout(sublayer(x))) # 有的把x提出来加速收敛 x_norm = x + self.norm(self.dropout(sublayer(x))) return x_norm 规范化层 1234567891011class LayerNorm(nn.Module): def __init__(self, feature_size, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(feature_size)) self.b_2 = nn.Parameter(torch.zeros(feature_size)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 1234567891011121314class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): # 多注意力层 x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # 前馈传播层 z = self.sublayer[1](x, self.feed_forward) return z attention层，采用多头注意力机制。 单头注意力中，QK矩阵内积求出相关性系数scores，判断是否使用掩码，对scores进行softmax，乘上V得到输出。 123456789def attention(query, key, value, mask=None, dropout=None): d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim = -1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn 多头注意力，设计多种Q均衡偏差，让词义有多种表达。给每个头分配等量的词特征，四个线性层中有三个分别对应QKV，最后一个是对应拼接后的。 12345678910111213141516171819202122232425class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: mask = mask.unsqueeze(1) nbatches = query.size(0) query, key, value = \\ [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) x = x.transpose(1, 2).contiguous() \\ .view(nbatches, -1, self.h * self.d_k) return self.linears[-1](x) feed forward 层包括两个线性层和一个relu层。 123456789class PositionwiseFeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) Decoderdecoder 根据 encoder 的输出和上一次的预测结果，预测序列的下一个输出，由 N 个相同的层堆叠。 12345678910111213141516171819202122232425class Decoder(nn.Module): def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)class DecoderLayer(nn.Module): def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) Output1234567class Generator(nn.Module): def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return F.log_softmax(self.proj(x), dim=-1) Overview123456789101112131415161718192021222324252627282930313233343536373839class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): memory = self.encode(src, src_mask) res = self.decode(memory, src_mask, tgt, tgt_mask) return res def encode(self, src, src_mask): src_embedds = self.src_embed(src) return self.encoder(src_embedds, src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): target_embedds = self.tgt_embed(tgt) return self.decoder(target_embedds, memory, src_mask, tgt_mask)def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1): c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab)) for p in model.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) return model","link":"/2022/06/12/%E5%A4%8D%E4%B9%A0%20transformer/"},{"title":"Review - Bert","text":"","link":"/2022/06/12/%E5%A4%8D%E4%B9%A0%20bert/"},{"title":"论文阅读 - Associating Natural Language Comment and Source Code Entities","text":"Paper，AAAI-2020摘要comment 是与 source code elements 相关联的自然语言描述，理解明确的关联可以提高代码的可理解性，并保持代码和注释之间的一致性。为了初步实现这个目标，我们解决了实体对齐的任务，关联 Javadoc comments 的实体和 Java source code elements 的实体。我们提出一种方法，从开源项目的修正历史自动提取监督数据，并为这个任务提出了一个手动注释的评估数据集。我们提出二分类和序列标注模型，通过构建一个丰富的特征集，包括了代码、注释、它们之间的关系。实验表明，我们的系统优于所提出的监督学习的几个基线。 疑问1：为什么要实体对齐？怎么对齐？ 疑问2：数据集提取过程？规模？影响力？ 疑问3：模型的细节，特征集怎么构造，基线是什么？ 疑问4：这个工作带来的影响，现在的进展如何？ 介绍自然语言元素用于记录源代码的各个方面，summaries 提供了给定代码片段功能的高级概述， commit messages 描述了在软件项目的两个版本之间进行的代码更改，API comments 定义了代码块的特定属性（如先决条件和返回值）。 每一个都为开发者之间的沟通提供了一种重要的模式，对开发过程的高效性至关重要。这些自然语言元素越来越流行，在自然语言处理社区的 code summarization，commit message generation，code generation 的研究中。 特别的是，人们对结合自然语言注释和源代码的跨模态任务越来越感兴趣。要完成这些任务，必须了解注释中的元素如何与相应的代码中的元素关联。之前的研究中，检测代码和注释之间不一致的工作，合并对特定任务的规则来连接注释组件到代码的各个方面。最近在自动注释生成方面的工作，依赖一种注意力机制隐式的逼近注释中生成某种术语应注意的代码部分。 与这些方法相反，我们制定了一个任务，目的是学习注释中的实体和相应源代码中的元素之间的显式关联。我们相信显式关联会帮助系统为下游应用带来改进。比如在代码和注释生成任务，它们可以作为监督注意机制，并使用显示知识来增强神经网络模型，这往往带来性能的显著提高。此外，这提供了一种进行更加细粒度的代码注释不一致检测的方法，而不是识别完整注释是否与代码体不一致的常见方法。这样的系统可以成为自动化代码注释维护的有价值组件，目的在于使注释与它所描述的代码保持一致。通过提供注释中哪些元素被注释中的给定实体引用的信号，系统可以自动检测到注释中的实体是否与代码不一致。 学习这种关联的第一步，我们关注在 Javadoc @return comments，它用于描述返回类型和依赖于给定方法中各种条件的潜在返回值。我们观察到 @return comments 往往比其他形式的注释更加结构化，使其成为更干净的数据源，因此是所提任务的合理开头。此外，我们观察到注释通常描述给定代码体中的实体和动作，它们映射到自然语言中的名词短语和动词短语。至于 @return comments，它们描述的返回值通常是实体和与实体相关的条件（如输入参数、程序状态）。因为我们在本文关注与这样的评论，我们针对 @return comments 中的名词短语实体作为第一步，如图1、2，在注释中给定名词短语，任务是识别与它们关联的 code tokens 。 然而，学习自动解决注释和代码之间的关联，在数据收集方面具有挑战性。为 code/language tasks 获取注释数据是困难的，因为它需要理解特定编程语言的源代码的专业知识。此外，收集包含源代码和自然语言的高质量并行语料库具有挑战性，因为大型在线代码库中的数据本身就存在噪声。我们提出了一种新的方法，不需要人工注释，利用该平台的提交历史特性，从 GitHub 获得该任务的噪声监督。我们证明这种噪声监督提供了有价值的训练信号。 为今后的研究奠定基础，我们和相对简单的模型设计了一组高度显著的特征。我们提出了两种模型，在噪声数据上训练，在人工标注数据集上评估。第一个是二分类模型，单独的对给定的代码块的每一个元素进行分类，判断它是否与相关注释中的指定名词短语相关联。第二个是序列标记模型，特别是一个条件随机场 CRF 模型，它共同为代码中的元素分配标签，其中标签表示一个元素是否与指定的名词短语相关联。我们设计了一套新颖的特征来捕获上下文表示、余弦相似度以及与编程语言相关的 API 和语法。 在噪声数据上训练，两种模型的表现大大优于基线，二值分类器获得 F1 score 0.677，CRF 获得 F1 score 0.618，分别比基线提高了39.6%和27.4%。我们通过随着噪声训练数据量的增加，模型的性能提升来证明噪声数据的价值性。此外，通过消融研究，我们强调了模型所用的特征的实用性。本文的主要贡献如下： 新任务：关联自然语言的注释和源代码的元素，结合一个人工标注的评估数据集。 一种从软件改变历史和利用监督形式的机器学习系统中获取噪声监督的技术。 一个新的特征集，捕捉代码和注释的特征和它们之间的关系，被用于模型中，可以作为未来工作的 baseline。 任务给定注释中的一个名词短语（NP），其任务是将它和相应代码中的每个候选 code token 之间的关系分类为关联的和不关联的。候选项是code tokens，不包括Java关键字（如try，public，throw），运算符（如=），符号（如[，{）这些元素与编程语言语法相关，通常不会在注释中进行描述，如图中 token int，opcode，currentBC 和NP中的“the current bytecode”相关，但是int，setBCI，_nextBCT不是。该任务和自然语言文本的 anaphora resolution 回指解析有相似之处，包括明确地提到 antecedents 先行词（coreference 共引用）以及关联关系（bridge anaphora 桥接回指）。在这种设置下，注释中被选择的名词短语是 anaphora 隐喻，属于源代码的 tokens 是 candidate antecedents 候选先行词。然而，我们的任务与两者不同，因为它需要对两种不同的模式进行推理。如图1，“problem”显式关联e，但我们需要知道 InterruptedException 是它的类型，这是Exception的一种，Exception是“problem”的一种编程术语。此外，在我们的设置中，注释中的一个NP可以关联源代码中不属于同一个co-reference “chain” 共同引用链的多个不同的元素。由于这些问题，我们将我们的任务广泛的定义为将自然语言注释中的一个名词短语与相应代码体中的单个 code token 关联起来。 在这项工作中，我们使用Java编程语言和Javadoc注释，即 @return 注释。然而，这项任务和方法可以扩展到其他编程语言。例如，Python Docstring和c#XML文档注释也有类似的目的。 数据我们使用Java中结构最好的注释类型，即带有 @return 的注释，它是Javadoc 文档的一部分，如图1、2。 @return 注释描述了输出，输出是由组成方法的各种语句计算的。相比之下，其他Javadoc 标签中的内容通常更加窄，非结构注释往往本质上更长和高级，这使得很难映射到代码中的元素，如补充材料。我们未来的工作将把提出的任务扩展到其他类型的评论。在此，我们提到评论时，指的是@return标签后的内容。 我们通过从Github上流行的开源项目的所有commits中提取示例来构造数据集。我们根据stars量来排序项目，使用了前1000个项目，因为它们被认为质量更高。我们提取的每个示例都包括对方法体的代码更改以及相应的@return注释的更改。 噪声监督我们的噪声监督提取方法的核心思想是利用软件版本控制系统（如Git）的修改历史，基于先前研究表明源代码和评论共同进化。本质上，如果注释中的实体和源代码中的实体同时被edit，则它们相关联的概率会更高，即可近似为同时commit。因此，挖掘这样的共编辑让我们为这个任务获得噪声监督：我们用版本控制系统Git来隔离一起添加和删除的部分代码和注释。 监督设定 添加 部分添加的代码可能和部分同时添加的注释有关联，基于这样的直觉，我们为代码tokens分配了有噪声的标签。也就是说，我们将在给定提交中添加的任何代码tokens标记为与同一提交中的注释中引入的NP相关联的代码，并将所有其他代码tokens标记为与NP无关的代码。这些正标签是有噪声的，因为一个开发者可能同时做其他的与添加的NP无关的代码更改。另一方面，负标签（未关联）具有最小的噪声，因为从以前版本中保留的代码tokens不太可能与以前版本注释中不存在的NP关联。我们从添加的数据中收集的这一组示例构成了我们的主要数据集。 删除 理论上，如果我们假设被删除的代码tokens与从注释中删除的NP相关，我们可以从每个提交中提取一个示例。然而，删除的NPs在这方面比添加的NPs要微妙得多。如上面所说，由于添加的NP在以前的版本中不存在，因此以前存在的代码tokens不太可能与之相关联。但由于被删除的NP确实存在于以前的版本中，因此我们不能完全地声称一个在代码中的不同版本之间保持不变的token与删掉的NP没有关联。这将可能会导致更多的负标签噪声，除了固有存在的正标签噪声。如图3，nextBCI被自动标记为与被删除的NP“下一个字节码”无关，即使它可以说是关联的。因此，我们将这些示例从我们的主数据集中分离出来，并形成另一组我们称为删除数据集的示例。 数据处理 我们在提交中检查代码和注释的两个版本：提交之前和提交之后。使用spaCy，我们从两个版本的注释中提取NPs，并使用javalang库，对代码的两个版本进行tokenize。使用difflib库，我们计算了两个版本的注释中的NPs之间的差异，以及两个版本的tokenized代码序列之间的差异。这些差异的每一行变化都用正负号标记，如图3所示。 从不同的结果中，我们分别识别了之前和之后的注释和代码版本中唯一的NPs和code tokens，允许我们构建两对(NPs，相关的code tokens)。一个是删除的案例，删除部分的注释和代码只出现在先前的版本中。一个是添加的案例，添加部分的注释和代码只出现在新版本中。 如果提取的NPs或相关code tokens列表为空，我们丢弃这对。此外，我们丢弃由多个NP组成的对，以获得不模糊的训练数据，以确定哪些code tokens应该与哪个NP相关联。因此，最后的对形式是(NP，相关的code tokens)。注意，对于关联的code tokens中的任何token，如果它不是一个通用的Java类型(例如，int，String)，我们会用关联到相同文字字符串来处理code tokens中任何其他token。 然后，我们回到前后版本的代码(不包括Java关键字、操作符和符号，参照第2节)。我们将代码序列tokenize，并将任何不存在在关联code tokens中的tokens标记为不关联。按照这个过程，每个示例都由一个NP和一个被标记的code tokens序列组成。从以前版本（以前）提取的示例将添加到删除数据集中，从新版本（之后）提取的示例被添加到主（添加）数据集中。 数据过滤 虽然大型代码基地如Github和StackOverflow提供了大量数据，为源代码和自然语言任务获取海量高质量的并行数据仍是一个挑战，由于显著的噪声和代码重复等原因。先前的工作通过使用人工标记数据训练的分类器来过滤低质量的数据来解决这个问题。但是，手动获取数据是很困难的，我们选择用启发式，如之前的工作，我们施加约束来过滤掉噪声数据，包括重复、细碎的场景，和不相干的代码与注释更改组成的示例数据。 我们定义细碎场景为涉及到由几个都关联到这个NP的code tokens组成的单行方法的示例，还有那些用简单字符串匹配工具就可以解决关联的示例。 此外，在手动检查了大约200个示例的样本后，我们建立了启发式来最小化不相干代码注释更改的示例数量： 那些有冗长的方法或大量代码更改，这些更改可能不都与注释相关。 与重新格式化、更正错误修复和简单改述的相关代码和注释变动的示例。 注释变化包括动词短语的示例，因为相关的代码变化可能与这些短语有关，而不是NP。 此外，由于我们关注的是描述Java方法返回值的@return标记，因此我们消除了不包括返回类型更改或至少一个返回语句的代码更改示例。具体参数和每个启发式丢弃的示例数量见补充材料。 应用这种启发式方法大大减少了数据集的大小。然而，我们在手动检查200个例子并观察到显著的噪声后，确定这种过滤是必要的，并发现这与上述之前的工作一致，表明了在大的代码库中，如果没有积极的过滤和预处理，就无法从中学习。 经过过滤后，我们将主数据集划分为训练集、测试集和验证集，如表1所示。基于训练集，NP的中位数是2，四分位数范围(IQR，差异在25%和75%的百分位)1，code token的中位数25,IQR 21，相关的code token是10,IQR 13。我们报告了IQR，因为这些分布不是正态的。 测试集 测试集中的117个示例是由一位有7年Java使用经验的作者进行注释的。在试验研究中，两个注释者在集中专注于注释的标准之前，共同检查了一个用于注释的方法/注释对的样本集。用于识别相关的code tokens的标准包括：它是否由NP直接引用；它是与NP引用的实体对应的属性、类型或方法；它被设置为等于NP引用的实体；如果令牌被更改，则需要更新NP。有关注释的示例，请参见补充材料。为了评估注释的质量，我们询问了一个不是作者之一且有5年Java经验的研究生，注释286个代码标记（来自测试集中的25个示例），这些标记在嘈杂的监督下被标记为相关的。两组注释之间的Cohen’s kappa得分为0.713，表明一致性令人满意。 表示和特征我们设计了一组特征，包括表面特征，单词表示，代码标记表示，余弦相似性、代码结构和java api。我们的模型利用通过连接这些特征而得到的1852维特征向量。 surface feature 表面特征 我们合并了两个binary features，subtokens matching 和返回语句中的 presence，这些也被用于下节讨论的baseline。subtoken matching feature 表示一个候选code token完全匹配给定名词短语的组件，在给定token-level或者subtoken-level。subtokenization 是指java中常用的分裂驼峰式大小写，返回语句的presence feature表示候选code token是否出现返回语句还是完全匹配在返回语句出现的任何token。 单词和代码表示 为了在注释和代码中获得术语表示，我们预先训练了注释的character-level和word-level嵌入表示，代码的character-level、subtoken-level和token-level的嵌入表示。这些128维度的嵌入在大语料库上训练，由GitHub中提取的128,168个@return标签/Java方法对组成。训练前的任务是使用单层、单向的SEQ2SEQ模型为Java方法生成@return注释。我们使用平均嵌入来获得NP和候选代码标记的表示。此外，为了提供一个有意义的上下文，我们对完整@return注释对应的嵌入以及候选令牌出现的同一行中标记对应的嵌入取平均值。 余弦相似性 最近的工作使用代码和自然语言描述组成的对的联合向量空间，表明一个代码体和它相关描述有相似的向量。由于@return注释的内容经常在代码中提到实体，不是建立一个联合的向量空间，而是通过计算关于java代码训练的嵌入向量表示，我们将NP投影到代码的相同向量空间。然后我们计算NP和候选code token的余弦相似性，在token-level，subtoken-level，character-level。类似的计算NP和候选code tokens出现的代码行的余弦相似性。 代码结构 抽象语法树捕捉给定代码体在树形式的语法结构，由Java语法定义。使用javalang的AST解析器，我们获得了该method对应的AST。为了表示候选code token相对于该method的整体结构属性，我们提取其父节点和祖父节点的节点类型，用one-hot编码来表示它们。这在method的更广泛的内容的候选code token作用，提供了更深的了解，通过传递详细的信息，如是否在方法调用中、变量声明、循环、参数、try/catch block等等。 java api 我们用one-hot编码来表示与通用Java类型和java.util包（一个实用程序类的集合，如List，我们往往会经常使用）。我们假设这些特征可以揭示这些经常出现的tokens的显示形式。为了捕捉本地文本，我们还包括了与候选code token相邻的code token的java相关特性，例如它是通用的Java类型还是Java关键字之一。 模型我们开发了代表不同的方式来解决我们提出的任务的两种模型：二进制分类和序列标记。我们还制定了多个基于规则的baseline。 二分类器 给定一个code token序列和注释中的一个NP，我们独立地将每个标记分类为关联或不关联。我们的分类器是一个前馈神经网络，有4个全连接层和最后一个最终输出层。作为输入，网络接受一个与候选code token对应的特征向量（在前一节中讨论），并且模型输出对该token的二值预测。在实验中，我们还用了逻辑回归模型作为分类器，但它的表现不如神经网络。 序列标记 给定一个code token序列和注释中的一个NP，我们对代码序列的tokens是否与NP相关联进行一起分类。以这种方式构建问题背后的直觉是，对给定代码标记的分类通常依赖于对附近标记的分类。例如，在图3c中，表示next()函数的返回类型的int的token与指定的NP没有关联，而与opcode相邻的int的token被认为是关联的，因为opcode是关联的，而int是它的类型。 为了重新建立原始序列的连续顺序，我们将已删除的Java关键字和符号注入回序列中，并引入第三个类作为这些插入标记的黄金标签。具体来说，我们预测了这三个标签：关联的、非关联的和一个伪标签Java。请注意，我们在评估过程中忽略了这些令牌的分类，也就是说，如果在测试时预测了任何code token标记为伪标签，我们会自动将其分配为不关联（平均而言，这种情况的约为1%）。我们构建一个CRF模型，通过在前馈神经网络的前面加上神经CRF层，类似于二元分类器的结构，让网络接受一个由method中的所有tokens的特征向量组成的矩阵。在实验中，我们还用了非神经网络的CRF，采用sklearn-crfsuite，但它的表现不如神经网络。 模型参数 四个全连接层有512，384，256，126个units，每个被dropout的概率是0.2。如果在5个连续epochs（10epochs之后），F1 score没有改进，我们就终止训练。我们使用最高F1 score的模型。我们用tensorflow实现了这两种模型。 baselines Random 基于均匀分布的code token的随机分类。 weighted random 根据从训练集中观察到的相关类和非相关类的概率分别为42.8%和57.2%，对代码标记进行随机分类。 subtoken matching 将subtoken matching表面特征（在上一节中介绍）设置为true的任何token都被分类为关联，而所有其他token都被分类为不关联。请注意，永远不会有这种情况出现，所有相关的code token在token-level或subtoken-level与NP匹配。在过滤过程中，我们从数据集中删除了这些琐碎的例子，因为它们可以用简单的字符串匹配工具来解决，而不是本工作的重点。 presence in return statement 将返回语句表面特征（在上一节中讨论）设置为true的任何token都被分类为关联，所有其他token都被分类为未关联。 结果采用micro-level precision，recall，F1 metrics评估模型。在token-level上，测试集中有3592NP-code token pairs。所有报告的分数都是三次运行取平均。下面讨论在主训练集上训练的结果，将删除数据集纳入训练的结果，二值分类器和CRF模型使用的特征的消融研究结果。 主训练集上训练结果 三个baselines和我们的模型的结果如表2所示。我们的分析主要基于注释测试集上的结果，为了完整性，我们展示了来自未注释集的结果。相对于未注释集的分数，模型通过注释集获得较低的精度分数和较高的召回分数。这是意料之中的，因为在注释过程中，关联有黄金标签的令牌数量减少了。 我们的两个模型的表现都大大优于baseline。从二进制分类器中获得的样本输出见补充材料。虽然CRF的召回率得分略高于二值分类器，但很明显，二值分类器总体上优于F1得分。这可能是由于CRF需要额外的参数来建模依赖关系，这可能无法准确设置，因为我们的实验设置中示例级数据数量有限。此外，虽然我们期望CRF比二值分类器更对上下文敏感，但我们确实将二值分类器结合了许多上下文特征(周围和邻近标记的嵌入、上下文与NP的相似性以及相邻标记的Java API知识)。我们发现有错误分析，CRF模型倾向于在Java关键字之后的令牌以及在稍后出现的方法中的令牌上犯错误。这表明CRF模型可能难以对更长范围的依赖关系和更长范围的序列进行推理。此外，与二进制分类设置相比，Java关键字出现在序列标记设置，因此CRF模型必须比二进制分类器推理更多的code tokens。 使用删除功能来增强训练 我们通过从删除数据集中分阶段添加数据来增加训练集。在这些新的补充数据集上训练二值分类器和CRF的结果如表3所示。对于二值分类器，添加500和867个被删除的示例似乎对F1有显著的提升，而对于CRF模型，添加任何数量的被删除的示例都会提高性能。这表明，我们的模型可以从我们认为比我们收集的主要训练集更有噪声的数据中学习。由于我们能够在添加的情况以及对应的删除的情况下找到价值，我们能够大大增加可以收集来训练执行我们提议的任务的模型的数据量的上限。考虑到为这项任务获得大量高质量的数据是多么困难，这尤其令人鼓舞。尽管我们从超过1,000个项目的所有提交的源代码文件中的方法中提取了示例，在过滤噪声后，我们只从添加的案例中获得了总共970个例子。通过包括已删除案例中的867个例子，我们将这个数字增加到1837个。虽然这仍然是一个相对较小的数字，但我们预计随着任务范围扩展到本文中关注的@return之外的其他评论，潜在的规模将大幅增加。 消融研究 我们对在主数据集上训练的二元分类器进行了消融研究，以分析我们所引入的特征的影响。我们消除了余弦相似性，嵌入，以及和java相关的特性。嵌入特性包括代码嵌入（即对应于候选代码标记的嵌入和方法行中的标记）和注释嵌入(即对应于NP和@retern注释的嵌入)。6根据表4所示的结果，相对于f1度量，所有这些特征都对完整模型的性能做出了积极的贡献。 相关工作之前的工作研究了一项任务，包括将对话系统中的名词短语接地到编程环境中(Li和Boyer 2015；Li和Boyer2016)。名词短语是从学生和导师之间的互动中提取出来的，而编程环境承载着学生的代码。他们的工作性质类似于共引用解析，因为目标是识别编程环境中被给定名词短语引用的实体。在对话中，学生和导师讨论与代码中特定实体相关的实现细节，这使得共同引用解析成为框架任务的适当方式。相比之下，他们的工作主题——伴随源代码的注释——通常描述高级功能，而不是实现细节。由于代码中的多个组件相互作用以组成功能，因此代码中可能存在由注释中的给定元素直接或间接引用的实体。由于它们的数据和实现无法公开获得，因此我们无法对这些任务和方法进行任何进一步的比较。 Fluri、W¨ursch和Gall（2007）研究了一个变体任务，即基于距离度量和其他简单启发式方法将单个源代码组件（如类、方法、语句）映射到行或块注释。相比之下，我们的方法在更细粒度的级别上处理代码和注释——我们在令牌级别上对代码进行建模，并在注释中考虑NPs。此外，在我们的框架下，多个代码标记可以映射到同一个NP，并且这些映射是从从更改中提取的数据中学习到的。 Liu等人（2018）介绍了一项任务，该任务涉及将单个提交消息中包含的不同更改意图链接到在提交中发生更改的软件项目中的源代码文件。虽然这需要将自然语言消息中的组件与源代码关联起来，就像我们提出的任务一样，但我们感兴趣的关联占据了更高的粒度。也就是说，我们关注评论中的NPs，而他们的工作是关注提交消息中的句子和子句，在我们的例子中，分类单位是每个单独的代码标记，而在他们的工作中是每个文件。此外，在它们的工作中构建的数据集提取已经更改并提交消息的源代码文件，这些文件是为每个提交新编写的。相反，对于我们的任务，我们收集了包含源代码和注释中的更改的例子，这些源代码和注释是共同发展的。 我们从Git版本控制系统中提取示例的过程与Faruqui等人（2018）基于维基百科的编辑历史建立一个维基百科编辑语料库的方法类似。他们从维基百科文章中的句子中连续文本的插入中提取样本，这些例子有望展示自然语言文本通常是如何被编辑的。相比之下，我们不仅仅限制插入或要求编辑是连续的。此外，我们努力收集一些演示两种模式如何一起编辑的例子。 总结在本文中，我们制定了将Javadoc注释中的实体与Java源代码中的元素关联起来的任务。我们提出了一种新的方法来获得有噪声的监督，并提出了一组丰富的特性，旨在捕获代码、注释和它们之间的关系等方面。基于在一个手动标记的测试集上进行的评估，我们表明，在这样的噪声数据上训练的两个不同的模型可以显著优于多个基线。此外，我们通过展示增加有噪声训练数据的大小如何提高性能，证明了从有噪声数据中学习的潜力。我们还通过消融研究强调了我们的特征集的价值。 Code and datasetsenv：py2.7，tnsorflow-gpu=1.15.0待改进： 数据集太小 模型太简单，baseline太弱","link":"/2022/08/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2003/"},{"title":"NLP - 01Introduction and Word Vectors","text":"词向量将词编码为向量可以在词空间中表示为一个点，词空间的维度实际上可能比词数量要小，就足以编码所有单词的语义。one-hot vector：将每个词表示为含有0和1的向量，$R^{|V|·1}$，V为词汇表的大小。这样的词表示没给我们直接的相似性概念，每个词都是线性无关的，所以尝试减小空间来找到一个子空间编码单词之间的关系。 Denotational semantics: 用符号（独热向量）表示，是稀疏的不能捕获相似性。 Distributional semantics: 根据上下文表示单词的含义，密度大可以很好的捕捉相似性。 SVD based methods这类的方法是找到一个词嵌入，我们先循环一遍数据集，用矩阵X累计单词共现的频率，然后在X上进行奇异值分解，得到$USV^T$，用U的行作为字典里所有单词的词嵌入。下面讨论几种X的选择。 Using Word-Word Co-occurrenceMatrix: Generate |V| × |V| co-occurrencematrix, X. Apply SVD on X to get X = $USV^T$. Select the first k columns of U to get a k-dimensional word vectors. $\\frac{\\sum^{k}{i=1}\\sigma{i}}{\\sum^{|V|}{i=1}\\sigma{i}}$ indicates the amount of variance captured by the first k dimensions.Word-Document Matrix 猜测：有联系的单词经常出现在相同的文档里 构建X：循环非常多的文档，每次单词i出现在文档j里，我们将它添加到$X_{ij}$，维度为$|V|·M$，随着文档数增加。 Window based Co-occurrence Matrix计算每个单词出现在感兴趣的单词的周围的特定大小滑动窗口中的次数。 对共现矩阵使用SVD对X执行SVD，观察奇异值，S矩阵的对角项，根据捕获的期望百分比方差，在某个索引k处切断它们。 基于统计的方法有很多问题：矩阵的维数经常发生变化；矩阵稀疏；高维；计算代价；单词频率不平衡 Iteration Based Methods - Word2vec我们尝试一种新的方法，不是计算存储关于大数据集的全局信息，我们尝试构建一个模型，有能力一次学习一个迭代，最终能够对给定上下文的单词的概率进行编码。 Iteration-based methods：每次捕捉单词的共现，而不是SVD直接捕获所有的共现 Word2vec is a software package that actually includes : 2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary. 语言模型","link":"/2022/09/16/NLP%20-%2001/"},{"title":"pytorch - 基础知识","text":"","link":"/2022/10/10/pytorch-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"title":"论文阅读 - Contrastive Code Representation Learning","text":"摘要","link":"/2022/09/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2004/"},{"title":"论文阅读 - code2vec Learning Distributed Representations of Code","text":"泛读我们提出了一个神经网络模型，将代码片段表示为连续分布向量（代码嵌入）。其主要思想是将一个代码片段表示为一个固定长度的代码向量，可用于预测代码片段的语义属性。为此，首先将代码分解为其抽象语法树中的路径集合。然后，网络学习每条路径的原子表示，同时学习如何聚合其中的一组路径。 通过用从它身上的向量表示来预测方法名称，来证明我们方法的有效性。","link":"/2022/09/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2005/"}],"tags":[{"name":"DRL","slug":"DRL","link":"/tags/DRL/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"Intelligence Code","slug":"Intelligence-Code","link":"/tags/Intelligence-Code/"},{"name":"Draft","slug":"Draft","link":"/tags/Draft/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"}],"categories":[{"name":"杂","slug":"杂","link":"/categories/%E6%9D%82/"},{"name":"数学","slug":"数学","link":"/categories/%E6%95%B0%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"论文阅读","slug":"论文阅读","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"深度强化学习","slug":"深度强化学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"Pytorch","slug":"Pytorch","link":"/categories/Pytorch/"}]}