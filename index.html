<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>🐏&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="🐏&#039;s Blog"><meta name="msapplication-TileImage" content="/img/logo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="🐏&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="🐏&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="🐏&#039;s Blog"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Yang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"🐏's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Yang"},"publisher":{"@type":"Organization","name":"🐏's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/logo.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 6.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="🐏&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Yang-Emily/Yang-Emily.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-22T06:43:54.000Z" title="2022/10/22 下午2:43:54">2022-10-22</time>发表</span><span class="level-item"><time dateTime="2022-10-21T17:12:18.246Z" title="2022/10/22 上午1:12:18">2022-10-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">9 分钟读完 (大约1366个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/22/pytorch-%E5%8F%AF%E8%A7%86%E5%8C%96/">pytorch - 可视化</a></h1><div class="content"><h1 id="可视化网络结构"><a href="#可视化网络结构" class="headerlink" title="可视化网络结构"></a>可视化网络结构</h1><ol>
<li>使用print函数打印模型基础信息<br>只能得出基础构件的信息，既不能显示出每一层的shape，也不能显示对应参数量的大小<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">model = models.resnet18()</span><br></pre></td></tr></table></figure></li>
<li>使用torchinfo可视化网络结构<br>输出结构化的更详细的信息，包括模块信息（每一层的类型、输出shape和参数量）、模型整体的参数量、模型大小、一次前向或者反向传播需要的内存大小等<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">resnet18 = models.resnet18() <span class="comment"># 实例化模型</span></span><br><span class="line">summary(resnet18, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)) <span class="comment"># 1：batch_size 3:图片的通道数 224: 图片的高宽</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="CNN可视化"><a href="#CNN可视化" class="headerlink" title="CNN可视化"></a>CNN可视化</h1><ol>
<li>CNN卷积核可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg11</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dict</span>(model.features.named_children()))</span><br><span class="line">conv1 = <span class="built_in">dict</span>(model.features.named_children())[<span class="string">&#x27;3&#x27;</span>]</span><br><span class="line">kernel_set = conv1.weight.detach()</span><br><span class="line">num = <span class="built_in">len</span>(conv1.weight.detach())</span><br><span class="line"><span class="built_in">print</span>(kernel_set.shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num):</span><br><span class="line">    i_kernel = kernel_set[i]</span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">17</span>))</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(i_kernel)) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> idx, filer <span class="keyword">in</span> <span class="built_in">enumerate</span>(i_kernel):</span><br><span class="line">            plt.subplot(<span class="number">9</span>, <span class="number">9</span>, idx+<span class="number">1</span>) </span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">            plt.imshow(filer[ :, :].detach(),cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li>CNN特征图可视化方法<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hook</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.module_name = []</span><br><span class="line">        self.features_in_hook = []</span><br><span class="line">        self.features_out_hook = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self,module, fea_in, fea_out</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;hooker working&quot;</span>, self)</span><br><span class="line">        self.module_name.append(module.__class__)</span><br><span class="line">        self.features_in_hook.append(fea_in)</span><br><span class="line">        self.features_out_hook.append(fea_out)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_feature</span>(<span class="params">model, idx, inputs</span>):</span></span><br><span class="line">    hh = Hook()</span><br><span class="line">    model.features[idx].register_forward_hook(hh)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># forward_model(model,False)</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    _ = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(hh.module_name)</span><br><span class="line">    <span class="built_in">print</span>((hh.features_in_hook[<span class="number">0</span>][<span class="number">0</span>].shape))</span><br><span class="line">    <span class="built_in">print</span>((hh.features_out_hook[<span class="number">0</span>].shape))</span><br><span class="line">    </span><br><span class="line">    out1 = hh.features_out_hook[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    total_ft  = out1.shape[<span class="number">1</span>]</span><br><span class="line">    first_item = out1[<span class="number">0</span>].cpu().clone()    </span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">17</span>))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ftidx <span class="keyword">in</span> <span class="built_in">range</span>(total_ft):</span><br><span class="line">        <span class="keyword">if</span> ftidx &gt; <span class="number">99</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ft = first_item[ftidx]</span><br><span class="line">        plt.subplot(<span class="number">10</span>, <span class="number">10</span>, ftidx+<span class="number">1</span>) </span><br><span class="line">        </span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        <span class="comment">#plt.imshow(ft[ :, :].detach(),cmap=&#x27;gray&#x27;)</span></span><br><span class="line">        plt.imshow(ft[ :, :].detach())</span><br></pre></td></tr></table></figure></li>
<li>CNN class activation map可视化方法<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg11,resnet18,resnet101,resnext101_32x8d</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=<span class="literal">True</span>)</span><br><span class="line">img_path = <span class="string">&#x27;./dog.png&#x27;</span></span><br><span class="line"><span class="comment"># resize操作是为了和传入神经网络训练图片大小一致</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path).resize((<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 需要将原始图片转为np.float32格式并且在0-1之间 </span></span><br><span class="line">rgb_img = np.float32(img)/<span class="number">255</span></span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_grad_cam <span class="keyword">import</span> GradCAM,ScoreCAM,GradCAMPlusPlus,AblationCAM,XGradCAM,EigenCAM,FullGrad</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.model_targets <span class="keyword">import</span> ClassifierOutputTarget</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.image <span class="keyword">import</span> show_cam_on_image</span><br><span class="line"></span><br><span class="line">target_layers = [model.features[-<span class="number">1</span>]]</span><br><span class="line"><span class="comment"># 选取合适的类激活图，但是ScoreCAM和AblationCAM需要batch_size</span></span><br><span class="line">cam = GradCAM(model=model,target_layers=target_layers)</span><br><span class="line">targets = [ClassifierOutputTarget(preds)]   </span><br><span class="line"><span class="comment"># 上方preds需要设定，比如ImageNet有1000类，这里可以设为200</span></span><br><span class="line">grayscale_cam = cam(input_tensor=img_tensor, targets=targets)</span><br><span class="line">grayscale_cam = grayscale_cam[<span class="number">0</span>, :]</span><br><span class="line">cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(cam_img))</span><br><span class="line">Image.fromarray(cam_img)</span><br></pre></td></tr></table></figure></li>
<li>使用FlashTorch快速实现CNN可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download example images</span></span><br><span class="line"><span class="comment"># !mkdir -p images</span></span><br><span class="line"><span class="comment"># !wget -nv \</span></span><br><span class="line"><span class="comment">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/great_grey_owl.jpg \</span></span><br><span class="line"><span class="comment">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/peacock.jpg   \</span></span><br><span class="line"><span class="comment">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg    \</span></span><br><span class="line"><span class="comment">#    -P /content/images</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> flashtorch.utils <span class="keyword">import</span> apply_transforms, load_image</span><br><span class="line"><span class="keyword">from</span> flashtorch.saliency <span class="keyword">import</span> Backprop</span><br><span class="line"></span><br><span class="line">model = models.alexnet(pretrained=<span class="literal">True</span>)</span><br><span class="line">backprop = Backprop(model)</span><br><span class="line"></span><br><span class="line">image = load_image(<span class="string">&#x27;/content/images/great_grey_owl.jpg&#x27;</span>)</span><br><span class="line">owl = apply_transforms(image)</span><br><span class="line"></span><br><span class="line">target_class = <span class="number">24</span></span><br><span class="line">backprop.visualize(owl, target_class, guided=<span class="literal">True</span>, use_gpu=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> flashtorch.activmax <span class="keyword">import</span> GradientAscent</span><br><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">g_ascent = GradientAscent(model.features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># specify layer and filter info</span></span><br><span class="line">conv5_1 = model.features[<span class="number">24</span>]</span><br><span class="line">conv5_1_filters = [<span class="number">45</span>, <span class="number">271</span>, <span class="number">363</span>, <span class="number">489</span>]</span><br><span class="line"></span><br><span class="line">g_ascent.visualize(conv5_1, conv5_1_filters, title=<span class="string">&quot;VGG16: conv5_1&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="使用TensorBoard可视化训练过程"><a href="#使用TensorBoard可视化训练过程" class="headerlink" title="使用TensorBoard可视化训练过程"></a>使用TensorBoard可视化训练过程</h1><ol>
<li>TensorBoard安装</li>
<li>TensorBoard可视化的基本逻辑</li>
<li>TensorBoard的配置与启动</li>
<li>TensorBoard模型结构可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">32</span>,kernel_size = <span class="number">3</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size = <span class="number">2</span>,stride = <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>,out_channels=<span class="number">64</span>,kernel_size = <span class="number">5</span>)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line">writer.add_graph(model, input_to_model = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li>TensorBoard图像可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_test = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">train_data = datasets.CIFAR10(<span class="string">&quot;.&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_train)</span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&quot;.&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform_test)</span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">images, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 仅查看一张图片</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images[0]&#x27;</span>, images[<span class="number">0</span>])</span><br><span class="line">writer.close()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将多张图片拼接成一张图片，中间用黑色网格分割</span></span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;image_grid&#x27;</span>, img_grid)</span><br><span class="line">writer.close()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将多张图片直接写入</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line">writer.add_images(<span class="string">&quot;images&quot;</span>,images,global_step = <span class="number">0</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li>TensorBoard连续变量可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    x = i</span><br><span class="line">    y = x**<span class="number">2</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;x&quot;</span>, x, i) <span class="comment">#日志中记录x在第step i 的值</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y&quot;</span>, y, i) <span class="comment">#日志中记录y在第step i 的值</span></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">writer1 = SummaryWriter(<span class="string">&#x27;./pytorch_tb/x&#x27;</span>)</span><br><span class="line">writer2 = SummaryWriter(<span class="string">&#x27;./pytorch_tb/y&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    x = i</span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    writer1.add_scalar(<span class="string">&quot;same&quot;</span>, x, i) <span class="comment">#日志中记录x在第step i 的值</span></span><br><span class="line">    writer2.add_scalar(<span class="string">&quot;same&quot;</span>, y, i) <span class="comment">#日志中记录y在第step i 的值</span></span><br><span class="line">writer1.close()</span><br><span class="line">writer2.close()</span><br></pre></td></tr></table></figure></li>
<li>TensorBoard参数分布可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建正态分布的张量模拟参数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span>(<span class="params">mean, std</span>):</span></span><br><span class="line">    t = std * torch.randn((<span class="number">100</span>, <span class="number">20</span>)) + mean</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"> </span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb/&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> step, mean <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>)):</span><br><span class="line">    w = norm(mean, <span class="number">1</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&quot;w&quot;</span>, w, step)</span><br><span class="line">    writer.flush()</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li>服务器端使用TensorBoard</li>
<li>总结<br>TensorBoard的基本逻辑就是文件的读写逻辑，写入想要可视化的数据，然后TensorBoard自己会读出来。<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%83%E7%AB%A0/index.html">深入浅出PyTorch</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-20T06:43:54.000Z" title="2022/10/20 下午2:43:54">2022-10-20</time>发表</span><span class="level-item"><time dateTime="2022-10-19T16:59:07.090Z" title="2022/10/20 上午12:59:07">2022-10-20</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">21 分钟读完 (大约3084个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/20/pytorch-%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/">pytorch - 训练技巧</a></h1><div class="content"><h1 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h1><ol>
<li>以函数定义<br>简单直接<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_loss</span>(<span class="params">output, target</span>):</span></span><br><span class="line">    loss = torch.mean((output - target)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></li>
<li>以类定义<br>更加常用，继承自nn.Module，可以当成神经网络的一层，使用tensor可以自动求导<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiceLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,weight=<span class="literal">None</span>,size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DiceLoss,self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,inputs,targets,smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        intersection = (inputs * targets).<span class="built_in">sum</span>()                   </span><br><span class="line">        dice = (<span class="number">2.</span>*intersection + smooth)/(inputs.<span class="built_in">sum</span>() + targets.<span class="built_in">sum</span>() + smooth)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - dice</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方法    </span></span><br><span class="line">criterion = DiceLoss()</span><br><span class="line">loss = criterion(<span class="built_in">input</span>,targets)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiceBCELoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DiceBCELoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        intersection = (inputs * targets).<span class="built_in">sum</span>()                     </span><br><span class="line">        dice_loss = <span class="number">1</span> - (<span class="number">2.</span>*intersection + smooth)/(inputs.<span class="built_in">sum</span>() + targets.<span class="built_in">sum</span>() + smooth)  </span><br><span class="line">        BCE = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        Dice_BCE = BCE + dice_loss</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Dice_BCE</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IoULoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(IoULoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        intersection = (inputs * targets).<span class="built_in">sum</span>()</span><br><span class="line">        total = (inputs + targets).<span class="built_in">sum</span>()</span><br><span class="line">        union = total - intersection </span><br><span class="line">        </span><br><span class="line">        IoU = (intersection + smooth)/(union + smooth)</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - IoU</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ALPHA = <span class="number">0.8</span></span><br><span class="line">GAMMA = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FocalLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        BCE = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        BCE_EXP = torch.exp(-BCE)</span><br><span class="line">        focal_loss = alpha * (<span class="number">1</span>-BCE_EXP)**gamma * BCE</span><br><span class="line">                       </span><br><span class="line">        <span class="keyword">return</span> focal_loss</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="动态调整学习率"><a href="#动态调整学习率" class="headerlink" title="动态调整学习率"></a>动态调整学习率</h1><ol>
<li><p>使用scheduler<br>PyTorch在torch.optim.lr_scheduler封装好了一些动态调整学习率的方法。</p>
<ul>
<li>lr_scheduler.LambdaLR</li>
<li>lr_scheduler.MultiplicativeLR</li>
<li>lr_scheduler.StepLR</li>
<li>lr_scheduler.MultiStepLR</li>
<li>lr_scheduler.ExponentialLR</li>
<li>lr_scheduler.CosineAnnealingLR</li>
<li>lr_scheduler.ReduceLROnPlateau</li>
<li>lr_scheduler.CyclicLR</li>
<li>lr_scheduler.OneCycleLR</li>
<li>lr_scheduler.CosineAnnealingWarmRestarts</li>
</ul>
<p> 将scheduler.step()放在optimizer.step()后面进行使用。<br> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择一种优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(...) </span><br><span class="line"><span class="comment"># 选择上面提到的一种或多种动态调整学习率的方法</span></span><br><span class="line">scheduler1 = torch.optim.lr_scheduler.... </span><br><span class="line">scheduler2 = torch.optim.lr_scheduler....</span><br><span class="line">...</span><br><span class="line">schedulern = torch.optim.lr_scheduler....</span><br><span class="line"><span class="comment"># 进行训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 需要在优化器参数更新之后再动态调整学习率</span></span><br><span class="line">    scheduler1.step() </span><br><span class="line">    ...</span><br><span class="line">    schedulern.step()</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>自定义scheduler</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span></span><br><span class="line">    lr = args.lr * (<span class="number">0.1</span> ** (epoch // <span class="number">30</span>)) <span class="comment"># 学习率每30轮下降为原来的1/10</span></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = <span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    adjust_learning_rate(optimizer,epoch)        </span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="模型微调-torchvision"><a href="#模型微调-torchvision" class="headerlink" title="模型微调-torchvision"></a>模型微调-torchvision</h1><ol>
<li>模型微调-torchvision<ul>
<li>在源数据集上预训练一个源模型</li>
<li>创建一个新的目标模型，复制源模型除了输出层外的所有结构，其参数学习到了源数据集的知识，假设其同样适用于目标数据集，且源模型输出层和源数据集标签密切相关，所以目标模型不采用它。</li>
<li>为目标层添加目标数据集类别个数的输出层，随机初始化模型参数。</li>
<li>在目标数据集上训练目标模型，从头训练输出层，其他层参数微调。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/finetune.png"></li>
</ul>
</li>
<li>使用已有模型结构<ul>
<li>实例化网络 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18()</span><br><span class="line"><span class="comment"># resnet18 = models.resnet18(pretrained=False)  等价于与上面的表达式</span></span><br><span class="line">alexnet = models.alexnet()</span><br><span class="line">vgg16 = models.vgg16()</span><br><span class="line">squeezenet = models.squeezenet1_0()</span><br><span class="line">densenet = models.densenet161()</span><br><span class="line">inception = models.inception_v3()</span><br><span class="line">googlenet = models.googlenet()</span><br><span class="line">shufflenet = models.shufflenet_v2_x1_0()</span><br><span class="line">mobilenet_v2 = models.mobilenet_v2()</span><br><span class="line">mobilenet_v3_large = models.mobilenet_v3_large()</span><br><span class="line">mobilenet_v3_small = models.mobilenet_v3_small()</span><br><span class="line">resnext50_32x4d = models.resnext50_32x4d()</span><br><span class="line">wide_resnet50_2 = models.wide_resnet50_2()</span><br><span class="line">mnasnet = models.mnasnet1_0()</span><br></pre></td></tr></table></figure></li>
<li>传递pretrained参数 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">alexnet = models.alexnet(pretrained=<span class="literal">True</span>)</span><br><span class="line">squeezenet = models.squeezenet1_0(pretrained=<span class="literal">True</span>)</span><br><span class="line">vgg16 = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">densenet = models.densenet161(pretrained=<span class="literal">True</span>)</span><br><span class="line">inception = models.inception_v3(pretrained=<span class="literal">True</span>)</span><br><span class="line">googlenet = models.googlenet(pretrained=<span class="literal">True</span>)</span><br><span class="line">shufflenet = models.shufflenet_v2_x1_0(pretrained=<span class="literal">True</span>)</span><br><span class="line">mobilenet_v2 = models.mobilenet_v2(pretrained=<span class="literal">True</span>)</span><br><span class="line">mobilenet_v3_large = models.mobilenet_v3_large(pretrained=<span class="literal">True</span>)</span><br><span class="line">mobilenet_v3_small = models.mobilenet_v3_small(pretrained=<span class="literal">True</span>)</span><br><span class="line">resnext50_32x4d = models.resnext50_32x4d(pretrained=<span class="literal">True</span>)</span><br><span class="line">wide_resnet50_2 = models.wide_resnet50_2(pretrained=<span class="literal">True</span>)</span><br><span class="line">mnasnet = models.mnasnet1_0(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
 pytorch模型扩展为.pt .pth，模型权重一旦被下载下次就不需要加载，可以将自己的权重下载下来放到同文件夹下，然后再将参数加载网络。如果中途强行停止下载的话，一定要去对应路径下将权重文件删除干净，要不然可能会报错。 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.model = models.resnet50(pretrained=<span class="literal">False</span>)</span><br><span class="line">self.model.load_state_dict(torch.load(<span class="string">&#x27;./model/resnet50-19c8e357.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>训练特定层<br>如果我们正在提取特征并且只想为新初始化的层计算梯度，其他参数不进行改变。那我们就需要通过设置requires_grad &#x3D; False来冻结部分层。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_parameter_requires_grad</span>(<span class="params">model, feature_extracting</span>):</span></span><br><span class="line">    <span class="keyword">if</span> feature_extracting:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="comment"># 冻结参数的梯度</span></span><br><span class="line">feature_extract = <span class="literal">True</span></span><br><span class="line">model = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">set_parameter_requires_grad(model, feature_extract)</span><br><span class="line"><span class="comment"># 修改模型</span></span><br><span class="line">num_ftrs = model.fc.in_features</span><br><span class="line">model.fc = nn.Linear(in_features=num_ftrs, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
之后在训练过程中，model仍会进行梯度回传，但是参数更新则只会发生在fc层。</li>
</ol>
<h1 id="模型微调-timm"><a href="#模型微调-timm" class="headerlink" title="模型微调 - timm"></a>模型微调 - timm</h1><p>torchvision的扩充版本</p>
<ol>
<li>查看预训练模型种类<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timm</span><br><span class="line">avail_pretrained_models = timm.list_models(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">len</span>(avail_pretrained_models)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模糊查询</span></span><br><span class="line">all_densnet_models = timm.list_models(<span class="string">&quot;*densenet*&quot;</span>)</span><br><span class="line">all_densnet_models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型参数</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,num_classes=<span class="number">10</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line">model.default_cfg</span><br></pre></td></tr></table></figure></li>
<li>使用和修改预训练模型<br>通过timm.create_model()的方法来进行模型的创建，传入参数pretrained&#x3D;True，来使用预训练模型。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line">x = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">output = model(x)</span><br><span class="line">output.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某一层模型参数</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">dict</span>(model.named_children())[<span class="string">&#x27;conv1&#x27;</span>].parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改模型</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,num_classes=<span class="number">10</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line">x = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">output = model(x)</span><br><span class="line">output.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变输入通道数</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,num_classes=<span class="number">10</span>,pretrained=<span class="literal">True</span>,in_chans=<span class="number">1</span>)</span><br><span class="line">x = torch.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">output = model(x)</span><br></pre></td></tr></table></figure></li>
<li>模型保存<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(),<span class="string">&#x27;./checkpoint/timm_model.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;./checkpoint/timm_model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="半精度训练"><a href="#半精度训练" class="headerlink" title="半精度训练"></a>半精度训练</h1><p>GPU的性能主要分为两部分：算力和显存，前者决定了显卡计算的速度，后者则决定了显卡可以同时放入多少数据用于计算。在可以使用的显存数量一定的情况下，每次训练能够加载的数据更多（也就是batch size更大），则也可以提高训练效率。另外，有时候数据本身也比较大（比如3D图像、视频等），显存较小的情况下可能甚至batch size为1的情况都无法实现。因此，合理使用显存也就显得十分重要。</p>
<p>我们观察PyTorch默认的浮点数存储方式用的是torch.float32，小数点后位数更多固然能保证数据的精确性，但绝大多数场景其实并不需要这么精确，只保留一半的信息也不会影响结果，也就是使用torch.float16格式。由于数位减了一半，因此被称为“半精度”。显然半精度能够减少显存占用，使得显卡可以同时加载更多数据进行计算。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/float16.jpg"></p>
<ol>
<li>半精度训练的设置<br>import autocast<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br></pre></td></tr></table></figure>
在模型定义中，使用python的装饰器方法，用autocast装饰模型中的forward函数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@autocast()   </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
在训练过程中，只需在将数据输入模型及其之后的部分放入“with autocast():“即可：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> train_loader:</span><br><span class="line">x = x.cuda()</span><br><span class="line"><span class="keyword">with</span> autocast():</span><br><span class="line">       output = model(x)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="数据增强-imgaug"><a href="#数据增强-imgaug" class="headerlink" title="数据增强-imgaug"></a>数据增强-imgaug</h1><ol>
<li>单张图片处理<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">import</span> imgaug <span class="keyword">as</span> ia</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片的读取</span></span><br><span class="line">img = imageio.imread(<span class="string">&quot;./Lenna.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Image进行读取</span></span><br><span class="line"><span class="comment"># img = Image.open(&quot;./Lenna.jpg&quot;)</span></span><br><span class="line"><span class="comment"># image = np.array(img)</span></span><br><span class="line"><span class="comment"># ia.imshow(image)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化图片</span></span><br><span class="line">ia.imshow(img)</span><br></pre></td></tr></table></figure>
imgaug包含了许多从Augmenter继承的数据增强的操作<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imgaug <span class="keyword">import</span> augmenters <span class="keyword">as</span> iaa</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子</span></span><br><span class="line">ia.seed(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化方法</span></span><br><span class="line">rotate = iaa.Affine(rotate=(-<span class="number">4</span>,<span class="number">45</span>))</span><br><span class="line">img_aug = rotate(image=img)</span><br><span class="line">ia.imshow(img_aug)</span><br></pre></td></tr></table></figure>
对一张图片做多种数据增强处理<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">iaa.Sequential(children=<span class="literal">None</span>, <span class="comment"># Augmenter集合</span></span><br><span class="line">               random_order=<span class="literal">False</span>, <span class="comment"># 是否对每个batch使用不同顺序的Augmenter list</span></span><br><span class="line">               name=<span class="literal">None</span>,</span><br><span class="line">               deterministic=<span class="literal">False</span>,</span><br><span class="line">               random_state=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 构建处理序列</span></span><br><span class="line">aug_seq = iaa.Sequential([</span><br><span class="line">    iaa.Affine(rotate=(-<span class="number">25</span>,<span class="number">25</span>)),</span><br><span class="line">    iaa.AdditiveGaussianNoise(scale=(<span class="number">10</span>,<span class="number">60</span>)),</span><br><span class="line">    iaa.Crop(percent=(<span class="number">0</span>,<span class="number">0.2</span>))</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 对图片进行处理，image不可以省略，也不能写成images</span></span><br><span class="line">image_aug = aug_seq(image=img)</span><br><span class="line">ia.imshow(image_aug)</span><br></pre></td></tr></table></figure></li>
<li>对批次图片进行处理<br>对批次的图片以同一种方式处理<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">images = [img,img,img,img,]</span><br><span class="line">images_aug = rotate(images=images)</span><br><span class="line">ia.imshow(np.hstack(images_aug))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对批次图片进行多种增强</span></span><br><span class="line">aug_seq = iaa.Sequential([</span><br><span class="line">    iaa.Affine(rotate=(-<span class="number">25</span>, <span class="number">25</span>)),</span><br><span class="line">    iaa.AdditiveGaussianNoise(scale=(<span class="number">10</span>, <span class="number">60</span>)),</span><br><span class="line">    iaa.Crop(percent=(<span class="number">0</span>, <span class="number">0.2</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 传入时需要指明是images参数</span></span><br><span class="line">images_aug = aug_seq.augment_images(images = images)</span><br><span class="line"><span class="comment">#images_aug = aug_seq(images = images) </span></span><br><span class="line">ia.imshow(np.hstack(images_aug))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对批次的图片分部分处理</span></span><br><span class="line">iaa.Sometimes(p=<span class="number">0.5</span>,  <span class="comment"># 代表划分比例</span></span><br><span class="line">              then_list=<span class="literal">None</span>,  <span class="comment"># Augmenter集合。p概率的图片进行变换的Augmenters。</span></span><br><span class="line">              else_list=<span class="literal">None</span>,  <span class="comment">#1-p概率的图片会被进行变换的Augmenters。注意变换的图片应用的Augmenter只能是then_list或者else_list中的一个。</span></span><br><span class="line">              name=<span class="literal">None</span>,</span><br><span class="line">              deterministic=<span class="literal">False</span>,</span><br><span class="line">              random_state=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不同大小的图片进行处理</span></span><br><span class="line"><span class="comment"># 构建pipline</span></span><br><span class="line">seq = iaa.Sequential([</span><br><span class="line">    iaa.CropAndPad(percent=(-<span class="number">0.2</span>, <span class="number">0.2</span>), pad_mode=<span class="string">&quot;edge&quot;</span>),  <span class="comment"># crop and pad images</span></span><br><span class="line">    iaa.AddToHueAndSaturation((-<span class="number">60</span>, <span class="number">60</span>)),  <span class="comment"># change their color</span></span><br><span class="line">    iaa.ElasticTransformation(alpha=<span class="number">90</span>, sigma=<span class="number">9</span>),  <span class="comment"># water-like effect</span></span><br><span class="line">    iaa.Cutout()  <span class="comment"># replace one squared area within the image by a constant intensity value</span></span><br><span class="line">], random_order=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载不同大小的图片</span></span><br><span class="line">images_different_sizes = [</span><br><span class="line">    imageio.imread(<span class="string">&quot;https://upload.wikimedia.org/wikipedia/commons/e/ed/BRACHYLAGUS_IDAHOENSIS.jpg&quot;</span>),</span><br><span class="line">    imageio.imread(<span class="string">&quot;https://upload.wikimedia.org/wikipedia/commons/c/c9/Southern_swamp_rabbit_baby.jpg&quot;</span>),</span><br><span class="line">    imageio.imread(<span class="string">&quot;https://upload.wikimedia.org/wikipedia/commons/9/9f/Lower_Keys_marsh_rabbit.jpg&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图片进行增强</span></span><br><span class="line">images_aug = seq(images=images_different_sizes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Image 0 (input shape: %s, output shape: %s)&quot;</span> % (images_different_sizes[<span class="number">0</span>].shape, images_aug[<span class="number">0</span>].shape))</span><br><span class="line">ia.imshow(np.hstack([images_different_sizes[<span class="number">0</span>], images_aug[<span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Image 1 (input shape: %s, output shape: %s)&quot;</span> % (images_different_sizes[<span class="number">1</span>].shape, images_aug[<span class="number">1</span>].shape))</span><br><span class="line">ia.imshow(np.hstack([images_different_sizes[<span class="number">1</span>], images_aug[<span class="number">1</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Image 2 (input shape: %s, output shape: %s)&quot;</span> % (images_different_sizes[<span class="number">2</span>].shape, images_aug[<span class="number">2</span>].shape))</span><br><span class="line">ia.imshow(np.hstack([images_different_sizes[<span class="number">2</span>], images_aug[<span class="number">2</span>]]))</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="使用argparse进行调参"><a href="#使用argparse进行调参" class="headerlink" title="使用argparse进行调参"></a>使用argparse进行调参</h1><p>直接在命令行中就可以向程序中传入参数。我们可以使用python file.py来运行python文件。而argparse的作用就是将命令行传入的其他参数进行解析、保存和使用。在使用argparse后，我们在命令行输入的参数就可以以这种形式python file.py –lr 1e-4 –batch_size 32来完成对常见超参数的设置。</p>
<h2 id="argparse的使用"><a href="#argparse的使用" class="headerlink" title="argparse的使用"></a>argparse的使用</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># demo.py</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建ArgumentParser()对象</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加参数</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-o&#x27;</span>, <span class="string">&#x27;--output&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, </span><br><span class="line">    <span class="built_in">help</span>=<span class="string">&quot;shows output&quot;</span>)</span><br><span class="line"><span class="comment"># action = `store_true` 会将output参数记录为True</span></span><br><span class="line"><span class="comment"># type 规定了参数的格式</span></span><br><span class="line"><span class="comment"># default 规定了默认值</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">3e-5</span>, <span class="built_in">help</span>=<span class="string">&#x27;select the learning rate, default=1e-3&#x27;</span>) </span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;input batch size&#x27;</span>)  </span><br><span class="line"><span class="comment"># 使用parse_args()解析函数</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.output:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;This is some output&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;learning rate:<span class="subst">&#123;args.lr&#125;</span> &quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输入python demo.py –lr 3e-4 –batch_size 32，得到：</p>
<pre><code>This is some output
learning rate: 3e-4
</code></pre>
<h2 id="更加高效使用argparse修改超参数"><a href="#更加高效使用argparse修改超参数" class="headerlink" title="更加高效使用argparse修改超参数"></a>更加高效使用argparse修改超参数</h2><p>为了使代码更加简洁和模块化，我一般会将有关超参数的操作写在config.py，然后在train.py或者其他文件导入就可以。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_options</span>(<span class="params">parser=argparse.ArgumentParser(<span class="params"></span>)</span>):</span>  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--workers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>,  </span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of data loading workers, you had better put it &#x27;</span>  </span><br><span class="line">                              <span class="string">&#x27;4 times of your gpu&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>, <span class="built_in">help</span>=<span class="string">&#x27;input batch size, default=64&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--niter&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>, <span class="built_in">help</span>=<span class="string">&#x27;number of epochs to train for, default=10&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">3e-5</span>, <span class="built_in">help</span>=<span class="string">&#x27;select the learning rate, default=1e-3&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">118</span>, <span class="built_in">help</span>=<span class="string">&quot;random seed&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;enables cuda&#x27;</span>)  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--checkpoint_path&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,default=<span class="string">&#x27;&#x27;</span>,  </span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Path to load a previous trained model if not empty (default empty)&#x27;</span>)  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output&#x27;</span>,action=<span class="string">&#x27;store_true&#x27;</span>,default=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&quot;shows output&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    opt = parser.parse_args()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> opt.output:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;num_workers: <span class="subst">&#123;opt.workers&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;batch_size: <span class="subst">&#123;opt.batch_size&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epochs (niters) : <span class="subst">&#123;opt.niter&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;learning rate : <span class="subst">&#123;opt.lr&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;manual_seed: <span class="subst">&#123;opt.seed&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;cuda enable: <span class="subst">&#123;opt.cuda&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;checkpoint_path: <span class="subst">&#123;opt.checkpoint_path&#125;</span>&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> opt  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  </span><br><span class="line">    opt = get_options()</span><br></pre></td></tr></table></figure>
<p>随后在train.py等其他文件，我们就可以使用下面的这样的结构来调用参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要库</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"></span><br><span class="line">opt = config.get_options()</span><br><span class="line"></span><br><span class="line">manual_seed = opt.seed</span><br><span class="line">num_workers = opt.workers</span><br><span class="line">batch_size = opt.batch_size</span><br><span class="line">lr = opt.lr</span><br><span class="line">niters = opt.niters</span><br><span class="line">checkpoint_path = opt.checkpoint_path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机数的设置，保证复现结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seed</span>(<span class="params">seed</span>):</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	set_seed(manual_seed)</span><br><span class="line">	<span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(niters):</span><br><span class="line">		train(model,lr,batch_size,num_workers,checkpoint_path)</span><br><span class="line">		val(model,lr,batch_size,num_workers,checkpoint_path)</span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E5%85%AD%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-16T06:43:54.000Z" title="2022/10/16 下午2:43:54">2022-10-16</time>发表</span><span class="level-item"><time dateTime="2022-10-16T12:34:44.177Z" title="2022/10/16 下午8:34:44">2022-10-16</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">12 分钟读完 (大约1837个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/16/pytorch-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/">pytorch - 模型定义</a></h1><div class="content"><h1 id="模型定义的方式"><a href="#模型定义的方式" class="headerlink" title="模型定义的方式"></a>模型定义的方式</h1><p>基于nn.Module，可以通过Sequential，ModuleList和ModuleDict三种方式定义PyTorch模型。</p>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>将模型的层按序排列起来，按顺序读取，不用写forward，但丧失灵活性</p>
<ol>
<li>Sequential<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Sequential: Direct list</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">net1 = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>), </span><br><span class="line">        )</span><br><span class="line"><span class="built_in">print</span>(net1)</span><br></pre></td></tr></table></figure>

<pre><code> Sequential(
   (0): Linear(in_features=784, out_features=256, bias=True)
   (1): ReLU()
   (2): Linear(in_features=256, out_features=10, bias=True)
 )
</code></pre>
</li>
<li>Ordered Dict<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">net2 = nn.Sequential(collections.OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(<span class="number">784</span>, <span class="number">256</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">          ]))</span><br><span class="line"><span class="built_in">print</span>(net2)</span><br></pre></td></tr></table></figure>

<pre><code> Sequential(
   (fc1): Linear(in_features=784, out_features=256, bias=True)
   (relu1): ReLU()
   (fc2): Linear(in_features=256, out_features=10, bias=True)
 )
</code></pre>
</li>
</ol>
<h2 id="ModuleList"><a href="#ModuleList" class="headerlink" title="ModuleList"></a>ModuleList</h2><p>ModuleList 接收一个子模块（或层，需属于nn.Module类）的列表作为输入，类似List那样进行append和extend操作。同时，子模块或层的权重也会自动添加到网络中来。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net3 = nn.ModuleList([nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU()])</span><br><span class="line">net3.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>)) <span class="comment"># # 类似List的append操作</span></span><br><span class="line"><span class="built_in">print</span>(net3[-<span class="number">1</span>])  <span class="comment"># 类似List的索引访问</span></span><br><span class="line"><span class="built_in">print</span>(net3)</span><br></pre></td></tr></table></figure>

<p>ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起。把modellist写到初始化，再定义forward函数明确传输顺序。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net3</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.modulelist = nn.ModuleList([nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU()])</span><br><span class="line">        self.modulelist.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.modulelist:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net3_ = Net3()</span><br><span class="line">out3_ = net3_(a)</span><br><span class="line"><span class="built_in">print</span>(out3_.shape)</span><br></pre></td></tr></table></figure>

<h2 id="ModuleDict"><a href="#ModuleDict" class="headerlink" title="ModuleDict"></a>ModuleDict</h2><p>ModuleDict和ModuleList的作用类似，只是ModuleDict能够更方便地为神经网络的层添加名称。同样地，ModuleDict并没有定义一个网络，它只是将不同的模块储存在一起，要定义forward。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleDict(&#123;</span><br><span class="line">    <span class="string">&#x27;linear&#x27;</span>: nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    <span class="string">&#x27;act&#x27;</span>: nn.ReLU(),</span><br><span class="line">&#125;)</span><br><span class="line">net[<span class="string">&#x27;output&#x27;</span>] = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="string">&#x27;linear&#x27;</span>]) <span class="comment"># 访问</span></span><br><span class="line"><span class="built_in">print</span>(net.output)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<h1 id="利用模型块快速搭建复杂网络"><a href="#利用模型块快速搭建复杂网络" class="headerlink" title="利用模型块快速搭建复杂网络"></a>利用模型块快速搭建复杂网络</h1><p>当模型有很多层的时候，其中很多重复出现的结构可以定义为一个模块，便利模型构建。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/5.2.1unet.png"><br>如U-Net所示，模型左右对称，每个子层内部有两次卷积，左侧下采样连接，右侧上采样连接，每层模型块和上下模型块连接，同层的左右模型块连接。</p>
<ol>
<li>双次卷积<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleConv</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;(convolution =&gt; [BN] =&gt; ReLU) * 2&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, mid_channels=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> mid_channels:</span><br><span class="line">            mid_channels = out_channels</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, mid_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(mid_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(mid_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.double_conv(x)</span><br></pre></td></tr></table></figure></li>
<li>下采样<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Down</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Downscaling with maxpool then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.maxpool_conv(x)</span><br></pre></td></tr></table></figure></li>
<li>上采样<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Up</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Upscaling then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if bilinear, use the normal convolutions to reduce the number of channels</span></span><br><span class="line">        <span class="keyword">if</span> bilinear: <span class="comment"># 插值</span></span><br><span class="line">            self.up = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">            self.conv = DoubleConv(in_channels, out_channels, in_channels // <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.up = nn.ConvTranspose2d(in_channels, in_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x1, x2</span>):</span></span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        <span class="comment"># input is CHW</span></span><br><span class="line">        diffY = x2.size()[<span class="number">2</span>] - x1.size()[<span class="number">2</span>]</span><br><span class="line">        diffX = x2.size()[<span class="number">3</span>] - x1.size()[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // <span class="number">2</span>, diffX - diffX // <span class="number">2</span>,</span><br><span class="line">                        diffY // <span class="number">2</span>, diffY - diffY // <span class="number">2</span>])</span><br><span class="line">        <span class="comment"># if you have padding issues, see</span></span><br><span class="line">        <span class="comment"># https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a</span></span><br><span class="line">        <span class="comment"># https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd</span></span><br><span class="line">        x = torch.cat([x2, x1], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 连接左侧的数据再卷积</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure></li>
<li>输出<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutConv</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure></li>
<li>组装<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span> // factor)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">unet = UNet(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">unet</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="模型修改"><a href="#模型修改" class="headerlink" title="模型修改"></a>模型修改</h1><p>当有一个现成的模型需要对结构进行修改使用时，我们可以在已有模型上修改。</p>
<ol>
<li>修改模型层<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">unet1 = copy.deepcopy(unet)</span><br><span class="line">unet1.outc</span><br></pre></td></tr></table></figure>
先复制，然后修改outc<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">out_unet1 = unet1(b)</span><br><span class="line"><span class="built_in">print</span>(out_unet1.shape)</span><br></pre></td></tr></table></figure>
要把输出Chanel变成5，重新实例化outc<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unet1.outc = OutConv(<span class="number">64</span>, <span class="number">5</span>)</span><br><span class="line">unet1.outc</span><br><span class="line">out_unet1 = unet1(b)</span><br><span class="line"><span class="built_in">print</span>(out_unet1.shape)</span><br></pre></td></tr></table></figure></li>
<li>添加额外输入<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet2, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span> // factor)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, add_variable</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        x = x + add_variable   <span class="comment">#修改点</span></span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">unet2 = UNet2(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">c = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">out_unet2 = unet2(b, c)</span><br><span class="line"><span class="built_in">print</span>(out_unet2.shape)</span><br></pre></td></tr></table></figure>
或用torch.cat实现了tensor的拼接，如x &#x3D; torch.cat((self.dropout(self.relu(x)), add_variable.unsqueeze(1)),1)。</li>
<li>添加额外输出<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet3</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet3, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span> // factor)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits, x5  <span class="comment"># 修改点</span></span><br><span class="line">unet3 = UNet3(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">c = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">out_unet3, mid_out = unet3(b)</span><br><span class="line"><span class="built_in">print</span>(out_unet3.shape, mid_out.shape)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="模型保存和读取"><a href="#模型保存和读取" class="headerlink" title="模型保存和读取"></a>模型保存和读取</h1><p>单卡&#x2F;多卡，整个&#x2F;部分模型, unet.state_dict()查看模型权重，保存的模型格式： pt pth pkl。</p>
<ol>
<li><p>CPU或单卡：保存&amp;读取整个模型</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(unet, <span class="string">&quot;./unet_example.pth&quot;</span>)</span><br><span class="line">loaded_unet = torch.load(<span class="string">&quot;./unet_example.pth&quot;</span>)</span><br><span class="line">loaded_unet.state_dict()</span><br></pre></td></tr></table></figure></li>
<li><p>CPU或单卡：保存&amp;读取模型权重</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(unet.state_dict(), <span class="string">&quot;./unet_weight_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_weights = torch.load(<span class="string">&quot;./unet_weight_example.pth&quot;</span>)</span><br><span class="line">unet.load_state_dict(loaded_unet_weights) <span class="comment"># 用已经定义好的模型结构加载变量</span></span><br><span class="line">unet.state_dict()</span><br></pre></td></tr></table></figure></li>
<li><p>多卡：保存&amp;读取整个模型。注意模型层名称前多了module<br>不建议，因为保存模型的GPU_id等信息和读取后训练环境可能不同，尤其是要把保存的模型交给另一用户使用的情况</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;2,3&#x27;</span></span><br><span class="line">unet_mul = copy.deepcopy(unet)</span><br><span class="line">unet_mul = nn.DataParallel(unet_mul).cuda()</span><br><span class="line">torch.save(unet_mul, <span class="string">&quot;./unet_mul_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_mul = torch.load(<span class="string">&quot;./unet_mul_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_mul</span><br></pre></td></tr></table></figure>
</li>
<li><p>多卡：保存&amp;读取模型权重。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(unet_mul.state_dict(), <span class="string">&quot;./unet_weight_mul_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_weights_mul = torch.load(<span class="string">&quot;./unet_weight_mul_example.pth&quot;</span>)</span><br><span class="line">unet_mul.load_state_dict(loaded_unet_weights_mul)</span><br><span class="line">unet_mul = nn.DataParallel(unet_mul).cuda()</span><br><span class="line">unet_mul.state_dict()</span><br></pre></td></tr></table></figure>
<p>另外，如果保存的是整个模型，也建议采用提取权重的方式构建新的模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unet_mul.state_dict = loaded_unet_mul.state_dict</span><br><span class="line">unet_mul = nn.DataParallel(unet_mul).cuda()</span><br><span class="line">unet_mul.state_dict()</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%94%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-12T06:43:54.000Z" title="2022/10/12 下午2:43:54">2022-10-12</time>发表</span><span class="level-item"><time dateTime="2022-10-12T15:33:48.906Z" title="2022/10/12 下午11:33:48">2022-10-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">1 小时读完 (大约11579个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/">pytorch - 各个组件和实践</a></h1><div class="content"><p>本章介绍pytorch进行深度学习模型训练的各个组件和实践，层层搭建神经网络模型。</p>
<h1 id="神经网络学习机制"><a href="#神经网络学习机制" class="headerlink" title="神经网络学习机制"></a>神经网络学习机制</h1><center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665563909211/D2B5CA33BD970F64A6301FA75AE2EB22" width="250">  
</center>

<ul>
<li>数据预处理<br>完成一项机器学习任务时的步骤，首先需要对数据进行预处理，其中重要的步骤包括数据格式的统一和必要的数据变换，同时划分训练集和测试集。</li>
<li>模型设计<br>选择模型。</li>
<li>损失函数和优化方案设计<br>设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失函数和优化器）。</li>
<li>前向传播<br>用模型去拟合训练集数据，</li>
<li>反向传播</li>
<li>更新参数</li>
<li>模型表现<br>在验证集&#x2F;测试集上计算模型表现。</li>
</ul>
<h1 id="深度学习在实现上的特殊性"><a href="#深度学习在实现上的特殊性" class="headerlink" title="深度学习在实现上的特殊性"></a>深度学习在实现上的特殊性</h1><ul>
<li>样本量大，需要分批加载<br>由于深度学习所需的样本量很大，一次加载全部数据运行可能会超出内存容量而无法实现；同时还有批（batch）训练等提高模型表现的策略，需要每次训练读取固定数量的样本送入模型中训练。</li>
<li>逐层、模块化搭建网络（卷积层、全连接层、LSTM等）<br>深度神经网络往往需要“逐层”搭建，或者预先定义好可以实现特定功能的模块，再把这些模块组装起来。</li>
<li>多样化的损失函数和优化器设计<br>由于模型设定的灵活性，因此损失函数和优化器要能够保证反向传播能够在用户自行定义的模型结构上实现</li>
<li>GPU的使用<br>需要把模型和数据“放到”GPU上去做运算，同时还需要保证损失函数和优化器能够在GPU上工作。如果使用多张GPU进行训练，还需要考虑模型和数据分配、整合的问题。</li>
<li>各个模块之间的配合<br>深度学习中训练和验证过程最大的特点在于读入数据是按批的，每次读入一个批次的数据，放入GPU中训练，然后将损失函数反向传播回网络最前面的层，同时使用优化器调整网络参数。这里会涉及到各个模块配合的问题。训练&#x2F;验证后还需要根据设定好的指标计算模型表现。</li>
</ul>
<h1 id="pytorch深度学习模块"><a href="#pytorch深度学习模块" class="headerlink" title="pytorch深度学习模块"></a>pytorch深度学习模块</h1><p>将PyTorch完成深度学习的步骤拆解为几个主要模块，实际使用根据自身需求修改对应模块即可，深度学习-&gt;搭积木。</p>
<h2 id="一、-基本配置"><a href="#一、-基本配置" class="headerlink" title="一、 基本配置"></a>一、 基本配置</h2><p>首先导入必要的包</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optimizer</span><br></pre></td></tr></table></figure>
<p>配置训练环境和超参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置GPU，这里有两种方式</span></span><br><span class="line"><span class="comment">## 方案一：使用os.environ，后续用.cuda()</span></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0&#x27;</span></span><br><span class="line"><span class="comment"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:1&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置其他超参数，如batch_size, num_workers, learning rate, 以及总的epochs</span></span><br><span class="line">batch_size = <span class="number">256</span> <span class="comment"># 每次训练读入的数据量</span></span><br><span class="line">num_workers = <span class="number">4</span>   <span class="comment"># 有多少线程来读入数据，对于Windows用户，这里应设置为0，否则会出现多线程错误</span></span><br><span class="line">lr = <span class="number">1e-4</span> <span class="comment"># 参数更新的步长</span></span><br><span class="line">epochs = <span class="number">20</span> <span class="comment"># 训练多少轮</span></span><br></pre></td></tr></table></figure>
<h2 id="二、-数据读入"><a href="#二、-数据读入" class="headerlink" title="二、 数据读入"></a>二、 数据读入</h2><p>有两种方式：</p>
<ul>
<li>下载并使用PyTorch提供的内置数据集<br>只适用于常见的数据集，如MNIST，CIFAR10等，PyTorch官方提供了数据下载。这种方式往往适用于快速测试方法（比如测试下某个idea在MNIST数据集上是否有效）<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先设置数据变换</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">28</span> </span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),   <span class="comment"># 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要</span></span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">train_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br><span class="line">test_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br></pre></td></tr></table></figure></li>
<li>从网站下载以csv格式存储的数据，读入并转成预期的格式<br>需要自己构建Dataset，这对于PyTorch应用于自己的工作中十分重要,同时，还需要对数据进行必要的变换，比如说需要将图片统一为一致的大小，以便后续能够输入网络训练；需要将数据格式转为Tensor类，等等。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式二：读入csv格式的数据，自行构建Dataset类</span></span><br><span class="line"><span class="comment"># csv数据下载链接：https://www.kaggle.com/zalando-research/fashionmnist</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FMDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.df = df</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.images = df.iloc[:,<span class="number">1</span>:].values.astype(np.uint8)</span><br><span class="line">        self.labels = df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.images)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        image = self.images[idx].reshape(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 1是单一通道</span></span><br><span class="line">        label = <span class="built_in">int</span>(self.labels[idx])</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image = torch.tensor(image/<span class="number">255.</span>, dtype=torch.<span class="built_in">float</span>) <span class="comment"># image/255 把数值归一化</span></span><br><span class="line">        label = torch.tensor(label, dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">train_df = pd.read_csv(<span class="string">&quot;./fashion-mnist_train.csv&quot;</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">&quot;./fashion-mnist_test.csv&quot;</span>)</span><br><span class="line">train_data = FMDataset(train_df, data_transform)</span><br><span class="line">test_data = FMDataset(test_df, data_transform)</span><br></pre></td></tr></table></figure>
PyTorch数据读入是通过Dataset+DataLoader的方式完成的，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：</li>
<li>_<em>init</em>_: 用于向类中传入外部参数，同时定义样本集</li>
<li>_<em>getitem</em>_: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练&#x2F;验证所需的数据</li>
<li>_<em>len</em>_: 用于返回数据集的样本数<br>在构建训练和测试数据集完成后，需要定义DataLoader类，以便在训练和测试时加载数据:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
读入后，我们可以做一些数据可视化操作，主要是验证我们读入的数据是否正确:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image, label = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="built_in">print</span>(image.shape, label.shape)</span><br><span class="line">plt.imshow(image[<span class="number">0</span>][<span class="number">0</span>], cmap=<span class="string">&quot;gray&quot;</span>)</span><br></pre></td></tr></table></figure>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665566323623/D2B5CA33BD970F64A6301FA75AE2EB22" width="250">  
</center></li>
</ul>
<h2 id="三、-模型构建"><a href="#三、-模型构建" class="headerlink" title="三、 模型构建"></a>三、 模型构建</h2><p>我们这里的任务是对10个类别的“时装”图像进行分类，FashionMNIST数据集中包含已经预先划分好的训练集和测试集，其中训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为32*32pixel，分属10个类别。由于任务较为简单，这里我们<strong>手搭一个CNN</strong>，而不考虑当下各种模型的复杂结构，模型构建完成后，将模型放到GPU上用于训练。<br>Module 类是nn模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__() </span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># x = nn.functional.normalize(x)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">model = model.cuda()</span><br><span class="line"><span class="comment"># model = nn.DataParallel(model).cuda()   # 多卡训练时的写法，之后的课程中会进一步讲解</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.nn.Conv2d(
  in_channels, 
  out_channels, 
  kernel_size, 
  stride=1, 
  padding=0, 
  dilation=1, 
  groups=1, 
  bias=True, 
  padding_mode=&#39;zeros&#39;, 
  device=None, 
  dtype=None)

torch.nn.MaxPool2d(
  kernel_size, 
  stride=None, 
  padding=0, 
  dilation=1, 
  return_indices=False, 
  ceil_mode=False)
</code></pre>
<p>$d_{out} &#x3D;(d_{in}−dilation∗(kernelsize−1)−1+2∗padding)&#x2F;stride+1)$<br><strong>下面再举一个其他模型MLP：</strong><br>继承Module类构造多层感知机,这里定义的MLP类重载了Module类的init函数和forward函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 backward 函数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)   </span><br></pre></td></tr></table></figure>
<p>我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 call 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">2</span>,<span class="number">784</span>)</span><br><span class="line">net = MLP()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p>注意，这里并没有将 Module 类命名为 Layer (层)或者 Model (模型)之类的名字，这是因为该类是一个可供⾃由组建的部件。它的子类既可以是⼀个层(如PyTorch提供的 Linear 类)，⼜可以是一个模型(如这里定义的 MLP 类)，或者是模型的⼀个部分。<br><strong>下面介绍一些神经网络中常见的层：</strong><br>深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层、卷积层、池化层与循环层等等。虽然PyTorch提供了⼤量常用的层，但有时候我们依然希望⾃定义层。</p>
<ol>
<li>不含模型参数的层<br>下⾯构造的 <strong>MyLayer</strong> 类通过继承 Module 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 forward 函数里。这个层里不含模型参数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyLayer, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()  </span><br><span class="line"><span class="comment"># 测试，实例化该层，然后做前向计算        </span></span><br><span class="line">layer = MyLayer()</span><br><span class="line">layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br></pre></td></tr></table></figure></li>
<li>含模型参数的层<br>自定义含模型参数的自定义层，其中的模型参数可以通过训练学出。<strong>Parameter</strong> 类其实是 Tensor 的子类，如果一个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ <strong>ParameterList</strong> 和 <strong>ParameterDict</strong> 分别定义参数的列表和字典。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyListDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i]) <span class="comment"># torch.mm矩阵相乘，两个二维张量相乘</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = MyListDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line">net = MyDictDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。</li>
<li>二维卷积层<br>二维卷积层将输入和<strong>卷积核</strong>做互相关运算，并加上一个<strong>标量偏差</strong>来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积运算（二维互相关）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span> </span><br><span class="line">    h, w = K.shape</span><br><span class="line">    X, K = X.<span class="built_in">float</span>(), K.<span class="built_in">float</span>()</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。给定一个高和宽为8的输入，我们发现输出的高和宽也是8。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span></span><br><span class="line">    <span class="comment"># (1, 1)代表批量大小和通道数</span></span><br><span class="line">    X = X.view((<span class="number">1</span>, <span class="number">1</span>) + X.shape) <span class="comment"># 加上两个维度</span></span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.view(Y.shape[<span class="number">2</span>:]) <span class="comment"># 排除不关心的前两维:批量和通道</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 ( 为大于1的整数)。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1，-5+2*2=-3+2*1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></li>
<li>池化层<br>池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。下面把池化层的前向计算实现在pool2d函数里。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
我们可以使用torch.nn包来构建神经网络。我们已经介绍了autograd包，nn包则依赖于autograd包来定义模型并对它们求导。一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。</li>
<li>LeNet模型示例<center> 
<img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.1.png" width="480">  
</center>
这是一个简单的前馈神经网络 (feed-forward network）（LeNet）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。一个神经网络的典型训练过程如下：
定义包含一些可学习参数(或者叫权重）的神经网络
在输入数据集上迭代
通过网络处理输入
计算 loss (输出和正确答案的距离）
将梯度反向传播给网络的参数
更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入图像channel：1；输出channel：6；5x5卷积核</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 2x2 Max pooling</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是方阵,则可以只使用一个数字进行定义</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 除去批处理维度的其他所有维度</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<pre><code> Net(
   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
   (fc1): Linear(in_features=400, out_features=120, bias=True)
   (fc2): Linear(in_features=120, out_features=84, bias=True)
   (fc3): Linear(in_features=84, out_features=10, bias=True)
 )
</code></pre>
</li>
</ol>
<p>我们只需要定义 forward 函数，backward函数会在使用autograd时自动定义，backward函数用来计算导数。我们可以在 forward 函数中使用任何针对张量的操作和计算。一个模型的可学习参数可以通过net.parameters()返回。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1的权重</span></span><br></pre></td></tr></table></figure>
<p>尝试随机输入</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="comment"># 清零所有参数的梯度缓存，然后进行随机梯度的反向传播：</span></span><br><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>torch.nn只支持小批量处理 (mini-batches）。整个 torch.nn 包只支持小批量样本的输入，不支持单个样本的输入。比如，nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width 如果是一个单独的样本，只需要使用input.unsqueeze(0) 来添加一个“假的”批大小维度。<br>torch.Tensor - 一个多维数组，支持诸如backward()等的自动求导操作，同时也保存了张量的梯度。<br>nn.Module - 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。<br>nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。<br>autograd.Function - 实现了自动求导前向和反向传播的定义，每个Tensor至少创建一个Function节点，该节点连接到创建Tensor的函数并对其历史进行编码。</p>
<ol>
<li>AlexNet模型示例<center> 
<img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.2.png" width="480">  
</center></li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">net = AlexNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<pre><code>AlexNet(
  (conv): Sequential(
    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU()
    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU()
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc): Sequential(
    (0): Linear(in_features=6400, out_features=4096, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
</code></pre>
<h2 id="四、-模型初始化"><a href="#四、-模型初始化" class="headerlink" title="四、 模型初始化"></a>四、 模型初始化</h2><ol>
<li>torch.nn.init使用<br>通常使用isinstance来进行判断模块<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">linear = nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">isinstance</span>(conv,nn.Conv2d)</span><br><span class="line"><span class="built_in">isinstance</span>(linear,nn.Conv2d)</span><br></pre></td></tr></table></figure>
查看不同初始化参数<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看随机初始化的conv参数</span></span><br><span class="line">conv.weight.data</span><br><span class="line"><span class="comment"># 查看linear的参数</span></span><br><span class="line">linear.weight.data</span><br></pre></td></tr></table></figure>
对不同类型层进行初始化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对conv进行kaiming初始化</span></span><br><span class="line">torch.nn.init.kaiming_normal_(conv.weight.data)</span><br><span class="line">conv.weight.data</span><br><span class="line"><span class="comment"># 对linear进行常数初始化</span></span><br><span class="line">torch.nn.init.constant_(linear.weight.data,<span class="number">0.3</span>)</span><br><span class="line">linear.weight.data</span><br></pre></td></tr></table></figure></li>
<li>初始化函数的封装<br>人们常常将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">		<span class="comment"># 判断是否属于Conv2d</span></span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">			torch.nn.init.xavier_normal_(m.weight.data)</span><br><span class="line">			<span class="comment"># 判断是否有偏置</span></span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				torch.nn.init.constant_(m.bias.data,<span class="number">0.3</span>)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">			torch.nn.init.normal_(m.weight.data, <span class="number">0.1</span>)</span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				torch.nn.init.zeros_(m.bias.data)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">			m.weight.data.fill_(<span class="number">1</span>) 		 </span><br><span class="line">			m.bias.data.zeros_()	</span><br></pre></td></tr></table></figure>
这段代码流程是遍历当前模型的每一层，然后判断各层属于什么类型，然后根据不同类型层，设定不同的权值初始化方法。我们可以通过下面的例程进行一个简短的演示：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型的定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)</span><br><span class="line"></span><br><span class="line">mlp = MLP()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(mlp.parameters()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------初始化-------&quot;</span>)</span><br><span class="line"></span><br><span class="line">initialize_weights(mlp)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(mlp.parameters()))</span><br></pre></td></tr></table></figure>

<pre><code> [Parameter containing:
 tensor([[[[ 0.2103, -0.1679,  0.1757],
           [-0.0647, -0.0136, -0.0410],
           [ 0.1371, -0.1738, -0.0850]]]], requires_grad=True), Parameter containing:
 tensor([0.2507], requires_grad=True), Parameter containing:
 tensor([[ 0.2790, -0.1247,  0.2762,  0.1149, -0.2121, -0.3022, -0.1859,  0.2983,
         -0.0757, -0.2868]], requires_grad=True), Parameter containing:
 tensor([-0.0905], requires_grad=True)]
 &quot;-------初始化-------&quot;
 [Parameter containing:
 tensor([[[[-0.3196, -0.0204, -0.5784],
           [ 0.2660,  0.2242, -0.4198],
           [-0.0952,  0.6033, -0.8108]]]], requires_grad=True),
 Parameter containing:
 tensor([0.3000], requires_grad=True),
 Parameter containing:
 tensor([[ 0.7542,  0.5796,  2.2963, -0.1814, -0.9627,  1.9044,  0.4763,  1.2077,
           0.8583,  1.9494]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]
</code></pre>
</li>
</ol>
<h2 id="五、-损失函数"><a href="#五、-损失函数" class="headerlink" title="五、 损失函数"></a>五、 损失函数</h2><p>这里使用torch.nn模块自带的CrossEntropy损失，PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss，这里需要确保label是从0开始的，同时模型不加softmax层（使用logits计算）,这也说明了PyTorch训练中各个部分不是独立的，需要通盘考虑。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="1-二分类交叉熵损失函数"><a href="#1-二分类交叉熵损失函数" class="headerlink" title="1. 二分类交叉熵损失函数"></a>1. 二分类交叉熵损失函数</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BCELoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。<br><strong>主要参数</strong>：<br><code>weight</code>:每个类别的loss设置权值<br><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。<br><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。<br>计算公式如下：</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580819001/D2B5CA33BD970F64A6301FA75AE2EB22" width="290">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;BCELoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>BCELoss损失函数的计算结果为 tensor(0.5732, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
</code></pre>
<h3 id="2-交叉熵损失函数"><a href="#2-交叉熵损失函数" class="headerlink" title="2. 交叉熵损失函数"></a>2. 交叉熵损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=-<span class="number">100</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算交叉熵函数<br><strong>主要参数</strong>：<br><code>weight</code>:每个类别的loss设置权值。<br><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。<br><code>ignore_index</code>:忽略某个类的损失函数。<br><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。<br>计算公式如下：<br>$<br>\operatorname{loss}(x, \text { class })&#x3D;-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)&#x3D;-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.0115, grad_fn=&lt;NllLossBackward&gt;)
</code></pre>
<h3 id="3-L1损失函数"><a href="#3-L1损失函数" class="headerlink" title="3. L1损失函数"></a>3. L1损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.L1Loss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算输出<code>y</code>和真实标签<code>target</code>之间的差值的绝对值。<code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。mean：加权平均，返回标量。如果选择<code>none</code>，那么返回的结果是和输入元素相同尺寸的。默认计算方式是求平均。<br><strong>计算公式如下：</strong><br>$<br>L_{n} &#x3D; |x_{n}-y_{n}|<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.L1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;L1损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>L1损失函数的计算结果为 tensor(1.5729, grad_fn=&lt;L1LossBackward&gt;)
</code></pre>
<h3 id="4-MSE损失函数"><a href="#4-MSE损失函数" class="headerlink" title="4. MSE损失函数"></a>4. MSE损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MSELoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算输出<code>y</code>和真实标签<code>target</code>之差的平方。</p>
<p>和<code>L1Loss</code>一样，<code>MSELoss</code>损失函数中，<code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。默认计算方式是求平均。</p>
<p><strong>计算公式如下：</strong></p>
<p>$<br>l_{n}&#x3D;\left(x_{n}-y_{n}\right)^{2}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MSE损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MSE损失函数的计算结果为 tensor(1.6968, grad_fn=&lt;MseLossBackward&gt;)
</code></pre>
<h3 id="5-平滑L1-Smooth-L1-损失函数"><a href="#5-平滑L1-Smooth-L1-损失函数" class="headerlink" title="5. 平滑L1 (Smooth L1)损失函数"></a>5. 平滑L1 (Smooth L1)损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.SmoothL1Loss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, beta=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> L1的平滑输出，其功能是减轻离群点带来的影响</p>
<p><code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。默认计算方式是求平均。</p>
<p><strong>提醒：</strong> 之后的损失函数中，关于<code>reduction</code> 这个参数依旧会存在。所以，之后就不再单独说明。</p>
<p><strong>计算公式如下：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\frac{1}{n} \sum_{i&#x3D;1}^{n} z_{i}<br>$<br>其中，</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580768639/D2B5CA33BD970F64A6301FA75AE2EB22" width="290">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.SmoothL1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SmoothL1Loss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>SmoothL1Loss损失函数的计算结果为 tensor(0.7808, grad_fn=&lt;SmoothL1LossBackward&gt;)
</code></pre>
<p><strong>平滑L1与L1的对比</strong></p>
<p>这里我们通过可视化两种损失函数曲线来对比平滑L1和L1两种损失函数的区别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.linspace(-<span class="number">10</span>, <span class="number">10</span>, steps=<span class="number">5000</span>)</span><br><span class="line">target = torch.zeros_like(inputs)</span><br><span class="line"></span><br><span class="line">loss_f_smooth = nn.SmoothL1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_smooth = loss_f_smooth(inputs, target)</span><br><span class="line">loss_f_l1 = nn.L1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_l1 = loss_f_l1(inputs,target)</span><br><span class="line"></span><br><span class="line">plt.plot(inputs.numpy(), loss_smooth.numpy(), label=<span class="string">&#x27;Smooth L1 Loss&#x27;</span>)</span><br><span class="line">plt.plot(inputs.numpy(), loss_l1, label=<span class="string">&#x27;L1 loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x_i - y_i&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss value&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.5.2.png"><br>可以看出，对于<code>smoothL1</code>来说，在 0 这个尖端处，过渡更为平滑。</p>
<h3 id="6-目标泊松分布的负对数似然损失"><a href="#6-目标泊松分布的负对数似然损失" class="headerlink" title="6. 目标泊松分布的负对数似然损失"></a>6. 目标泊松分布的负对数似然损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.PoissonNLLLoss(log_input=<span class="literal">True</span>, full=<span class="literal">False</span>, size_average=<span class="literal">None</span>, eps=<span class="number">1e-08</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 泊松分布的负对数似然损失函数<br><strong>主要参数：</strong><br><code>log_input</code>：输入是否为对数形式，决定计算公式。<br><code>full</code>：计算所有 loss，默认为 False。<br><code>eps</code>：修正项，避免 input 为 0 时，log(input) 为 nan 的情况。<br><strong>数学公式：</strong></p>
<ul>
<li>当参数<code>log_input=True</code>：<br>$<br>\operatorname{loss}\left(x_{n}, y_{n}\right)&#x3D;e^{x_{n}}-x_{n} \cdot y_{n}<br>$</li>
<li>当参数<code>log_input=False</code>：<br>  $<br>  \operatorname{loss}\left(x_{n}, y_{n}\right)&#x3D;x_{n}-y_{n} \cdot \log \left(x_{n}+\text { eps }\right)<br>  $</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.PoissonNLLLoss()</span><br><span class="line">log_input = torch.randn(<span class="number">5</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">output = loss(log_input, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;PoissonNLLLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>PoissonNLLLoss损失函数的计算结果为 tensor(0.7358, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="7-KL散度"><a href="#7-KL散度" class="headerlink" title="7. KL散度"></a>7. KL散度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.KLDivLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, log_target=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 <code>none</code>&#x2F;<code>sum</code>&#x2F;<code>mean</code>&#x2F;<code>batchmean</code>。</p>
<pre><code>none：逐个元素计算。
sum：所有元素求和，返回标量。
mean：加权平均，返回标量。
batchmean：batchsize 维度求平均值。
</code></pre>
<p><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582006862/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.05</span>], [<span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.2</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">loss = nn.KLDivLoss()</span><br><span class="line">output = loss(inputs,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;KLDivLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>KLDivLoss损失函数的计算结果为 tensor(-0.3335)
</code></pre>
<h3 id="8-MarginRankingLoss"><a href="#8-MarginRankingLoss" class="headerlink" title="8. MarginRankingLoss"></a>8. MarginRankingLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MarginRankingLoss(margin=<span class="number">0.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算两个向量之间的相似度，用于排序任务。该方法用于计算两组数据之间的差异。<br><strong>主要参数:</strong><br><code>margin</code>：边界值，$x_{1}$ 与$x_{2}$ 之间的差异值。<br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x 1, x 2, y)&#x3D;\max (0,-y *(x 1-x 2)+\operatorname{margin})<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MarginRankingLoss()</span><br><span class="line">input1 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">input2 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>).sign()</span><br><span class="line">output = loss(input1, input2, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MarginRankingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MarginRankingLoss损失函数的计算结果为 tensor(0.7740, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="9-多标签边界损失函数"><a href="#9-多标签边界损失函数" class="headerlink" title="9. 多标签边界损失函数"></a>9. 多标签边界损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiLabelMarginLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对于多标签分类问题计算损失函数。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i j} \frac{\max (0,1-x[y[j]]-x[i])}{x \cdot \operatorname{size}(0)}<br>$<br>$<br>\begin{array}{l}<br>\text { 其中, } i&#x3D;0, \ldots, x \cdot \operatorname{size}(0), j&#x3D;0, \ldots, y \cdot \operatorname{size}(0), \text { 对于所有的 } i \text { 和 } j \text {, 都有 } y[j] \geq 0 \text { 并且 }\<br>i \neq y[j]<br>\end{array}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MultiLabelMarginLoss()</span><br><span class="line">x = torch.FloatTensor([[<span class="number">0.9</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]])</span><br><span class="line"><span class="comment"># for target y, only consider labels 3 and 0, not after label -1</span></span><br><span class="line">y = torch.LongTensor([[<span class="number">3</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]])<span class="comment"># 真实的分类是，第3类和第0类</span></span><br><span class="line">output = loss(x, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MultiLabelMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MultiLabelMarginLoss损失函数的计算结果为 tensor(0.4500)
</code></pre>
<h3 id="10-二分类损失函数"><a href="#10-二分类损失函数" class="headerlink" title="10. 二分类损失函数"></a>10. 二分类损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.SoftMarginLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)torch.nn.(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算二分类的 logistic 损失。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i} \frac{\log (1+\exp (-y[i] \cdot x[i]))}{x \cdot \operatorname{nelement}()}<br>$<br>$<br><br>\text { 其中, } x . \text { nelement() 为输入 } x \text { 中的样本个数。注意这里 } y \text { 也有 } 1 \text { 和 }-1 \text { 两种模式。 }<br><br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]])  <span class="comment"># 两个样本，两个神经元</span></span><br><span class="line">target = torch.tensor([[-<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签</span></span><br><span class="line"></span><br><span class="line">loss_f = nn.SoftMarginLoss()</span><br><span class="line">output = loss_f(inputs, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SoftMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>SoftMarginLoss损失函数的计算结果为 tensor(0.6764)
</code></pre>
<h3 id="11-多分类的折页损失"><a href="#11-多分类的折页损失" class="headerlink" title="11. 多分类的折页损失"></a>11. 多分类的折页损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiMarginLoss(p=<span class="number">1</span>, margin=<span class="number">1.0</span>, weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算多分类的折页损失<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>p：</code>可选 1 或 2。<br><code>weight</code>：各类别的 loss 设置权值。<br><code>margin</code>：边界值<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\frac{\sum_{i} \max (0, \operatorname{margin}-x[y]+x[i])^{p}}{x \cdot \operatorname{size}(0)}<br>$<br>$<br>\begin{array}{l}<br>\text { 其中, } x \in{0, \ldots, x \cdot \operatorname{size}(0)-1}, y \in{0, \ldots, y \cdot \operatorname{size}(0)-1} \text {, 并且对于所有的 } i \text { 和 } j \text {, }\<br>\text { 都有 } 0 \leq y[j] \leq x \cdot \operatorname{size}(0)-1, \text { 以及 } i \neq y[j] \text { 。 }<br>\end{array}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]]) </span><br><span class="line">target = torch.tensor([<span class="number">0</span>, <span class="number">1</span>], dtype=torch.long) </span><br><span class="line"></span><br><span class="line">loss_f = nn.MultiMarginLoss()</span><br><span class="line">output = loss_f(inputs, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MultiMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MultiMarginLoss损失函数的计算结果为 tensor(0.6000)
</code></pre>
<h3 id="12-三元组损失"><a href="#12-三元组损失" class="headerlink" title="12. 三元组损失"></a>12. 三元组损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">2.0</span>, eps=<span class="number">1e-06</span>, swap=<span class="literal">False</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算三元组损失。<br><strong>三元组:</strong> 这是一种数据的存储或者使用格式。&lt;实体1，关系，实体2&gt;。在项目中，也可以表示为&lt; <code>anchor</code>, <code>positive examples</code> , <code>negative examples</code>&gt;<br>在这个损失函数中，我们希望去<code>anchor</code>的距离更接近<code>positive examples</code>，而远离<code>negative examples </code><br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>p：</code>可选 1 或 2。<br><code>margin</code>：边界值<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582222120/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">triplet_loss = nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">2</span>)</span><br><span class="line">anchor = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">positive = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">negative = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">output = triplet_loss(anchor, positive, negative)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TripletMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>TripletMarginLoss损失函数的计算结果为 tensor(1.1667, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="13-HingEmbeddingLoss"><a href="#13-HingEmbeddingLoss" class="headerlink" title="13. HingEmbeddingLoss"></a>13. HingEmbeddingLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.HingeEmbeddingLoss(margin=<span class="number">1.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对输出的embedding结果做Hing损失计算<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>margin</code>：边界值<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582284050/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<p><strong>注意事项：</strong> 输入x应为两个输入之差的绝对值。<br>可以这样理解，让个输出的是正例yn&#x3D;1,那么loss就是x，如果输出的是负例y&#x3D;-1，那么输出的loss就是要做一个比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_f = nn.HingeEmbeddingLoss()</span><br><span class="line">inputs = torch.tensor([[<span class="number">1.</span>, <span class="number">0.8</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line">output = loss_f(inputs,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;HingEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>HingEmbeddingLoss损失函数的计算结果为 tensor(0.7667)
</code></pre>
<h3 id="14-余弦相似度"><a href="#14-余弦相似度" class="headerlink" title="14. 余弦相似度"></a>14. 余弦相似度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CosineEmbeddingLoss(margin=<span class="number">0.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对两个向量做余弦相似度<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>margin</code>：可取值[-1,1] ，推荐为[0,0.5] 。<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582351089/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<p>这个损失函数应该是最广为人知的。对于两个向量，做余弦相似度。将余弦相似度作为一个距离的计算方式，如果两个向量的距离近，则损失函数值小，反之亦然。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss_f = nn.CosineEmbeddingLoss()</span><br><span class="line">inputs_1 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>], [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>]])</span><br><span class="line">inputs_2 = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">output = loss_f(inputs_1,inputs_2,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;CosineEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>CosineEmbeddingLoss损失函数的计算结果为 tensor(0.5000)
</code></pre>
<h3 id="15-CTC损失函数"><a href="#15-CTC损失函数" class="headerlink" title="15.CTC损失函数"></a>15.CTC损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CTCLoss(blank=<span class="number">0</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, zero_infinity=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 用于解决时序类数据的分类<br>计算连续时间序列和目标序列之间的损失。CTCLoss对输入和目标的可能排列的概率进行求和，产生一个损失值，这个损失值对每个输入节点来说是可分的。输入与目标的对齐方式被假定为 “多对一”，这就限制了目标序列的长度，使其必须是≤输入长度。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>blank</code>：blank label。<br><code>zero_infinity</code>：无穷大的值或梯度值为 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Target are to be padded</span></span><br><span class="line">T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line">S = <span class="number">30</span>      <span class="comment"># Target sequence length of longest target in batch (padding length)</span></span><br><span class="line">S_min = <span class="number">10</span>  <span class="comment"># Minimum target length, for demonstration purposes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">target = torch.randint(low=<span class="number">1</span>, high=C, size=(N, S), dtype=torch.long)</span><br><span class="line"></span><br><span class="line">input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line">target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span><br><span class="line">ctc_loss = nn.CTCLoss()</span><br><span class="line">loss = ctc_loss(<span class="built_in">input</span>, target, input_lengths, target_lengths)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target are to be un-padded</span></span><br><span class="line">T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line">input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">target_lengths = torch.randint(low=<span class="number">1</span>, high=T, size=(N,), dtype=torch.long)</span><br><span class="line">target = torch.randint(low=<span class="number">1</span>, high=C, size=(<span class="built_in">sum</span>(target_lengths),), dtype=torch.long)</span><br><span class="line">ctc_loss = nn.CTCLoss()</span><br><span class="line">loss = ctc_loss(<span class="built_in">input</span>, target, input_lengths, target_lengths)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;CTCLoss损失函数的计算结果为&#x27;</span>,loss)</span><br></pre></td></tr></table></figure>

<pre><code>CTCLoss损失函数的计算结果为 tensor(16.0885, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h2 id="六、-优化器"><a href="#六、-优化器" class="headerlink" title="六、 优化器"></a>六、 优化器</h2><p>这里使用Adam优化器</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<p>Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面提供了十种优化器。</p>
<ul>
<li>torch.optim.ASGD</li>
<li>torch.optim.Adadelta</li>
<li>torch.optim.Adagrad</li>
<li>torch.optim.Adam</li>
<li>torch.optim.AdamW</li>
<li>torch.optim.Adamax</li>
<li>torch.optim.LBFGS</li>
<li>torch.optim.RMSprop</li>
<li>torch.optim.Rprop</li>
<li>torch.optim.SGD</li>
<li>torch.optim.SparseAdam</li>
</ul>
<p>而以上这些优化算法均继承于<code>Optimizer</code>，下面我们先来看下所有优化器的基类<code>Optimizer</code>。定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, defaults</span>):</span>        </span><br><span class="line">        self.defaults = defaults</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">        self.param_groups = []</span><br></pre></td></tr></table></figure>

<p><strong><code>Optimizer</code>有三个属性：</strong></p>
<ul>
<li><code>defaults</code>：存储的是优化器的超参数，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>state</code>：参数的缓存，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;, &#123;<span class="title">tensor</span>(<span class="params">[[ <span class="number">0.3864</span>, -<span class="number">0.0131</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">        [-<span class="number">0.1911</span>, -<span class="number">0.4511</span>]], requires_grad=<span class="literal">True</span></span>):</span> &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>param_groups</code>：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.1022</span>, -<span class="number">1.6890</span>],[-<span class="number">1.5116</span>, -<span class="number">1.7846</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;]</span><br></pre></td></tr></table></figure>

<p><strong><code>Optimizer</code>还有以下的方法：</strong></p>
<ul>
<li><code>zero_grad()</code>：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self, set_to_none: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment">#梯度不为空</span></span><br><span class="line">                <span class="keyword">if</span> set_to_none: </span><br><span class="line">                    p.grad = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> p.grad.grad_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        p.grad.detach_()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        p.grad.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">                    p.grad.zero_()<span class="comment"># 梯度设置为0</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>step()</code>：执行一步梯度更新，参数更新</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure</span>):</span> </span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<ul>
<li><code>add_param_group()</code>：添加参数组</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_param_group</span>(<span class="params">self, param_group</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(param_group, <span class="built_in">dict</span>), <span class="string">&quot;param group must be a dict&quot;</span></span><br><span class="line"><span class="comment"># 检查类型是否为tensor</span></span><br><span class="line">    params = param_group[<span class="string">&#x27;params&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(params, torch.Tensor):</span><br><span class="line">        param_group[<span class="string">&#x27;params&#x27;</span>] = [params]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(params, <span class="built_in">set</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;optimizer parameters need to be organized in ordered collections, but &#x27;</span></span><br><span class="line">                        <span class="string">&#x27;the ordering of tensors in sets will change between runs. Please use a list instead.&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        param_group[<span class="string">&#x27;params&#x27;</span>] = <span class="built_in">list</span>(params)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> param_group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(param, torch.Tensor):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;optimizer can only optimize Tensors, &quot;</span></span><br><span class="line">                            <span class="string">&quot;but one of the params is &quot;</span> + torch.typename(param))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> param.is_leaf:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;can&#x27;t optimize a non-leaf Tensor&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, default <span class="keyword">in</span> self.defaults.items():</span><br><span class="line">        <span class="keyword">if</span> default <span class="keyword">is</span> required <span class="keyword">and</span> name <span class="keyword">not</span> <span class="keyword">in</span> param_group:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;parameter group didn&#x27;t specify a value of required optimization parameter &quot;</span> +</span><br><span class="line">                             name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            param_group.setdefault(name, default)</span><br><span class="line"></span><br><span class="line">    params = param_group[<span class="string">&#x27;params&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(params) != <span class="built_in">len</span>(<span class="built_in">set</span>(params)):</span><br><span class="line">        warnings.warn(<span class="string">&quot;optimizer contains a parameter group with duplicate parameters; &quot;</span></span><br><span class="line">                      <span class="string">&quot;in future, this will cause an error; &quot;</span></span><br><span class="line">                      <span class="string">&quot;see github.com/pytorch/pytorch/issues/40967 for more information&quot;</span>, stacklevel=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 上面好像都在进行一些类的检测，报Warning和Error</span></span><br><span class="line">    param_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        param_set.update(<span class="built_in">set</span>(group[<span class="string">&#x27;params&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> param_set.isdisjoint(<span class="built_in">set</span>(param_group[<span class="string">&#x27;params&#x27;</span>])):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;some parameters appear in more than one parameter group&quot;</span>)</span><br><span class="line"><span class="comment"># 添加参数</span></span><br><span class="line">    self.param_groups.append(param_group)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>load_state_dict()</code> ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_state_dict</span>(<span class="params">self, state_dict</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Loads the optimizer state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        state_dict (dict): optimizer state. Should be an object returned</span></span><br><span class="line"><span class="string">            from a call to :meth:`state_dict`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># deepcopy, to be consistent with module API</span></span><br><span class="line">    state_dict = deepcopy(state_dict)</span><br><span class="line">    <span class="comment"># Validate the state_dict</span></span><br><span class="line">    groups = self.param_groups</span><br><span class="line">    saved_groups = state_dict[<span class="string">&#x27;param_groups&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(groups) != <span class="built_in">len</span>(saved_groups):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;loaded state dict has a different number of &quot;</span></span><br><span class="line">                         <span class="string">&quot;parameter groups&quot;</span>)</span><br><span class="line">    param_lens = (<span class="built_in">len</span>(g[<span class="string">&#x27;params&#x27;</span>]) <span class="keyword">for</span> g <span class="keyword">in</span> groups)</span><br><span class="line">    saved_lens = (<span class="built_in">len</span>(g[<span class="string">&#x27;params&#x27;</span>]) <span class="keyword">for</span> g <span class="keyword">in</span> saved_groups)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">any</span>(p_len != s_len <span class="keyword">for</span> p_len, s_len <span class="keyword">in</span> <span class="built_in">zip</span>(param_lens, saved_lens)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;loaded state dict contains a parameter group &quot;</span></span><br><span class="line">                         <span class="string">&quot;that doesn&#x27;t match the size of optimizer&#x27;s group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the state</span></span><br><span class="line">    id_map = &#123;old_id: p <span class="keyword">for</span> old_id, p <span class="keyword">in</span></span><br><span class="line">              <span class="built_in">zip</span>(chain.from_iterable((g[<span class="string">&#x27;params&#x27;</span>] <span class="keyword">for</span> g <span class="keyword">in</span> saved_groups)),</span><br><span class="line">                  chain.from_iterable((g[<span class="string">&#x27;params&#x27;</span>] <span class="keyword">for</span> g <span class="keyword">in</span> groups)))&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cast</span>(<span class="params">param, value</span>):</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Make a deep copy of value, casting all tensors to device of param.&quot;&quot;&quot;</span></span><br><span class="line">   		.....</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Copy state assigned to params (and cast tensors to appropriate types).</span></span><br><span class="line">    <span class="comment"># State that is not assigned to params is copied as is (needed for</span></span><br><span class="line">    <span class="comment"># backward compatibility).</span></span><br><span class="line">    state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict[<span class="string">&#x27;state&#x27;</span>].items():</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> id_map:</span><br><span class="line">            param = id_map[k]</span><br><span class="line">            state[param] = cast(param, v)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            state[k] = v</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameter groups, setting their &#x27;params&#x27; value</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_group</span>(<span class="params">group, new_group</span>):</span></span><br><span class="line">       ...</span><br><span class="line">    param_groups = [</span><br><span class="line">        update_group(g, ng) <span class="keyword">for</span> g, ng <span class="keyword">in</span> <span class="built_in">zip</span>(groups, saved_groups)]</span><br><span class="line">    self.__setstate__(&#123;<span class="string">&#x27;state&#x27;</span>: state, <span class="string">&#x27;param_groups&#x27;</span>: param_groups&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>state_dict()</code>：获取优化器当前状态信息字典</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Returns the state of the optimizer as a :class:`dict`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It contains two entries:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * state - a dict holding current optimization state. Its content</span></span><br><span class="line"><span class="string">        differs between optimizer classes.</span></span><br><span class="line"><span class="string">    * param_groups - a dict containing all parameter groups</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Save order indices instead of Tensors</span></span><br><span class="line">    param_mappings = &#123;&#125;</span><br><span class="line">    start_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pack_group</span>(<span class="params">group</span>):</span></span><br><span class="line">		......</span><br><span class="line">    param_groups = [pack_group(g) <span class="keyword">for</span> g <span class="keyword">in</span> self.param_groups]</span><br><span class="line">    <span class="comment"># Remap state to use order indices as keys</span></span><br><span class="line">    packed_state = &#123;(param_mappings[<span class="built_in">id</span>(k)] <span class="keyword">if</span> <span class="built_in">isinstance</span>(k, torch.Tensor) <span class="keyword">else</span> k): v</span><br><span class="line">                    <span class="keyword">for</span> k, v <span class="keyword">in</span> self.state.items()&#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;state&#x27;</span>: packed_state,</span><br><span class="line">        <span class="string">&#x27;param_groups&#x27;</span>: param_groups,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置权重，服从正态分布  --&gt; 2 x 2</span></span><br><span class="line">weight = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 设置梯度为全1矩阵  --&gt; 2 x 2</span></span><br><span class="line">weight.grad = torch.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 输出现有的weight和data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The data of weight before step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight before step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 实例化优化器</span></span><br><span class="line">optimizer = torch.optim.SGD([weight], lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># 进行一步操作</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment"># 查看进行一步后的值，梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The data of weight after step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight after step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 权重清零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="comment"># 检验权重是否为0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight after optimizer.zero_grad():\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 输出参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;optimizer.params_group is \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br><span class="line"><span class="comment"># 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight in optimizer:&#123;&#125;\nweight in weight:&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;params&#x27;</span>][<span class="number">0</span>]), <span class="built_in">id</span>(weight)))</span><br><span class="line"><span class="comment"># 添加参数：weight2</span></span><br><span class="line">weight2 = torch.randn((<span class="number">3</span>, <span class="number">3</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer.add_param_group(&#123;<span class="string">&quot;params&quot;</span>: weight2, <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>&#125;)</span><br><span class="line"><span class="comment"># 查看现有的参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;optimizer.param_groups is\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br><span class="line"><span class="comment"># 查看当前状态信息</span></span><br><span class="line">opt_state_dict = optimizer.state_dict()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;state_dict before step:\n&quot;</span>, opt_state_dict)</span><br><span class="line"><span class="comment"># 进行5次step操作</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="comment"># 输出现有状态信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;state_dict after step:\n&quot;</span>, optimizer.state_dict())</span><br><span class="line"><span class="comment"># 保存参数信息</span></span><br><span class="line">torch.save(optimizer.state_dict(),os.path.join(<span class="string">r&quot;D:\pythonProject\Attention_Unet&quot;</span>, <span class="string">&quot;optimizer_state_dict.pkl&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;----------done-----------&quot;</span>)</span><br><span class="line"><span class="comment"># 加载参数信息</span></span><br><span class="line">state_dict = torch.load(<span class="string">r&quot;D:\pythonProject\Attention_Unet\optimizer_state_dict.pkl&quot;</span>) <span class="comment"># 需要修改为你自己的路径</span></span><br><span class="line">optimizer.load_state_dict(state_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;load state_dict successfully\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(state_dict))</span><br><span class="line"><span class="comment"># 输出最后属性信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.defaults))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.state))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br></pre></td></tr></table></figure>

<h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行更新前的数据，梯度</span></span><br><span class="line">The data of weight before step:</span><br><span class="line">tensor([[-<span class="number">0.3077</span>, -<span class="number">0.1808</span>],</span><br><span class="line">        [-<span class="number">0.7462</span>, -<span class="number">1.5556</span>]])</span><br><span class="line">The grad of weight before step:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 进行更新后的数据，梯度</span></span><br><span class="line">The data of weight after step:</span><br><span class="line">tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]])</span><br><span class="line">The grad of weight after step:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 进行梯度清零的梯度</span></span><br><span class="line">The grad of weight after optimizer.zero_grad():</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"><span class="comment"># 输出信息</span></span><br><span class="line">optimizer.params_group <span class="keyword">is</span> </span><br><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 证明了优化器的和weight的储存是在一个地方，Python基于值管理</span></span><br><span class="line">weight <span class="keyword">in</span> optimizer:<span class="number">1841923407424</span></span><br><span class="line">weight <span class="keyword">in</span> weight:<span class="number">1841923407424</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出参数</span></span><br><span class="line">optimizer.param_groups <span class="keyword">is</span></span><br><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;, &#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[ <span class="number">0.4539</span>, -<span class="number">2.1901</span>, -<span class="number">0.6662</span>],</span><br><span class="line">        [ <span class="number">0.6630</span>, -<span class="number">1.5178</span>, -<span class="number">0.8708</span>],</span><br><span class="line">        [-<span class="number">2.0222</span>,  <span class="number">1.4573</span>,  <span class="number">0.8657</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行更新前的参数查看，用state_dict</span></span><br><span class="line">state_dict before step:</span><br><span class="line"> &#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"><span class="comment"># 进行更新后的参数查看，用state_dict</span></span><br><span class="line">state_dict after step:</span><br><span class="line"> &#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储信息完毕</span></span><br><span class="line">----------done-----------</span><br><span class="line"><span class="comment"># 加载参数信息成功</span></span><br><span class="line">load state_dict successfully</span><br><span class="line"><span class="comment"># 加载参数信息</span></span><br><span class="line">&#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># defaults的属性输出</span></span><br><span class="line">&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># state属性输出</span></span><br><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;, &#123;<span class="title">tensor</span>(<span class="params">[[-<span class="number">1.3031</span>, -<span class="number">1.1761</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">        [-<span class="number">1.7415</span>, -<span class="number">2.5510</span>]], requires_grad=<span class="literal">True</span></span>):</span> &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># param_groups属性输出</span></span><br><span class="line">[&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">1.3031</span>, -<span class="number">1.1761</span>],</span><br><span class="line">        [-<span class="number">1.7415</span>, -<span class="number">2.5510</span>]], requires_grad=<span class="literal">True</span>)]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [tensor([[ <span class="number">0.4539</span>, -<span class="number">2.1901</span>, -<span class="number">0.6662</span>],</span><br><span class="line">        [ <span class="number">0.6630</span>, -<span class="number">1.5178</span>, -<span class="number">0.8708</span>],</span><br><span class="line">        [-<span class="number">2.0222</span>,  <span class="number">1.4573</span>,  <span class="number">0.8657</span>]], requires_grad=<span class="literal">True</span>)]&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ol>
<li><p>每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Moddule</span>):</span></span><br><span class="line">    ···</span><br><span class="line">net = Net()</span><br><span class="line">optim = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">optim.step()</span><br></pre></td></tr></table></figure>
</li>
<li><p>optimizer在一个神经网络的epoch中需要实现下面两个步骤：</p>
<ol>
<li>梯度置零</li>
<li>梯度更新<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-5</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">	...</span><br><span class="line">	optimizer.zero_grad()  <span class="comment">#梯度置零</span></span><br><span class="line">	loss = ...             <span class="comment">#计算loss</span></span><br><span class="line">	loss.backward()        <span class="comment">#BP反向传播</span></span><br><span class="line">	optimizer.step()       <span class="comment">#梯度更新</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>给网络不同的层赋予不同的优化器参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"></span><br><span class="line">net = resnet18()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>:net.fc.parameters()&#125;,<span class="comment">#fc的lr使用默认的1e-5</span></span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>:net.layer4[<span class="number">0</span>].conv1.parameters(),<span class="string">&#x27;lr&#x27;</span>:<span class="number">1e-2</span>&#125;],lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用param_groups查看属性</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>为了更好的了解优化器，对PyTorch中的优化器进行了一个小测试</p>
<p><strong>数据生成</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 升维操作</span></span><br><span class="line">x = torch.unsqueeze(a, dim=<span class="number">1</span>)</span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * torch.normal(torch.zeros(x.size()))</span><br></pre></td></tr></table></figure>

<p><strong>数据分布曲线</strong>：<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.1.png"></p>
<p><strong>网络结构</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line">        self.predict = nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.hidden(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下面这部分是测试图，纵坐标代表Loss，横坐标代表的是Step：<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.2.png"><br>在上面的图片上，曲线下降的趋势和对应的steps代表了在这轮数据，模型下的收敛速度</p>
<p><strong>注意:</strong></p>
<p>优化器的选择是需要根据模型进行改变的，不存在绝对的好坏之分，我们需要多进行一些测试。<br>后续会添加SparseAdam，LBFGS这两个优化器的可视化结果</p>
<h2 id="七、-训练与评估"><a href="#七、-训练与评估" class="headerlink" title="七、 训练与评估"></a>七、 训练与评估</h2><p>关注两者的主要区别：<br>模型状态设置<br>是否需要初始化优化器<br>是否需要将loss传回到网络<br>是否需要每步更新optimizer<br>此外，对于测试或验证过程，可以计算分类准确率</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        data, label = data.cuda(), label.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data) <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(output, label)</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 优化器更新权重</span></span><br><span class="line">        train_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    train_loss = train_loss/<span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span>(<span class="params">epoch</span>):</span>       </span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 测试和训练不一样</span></span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    gt_labels = []</span><br><span class="line">    pred_labels = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 不计算梯度</span></span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, label = data.cuda(), label.cuda()</span><br><span class="line">            output = model(data)</span><br><span class="line">            preds = torch.argmax(output, <span class="number">1</span>) <span class="comment"># 得到预测的结果是哪一类</span></span><br><span class="line">            gt_labels.append(label.cpu().data.numpy()) <span class="comment"># 拼接起来</span></span><br><span class="line">            pred_labels.append(preds.cpu().data.numpy())</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            val_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    val_loss = val_loss/<span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)</span><br><span class="line">    acc = np.<span class="built_in">sum</span>(gt_labels==pred_labels)/<span class="built_in">len</span>(pred_labels) <span class="comment"># pre和label相等的次数除上总数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tValidation Loss: &#123;:.6f&#125;, Accuracy: &#123;:6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, val_loss, acc))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    train(epoch)</span><br><span class="line">    val(epoch)</span><br></pre></td></tr></table></figure>
<p><img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665568130565/D2B5CA33BD970F64A6301FA75AE2EB22"></p>
<h2 id="八、-可视化"><a href="#八、-可视化" class="headerlink" title="八、 可视化"></a>八、 可视化</h2><p>见后续专题</p>
<h2 id="九、-保存模型"><a href="#九、-保存模型" class="headerlink" title="九、 保存模型"></a>九、 保存模型</h2><p>训练完成后，可以使用torch.save保存模型参数或者整个模型，也可以在训练过程中保存模型:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">save_path = <span class="string">&quot;./FahionModel.pkl&quot;</span></span><br><span class="line">torch.save(model, save_path)</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%89%E7%AB%A0/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-10T06:43:54.000Z" title="2022/10/10 下午2:43:54">2022-10-10</time>发表</span><span class="level-item"><time dateTime="2022-10-16T12:31:05.794Z" title="2022/10/16 下午8:31:05">2022-10-16</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">15 分钟读完 (大约2297个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/10/pytorch-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">pytorch - 基础知识</a></h1><div class="content"><p>由于张量是对数据的描述，在神经网络中通常将数据以张量的形式表示，如零维张量表示标量，一维张量表示向量，二维表示矩阵，三维张量表示图像，四维表示视频，这章首先介绍张量，然后介绍它的运算，再是核心包autograd自动微分。</p>
<h1 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h1><h2 id="张量的简介"><a href="#张量的简介" class="headerlink" title="张量的简介"></a>张量的简介</h2><p>pytorch的torch.Tensor和Numpy的多维数组非常相似，由于tensor提供GPU的自动求梯度和计算，它更适合深度学习。</p>
<ol>
<li><p>用dtype指定类型创建tensor，detype有tensor.float&#x2F;long等</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(<span class="number">1</span>,dtype=torch.int8)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用指定类型随机初始化</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.IntTensor(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>tensor和numpy array 互相转化，torch.tensor创建的张量是不共享内存的，但torch.from_numpy()和torch.as_tensor()从numpy array创建得到的张量和原数据是共享内存的，修改numpy array会导致对应tensor的改变</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">tensor = torch.tensor(array)</span><br><span class="line">array2tensor = torch.from_numpy(array)</span><br><span class="line">tensor2array = tensor.numpy()</span><br><span class="line"><span class="comment"># 修改array，对应的tensor也会改变</span></span><br><span class="line">array[<span class="number">0</span>,<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(array2tensor)</span><br></pre></td></tr></table></figure></li>
<li><p>从已存在的tensor创建</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">x = x.new_ones(<span class="number">4</span>, <span class="number">3</span>, dtype=torch.double) </span><br><span class="line"><span class="comment"># 创建一个新的全1矩阵tensor，返回的tensor默认具有相同的torch.dtype和torch.device</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># 重置数据类型</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 结果会有一样的size</span></span><br></pre></td></tr></table></figure></li>
<li><p>创建tensor的函数<br>  基础构造 torch.tensor([1,2,3,4])<br>  随机初始化 torch.rand(2,3) [0,1)的均匀分布， torch.randn(2,3) N(0,1)的正态分布，randperm(10) 随机排列<br>  正态分布 torch.normal(2,3) 均值为2标准差为3的正态分布<br>  全1矩阵 torch.ones(2,3)<br>  全0矩阵 torch.zeros(2,3)<br>  对角单位阵 torch.eye(2,3)<br>  有序序列 torch.arange(2,10,2) 从2到10，步长2<br>  均分序列 torch.linspace(2,10,2) 从2到10，均分成2份</p>
</li>
<li><p>查看tensor的维度</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">k = torch.tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(k.shape) </span><br><span class="line"><span class="built_in">print</span>(k.size())</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h2><ol>
<li><p>加法</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">k = torch.rand(<span class="number">2</span>, <span class="number">3</span>) </span><br><span class="line">l = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(k+l)</span><br><span class="line"></span><br><span class="line">torch.add(k,l)</span><br><span class="line"></span><br><span class="line">k.add(l) <span class="comment"># 原值修改</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>索引操作<br>与原数据内存共享，用clone()不会修改</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 取第二列</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>]) </span><br><span class="line"></span><br><span class="line">y = x[<span class="number">0</span>,:]</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :]) <span class="comment"># 源tensor也被改了了</span></span><br><span class="line"></span><br><span class="line">y -= <span class="number">1</span></span><br><span class="line">a = y.clone()</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :])</span><br></pre></td></tr></table></figure>
</li>
<li><p>维度变换</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p>view()共享内存，仅是更改了对张量的观察角度，而reshape()不共享内存，但原始tensor如果不连续，它会返回原值的copy，所以推荐先用clone创建副本再view，用clone能记录到计算图中，梯度回传到副本时也会传到源tensor</p>
<p> 扩展&#x2F;压缩tensor</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">o = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(o)</span><br><span class="line">r = o.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"><span class="built_in">print</span>(r.shape)</span><br><span class="line"></span><br><span class="line">s = r.squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"><span class="built_in">print</span>(s.shape)</span><br></pre></td></tr></table></figure></li>
<li><p>取值操作</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x)) </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x.item()))</span><br></pre></td></tr></table></figure>
<p>其他操作见<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">官方文档</a></p>
</li>
</ol>
<h2 id="张量的广播机制"><a href="#张量的广播机制" class="headerlink" title="张量的广播机制"></a>张量的广播机制</h2><p>两个形状不同的Tensor按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个Tensor形状相同后，再按元素运算。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">p = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line">q = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(q)</span><br><span class="line"><span class="built_in">print</span>(p + q)</span><br></pre></td></tr></table></figure>

<h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><h2 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h2><p>它是torch.tensor的核心类，设置它的属性requires_grad&#x3D;True来追踪张量，完成计算后调用backward()来自动计算所有梯度，导数会自动累积到grad，由链式法则可以计算导数，torch.autograd就是计算雅可比矩阵乘积的。</p>
<ul>
<li>requires_grad<br>如果没有指定的话，默认输入的这个标志是 False。</li>
<li>grad_fn<br>每个张量都有一个grad_fn属性，该属性引用了创建Tensor自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是None)。Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 缺失情况下默认 requires_grad = False</span></span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>下面举个例子说明梯度计算过程</p>
<ol>
<li>创建一个张量并设置requires_grad&#x3D;True用来追踪其计算历史<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">y = x**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line"></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br></pre></td></tr></table></figure></li>
<li>现在开始进行反向传播，因为out是一个标量，因此out.backward()和 out.backward(torch.tensor(1.)) 等价。由于grad在反向传播是累加的，所以在backward之前要清零.grad.data.zero_()。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再来反向传播⼀一次，注意grad是累加的</span></span><br><span class="line">out2 = x.<span class="built_in">sum</span>()</span><br><span class="line">out2.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">out3 = x.<span class="built_in">sum</span>()</span><br><span class="line">x.grad.data.zero_()</span><br><span class="line">out3.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>
雅可比向量积，.data.norm()它对张量y每个元素进行平方，然后对它们求和，最后取平方根，这些操作计算就是所谓的L2或欧几里德范数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>
当y不再是标量，torch.autograd不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给backward：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>
若不需要计算梯度，阻止autograd跟踪设置.requires_grad&#x3D;True的张量的历史记录<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>
若想修改tensor的数值，又不希望被autograd记录(即不会影响反向传播)， 那么我们可以对tensor.data进行操作<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.data) <span class="comment"># 还是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(x.data.requires_grad) <span class="comment"># 但是已经是独立于计算图之外,返回false</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># 更改data的值也会影响tensor的值,x为100</span></span><br><span class="line"><span class="built_in">print</span>(x.grad) <span class="comment"># 不会受到值改变的影响</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="并行计算简介"><a href="#并行计算简介" class="headerlink" title="并行计算简介"></a>并行计算简介</h1><p>PyTorch可以在编写完模型之后，让多个GPU来参与训练，减少训练时间。CUDA是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是CUDA语言来实现的。而pytorch编写深度学习代码使用的CUDA是表示开始要求我们的模型或者数据开始使用GPU了。当我们使用了.cuda()时，其功能是让我们的模型或者数据从CPU迁移到GPU(0)当中，通过GPU开始计算。</p>
<p>注：数据在GPU和CPU之间进行传递时会比较耗时，我们应当尽量避免数据的切换。GPU运算很快，但是在使用简单的操作时，我们应该尽量使用CPU去完成。当我们的服务器上有多个GPU，我们应该指明我们使用的GPU是哪一块，如果我们不设置的话，tensor.cuda()方法会默认将tensor保存到第一块GPU上，等价于tensor.cuda(0)，这将会导致爆出out of memory的错误。我们可以通过以下两种方式继续设置。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#设置在文件最开始部分</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICE&quot;</span>] = <span class="string">&quot;2&quot;</span> <span class="comment"># 设置默认的显卡</span></span><br><span class="line">CUDA_VISBLE_DEVICE=<span class="number">0</span>,<span class="number">1</span> python train.py <span class="comment"># 使用0，1两块GPU</span></span><br></pre></td></tr></table></figure>

<p>常见的并行方法：</p>
<ul>
<li><p>网络结构分布到不同的设备中(Network partitioning)<br>将一个模型的各个部分拆分，然后将不同的部分放入到GPU来做不同任务的计算。这里遇到的问题就是，不同模型组件在不同的GPU上时，GPU之间的传输就很重要，对于GPU之间的通信是一个考验。但是GPU的通信在这种密集任务中很难办到，所以这个方式慢慢淡出了视野。</p>
</li>
<li><p>同一层的任务分布到不同数据中(Layer-wise partitioning)<br>同一层的模型做一个拆分，让不同的GPU去训练同一层模型的部分任务。这样可以保证在不同组件之间传输的问题，但是在我们需要大量的训练，同步任务加重的情况下，会出现和第一种方式一样的问题。</p>
</li>
<li><p>不同的数据分布到不同的设备中，执行相同的任务(Data parallelism)<br>它的逻辑是，不再拆分模型，训练的时候模型都是一整个模型。但是我将输入的数据拆分。所谓的拆分数据就是，同一个模型在不同GPU中训练一部分数据，然后再分别计算一部分数据之后，只需要将输出的数据做一个汇总，然后再反传，这种方式可以解决之前模式遇到的通讯问题，是现在主流方式。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%8C%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-09-23T06:43:54.000Z" title="2022/9/23 下午2:43:54">2022-09-23</time>发表</span><span class="level-item"><time dateTime="2022-10-11T04:00:19.714Z" title="2022/10/11 下午12:00:19">2022-10-11</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span><span class="level-item">1 分钟读完 (大约169个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/09/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2005/">论文阅读 - code2vec Learning Distributed Representations of Code</a></h1><div class="content"><h1 id="泛读"><a href="#泛读" class="headerlink" title="泛读"></a>泛读</h1><p>我们提出了一个神经网络模型，将代码片段表示为连续分布向量（代码嵌入）。其主要思想是将一个代码片段表示为一个固定长度的代码向量，可用于预测代码片段的语义属性。为此，首先将代码分解为其抽象语法树中的路径集合。然后，网络学习每条路径的原子表示，同时学习如何聚合其中的一组路径。通过用从它身上的向量表示来预测方法名称，来证明我们方法的有效性。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-09-21T06:43:54.000Z" title="2022/9/21 下午2:43:54">2022-09-21</time>发表</span><span class="level-item"><time dateTime="2022-10-11T04:00:13.491Z" title="2022/10/11 下午12:00:13">2022-10-11</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span><span class="level-item">3 分钟读完 (大约498个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/09/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2004/">论文阅读 - Contrastive Code Representation Learning</a></h1><div class="content"><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>先前工作从上下文重建tokens来学习源代码上下文表示，对下游的语义理解任务，如代码克隆检测，这些表示应该理想的捕捉程序的功能。但是，我们发现流行的基于重构的RoBERTa模型对代码编辑很敏感，甚至当编辑保留了语义。</p>
<h1 id="ContraCode"><a href="#ContraCode" class="headerlink" title="ContraCode"></a>ContraCode</h1><p>我们提出ContraCode：一个对比的预训练任务，学习代码的功能，不只是形式。ContraCode 预训练一个神经网络在许多非等效的干扰项中识别功能相似的程序变体。我们使用一种自动源到源的编译器作为数据扩充的一种形式，伸缩的生成这些变体。对比性预训练在逆序代码克隆检测基准上的表现优于RoBERTa 39%AUROC。令人惊讶的是，改进的对抗性鲁棒性会比自然代码更准确；与竞争基线相比，ContraCode将汇总和TypeScript类型推理精度提高了2到13个百分点。<a href="thttps://github.com/parasj/contracode.">code</a>,<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.04973">paper</a></p>
<p>像GitHub这样的大型代码库是学习机器辅助编程工具的有力资源。然而，大多数当前的代码表示学习方法都需要标签，而像RoBERTa这样流行的无标签自我监督方法对对抗性输入并不鲁棒。我们不是像BERT这样重建tokens学习代码说什么，而是学习代码做什么。我们提出ContraCode，一种对比性的自我监督算法，它通过基于编译器的数据预测来学习表示不变性。在实验中，ContraCode学习功能的有效表示，并对对抗性代码编辑具有鲁棒性。我们发现，Contra Code显著提高了三个下游JavaScript代码理解任务的性能。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-09-16T06:43:54.000Z" title="2022/9/16 下午2:43:54">2022-09-16</time>发表</span><span class="level-item"><time dateTime="2022-09-21T09:10:01.635Z" title="2022/9/21 下午5:10:01">2022-09-21</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">5 分钟读完 (大约745个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/09/16/NLP%20-%2001/">NLP - 01Introduction and Word Vectors</a></h1><div class="content"><h1 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h1><p>将词编码为向量可以在词空间中表示为一个点，词空间的维度实际上可能比词数量要小，就足以编码所有单词的语义。one-hot vector：将每个词表示为含有0和1的向量，$R^{|V|·1}$，V为词汇表的大小。这样的词表示没给我们直接的相似性概念，每个词都是线性无关的，所以尝试减小空间来找到一个子空间编码单词之间的关系。</p>
<ul>
<li><p>Denotational semantics: 用符号（独热向量）表示，是稀疏的不能捕获相似性。</p>
</li>
<li><p>Distributional semantics: 根据上下文表示单词的含义，密度大可以很好的捕捉相似性。</p>
</li>
</ul>
<h1 id="SVD-based-methods"><a href="#SVD-based-methods" class="headerlink" title="SVD based methods"></a>SVD based methods</h1><p>这类的方法是找到一个词嵌入，我们先循环一遍数据集，用矩阵X累计单词共现的频率，然后在X上进行奇异值分解，得到$USV^T$，用U的行作为字典里所有单词的词嵌入。下面讨论几种X的选择。</p>
<ul>
<li>Using Word-Word Co-occurrence<br>Matrix:<ul>
<li>Generate |V| × |V| co-occurrence<br>matrix, X. </li>
<li>Apply SVD on X to get X &#x3D; $USV^T$.</li>
<li>Select the first k columns of U to get a k-dimensional word vectors.</li>
<li>$\frac{\sum^{k}<em>{i&#x3D;1}\sigma</em>{i}}{\sum^{|V|}<em>{i&#x3D;1}\sigma</em>{i}}$ indicates the amount of variance captured by the first k dimensions.<h2 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h2></li>
</ul>
</li>
</ul>
<p>猜测：有联系的单词经常出现在相同的文档里</p>
<p>构建X：循环非常多的文档，每次单词i出现在文档j里，我们将它添加到$X_{ij}$，维度为$|V|·M$，随着文档数增加。</p>
<h2 id="Window-based-Co-occurrence-Matrix"><a href="#Window-based-Co-occurrence-Matrix" class="headerlink" title="Window based Co-occurrence Matrix"></a>Window based Co-occurrence Matrix</h2><p>计算每个单词出现在感兴趣的单词的周围的特定大小滑动窗口中的次数。</p>
<h2 id="对共现矩阵使用SVD"><a href="#对共现矩阵使用SVD" class="headerlink" title="对共现矩阵使用SVD"></a>对共现矩阵使用SVD</h2><p>对X执行SVD，观察奇异值，S矩阵的对角项，根据捕获的期望百分比方差，在某个索引k处切断它们。</p>
<ul>
<li>基于统计的方法有很多问题：矩阵的维数经常发生变化；矩阵稀疏；高维；计算代价；单词频率不平衡</li>
</ul>
<h1 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h1><p>我们尝试一种新的方法，不是计算存储关于大数据集的全局信息，我们尝试构建一个模型，有能力一次学习一个迭代，最终能够对给定上下文的单词的概率进行编码。</p>
<center>

<img src="https://uploadfiles.nowcoder.com/images/20220917/799168342_1663424051564/D2B5CA33BD970F64A6301FA75AE2EB22"  align="bottom" />
  
</center>

<ul>
<li>Iteration-based methods：每次捕捉单词的共现，而不是SVD直接捕获所有的共现</li>
</ul>
<p>Word2vec is a software package that actually includes :</p>
<ul>
<li><p>2 algorithms: continuous bag-of-words (CBOW) and skip-gram.</p>
<p>CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</p>
</li>
<li><p>2 training methods: negative sampling and hierarchical softmax.</p>
<p>Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</p>
</li>
</ul>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-08-07T06:43:54.000Z" title="2022/8/7 下午2:43:54">2022-08-07</time>发表</span><span class="level-item"><time dateTime="2022-08-12T09:21:19.612Z" title="2022/8/12 下午5:21:19">2022-08-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span><span class="level-item">1 小时读完 (大约8844个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/08/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20-%2003/">论文阅读 - Associating Natural Language Comment and Source Code Entities</a></h1><div class="content"><h1 id="Paper，AAAI-2020"><a href="#Paper，AAAI-2020" class="headerlink" title="Paper，AAAI-2020"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.06728">Paper，AAAI-2020</a></h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>comment 是与 source code elements 相关联的自然语言描述，理解明确的关联可以提高代码的可理解性，并保持代码和注释之间的一致性。为了初步实现这个目标，我们解决了实体对齐的任务，关联 Javadoc comments 的实体和 Java source code elements 的实体。我们提出一种方法，从开源项目的修正历史自动提取监督数据，并为这个任务提出了一个手动注释的评估数据集。我们提出二分类和序列标注模型，通过构建一个丰富的特征集，包括了代码、注释、它们之间的关系。实验表明，我们的系统优于所提出的监督学习的几个基线。</p>
<hr>
<p>疑问1：为什么要实体对齐？怎么对齐？</p>
<p>疑问2：数据集提取过程？规模？影响力？</p>
<p>疑问3：模型的细节，特征集怎么构造，基线是什么？</p>
<p>疑问4：这个工作带来的影响，现在的进展如何？</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>自然语言元素用于记录源代码的各个方面，summaries 提供了给定代码片段功能的高级概述， commit messages 描述了在软件项目的两个版本之间进行的代码更改，API comments 定义了代码块的特定属性（如先决条件和返回值）。 每一个都为开发者之间的沟通提供了一种重要的模式，对开发过程的高效性至关重要。这些自然语言元素越来越流行，在自然语言处理社区的 code summarization，commit message generation，code generation 的研究中。</p>
<p>特别的是，人们对结合自然语言注释和源代码的跨模态任务越来越感兴趣。要完成这些任务，必须了解注释中的元素如何与相应的代码中的元素关联。之前的研究中，检测代码和注释之间不一致的工作，合并对特定任务的规则来连接注释组件到代码的各个方面。最近在自动注释生成方面的工作，依赖一种注意力机制隐式的逼近注释中生成某种术语应注意的代码部分。</p>
<p>与这些方法相反，我们制定了一个任务，目的是学习注释中的实体和相应源代码中的元素之间的显式关联。我们相信显式关联会帮助系统为下游应用带来改进。比如在代码和注释生成任务，它们可以作为监督注意机制，并使用显示知识来增强神经网络模型，这往往带来性能的显著提高。此外，这提供了一种进行更加细粒度的代码注释不一致检测的方法，而不是识别完整注释是否与代码体不一致的常见方法。这样的系统可以成为自动化代码注释维护的有价值组件，目的在于使注释与它所描述的代码保持一致。通过提供注释中哪些元素被注释中的给定实体引用的信号，系统可以自动检测到注释中的实体是否与代码不一致。</p>
<p>学习这种关联的第一步，我们关注在 Javadoc @return comments，它用于描述返回类型和依赖于给定方法中各种条件的潜在返回值。我们观察到 @return comments 往往比其他形式的注释更加结构化，使其成为更干净的数据源，因此是所提任务的合理开头。此外，我们观察到注释通常描述给定代码体中的实体和动作，它们映射到自然语言中的名词短语和动词短语。至于 @return comments，它们描述的返回值通常是实体和与实体相关的条件（如输入参数、程序状态）。因为我们在本文关注与这样的评论，我们针对 @return comments 中的名词短语实体作为第一步，如图1、2，在注释中给定<u>名词短语</u>，任务是识别与它们关联的 <strong>code tokens</strong> 。</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20220807/799168342_1659867479699/D2B5CA33BD970F64A6301FA75AE2EB22" width="270">  
</center>

<p>然而，学习自动解决注释和代码之间的关联，在数据收集方面具有挑战性。为 code&#x2F;language tasks 获取注释数据是困难的，因为它需要理解特定编程语言的源代码的专业知识。此外，收集包含源代码和自然语言的高质量并行语料库具有挑战性，因为大型在线代码库中的数据本身就存在噪声。我们提出了一种新的方法，不需要人工注释，利用该平台的提交历史特性，从 GitHub 获得该任务的噪声监督。我们证明这种噪声监督提供了有价值的训练信号。</p>
<p>为今后的研究奠定基础，我们和相对简单的模型设计了一组高度显著的特征。我们提出了两种模型，在噪声数据上训练，在人工标注数据集上评估。第一个是二分类模型，单独的对给定的代码块的每一个元素进行分类，判断它是否与相关注释中的指定名词短语相关联。第二个是序列标记模型，特别是一个条件随机场 CRF 模型，它共同为代码中的元素分配标签，其中标签表示一个元素是否与指定的名词短语相关联。我们设计了一套新颖的特征来捕获上下文表示、余弦相似度以及与编程语言相关的 API 和语法。</p>
<p>在噪声数据上训练，两种模型的表现大大优于基线，二值分类器获得 F1 score 0.677，CRF 获得 F1 score 0.618，分别比基线提高了39.6%和27.4%。我们通过随着噪声训练数据量的增加，模型的性能提升来证明噪声数据的价值性。此外，通过消融研究，我们强调了模型所用的特征的实用性。本文的主要贡献如下：</p>
<ul>
<li>新任务：关联自然语言的注释和源代码的元素，结合一个人工标注的评估数据集。</li>
<li>一种从软件改变历史和利用监督形式的机器学习系统中获取噪声监督的技术。</li>
<li>一个新的特征集，捕捉代码和注释的特征和它们之间的关系，被用于模型中，可以作为未来工作的 baseline。</li>
</ul>
<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>给定注释中的一个名词短语（NP），其任务是将它和相应代码中的每个候选 code token 之间的关系分类为关联的和不关联的。候选项是code tokens，不包括Java关键字（如try，public，throw），运算符（如&#x3D;），符号（如[，{）这些元素与编程语言语法相关，通常不会在注释中进行描述，如图中 token int，opcode，currentBC 和NP中的“the current bytecode”相关，但是int，setBCI，_nextBCT不是。<mark>该任务和自然语言文本的 <strong>anaphora resolution</strong> 回指解析有相似之处，包括明确地提到 antecedents 先行词（coreference 共引用）以及关联关系（bridge anaphora 桥接回指）。</mark>在这种设置下，注释中被选择的名词短语是 anaphora 隐喻，属于源代码的 tokens 是 candidate antecedents 候选先行词。然而，我们的任务与两者不同，因为它需要对两种不同的模式进行推理。如图1，“problem”显式关联e，但我们需要知道 InterruptedException 是它的类型，这是Exception的一种，Exception是“problem”的一种编程术语。此外，在我们的设置中，注释中的一个NP可以关联源代码中不属于同一个co-reference “chain” 共同引用链的多个不同的元素。<u>由于这些问题，我们将我们的任务广泛的定义为将自然语言注释中的一个名词短语与相应代码体中的单个 code token 关联起来。</u></p>
<p>在这项工作中，我们使用Java编程语言和Javadoc注释，即 @return 注释。然而，这项任务和方法可以扩展到其他编程语言。例如，Python Docstring和c#XML文档注释也有类似的目的。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>我们使用Java中结构最好的注释类型，即带有 @return 的注释，它是Javadoc 文档的一部分，如图1、2。</p>
<p><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/javadoc.html">@return</a> 注释描述了输出，输出是由组成方法的各种语句计算的。相比之下，其他Javadoc 标签中的内容通常更加窄，非结构注释往往本质上更长和高级，这使得很难映射到代码中的元素，如补充材料。我们未来的工作将把提出的任务扩展到其他类型的评论。在此，我们提到评论时，指的是@return标签后的内容。</p>
<p>我们通过从Github上流行的开源项目的所有commits中提取示例来构造数据集。我们根据stars量来排序项目，使用了前1000个项目，因为它们被认为质量更高。我们提取的每个示例都包括对方法体的代码更改以及相应的@return注释的更改。</p>
<h2 id="噪声监督"><a href="#噪声监督" class="headerlink" title="噪声监督"></a>噪声监督</h2><p>我们的噪声监督提取方法的核心思想是利用软件版本控制系统（如Git）的修改历史，基于先前研究表明源代码和评论共同进化。本质上，如果注释中的实体和源代码中的实体同时被edit，则它们相关联的概率会更高，即可近似为同时commit。因此，挖掘这样的共编辑让我们为这个任务获得噪声监督：我们用版本控制系统Git来隔离一起添加和删除的部分代码和注释。</p>
<ul>
<li><p>监督设定</p>
<ul>
<li><p>添加</p>
<p>部分添加的代码可能和部分同时添加的注释有关联，基于这样的直觉，我们为代码tokens分配了有噪声的标签。也就是说，我们将在给定提交中添加的任何代码tokens标记为与同一提交中的注释中引入的NP相关联的代码，并将所有其他代码tokens标记为与NP无关的代码。这些正标签是有噪声的，因为一个开发者可能同时做其他的与添加的NP无关的代码更改。另一方面，负标签（未关联）具有最小的噪声，因为从以前版本中保留的代码tokens不太可能与以前版本注释中不存在的NP关联。我们从添加的数据中收集的这一组示例构成了我们的主要数据集。</p>
</li>
<li><p>删除</p>
<p>理论上，如果我们假设被删除的代码tokens与从注释中删除的NP相关，我们可以从每个提交中提取一个示例。然而，删除的NPs在这方面比添加的NPs要微妙得多。如上面所说，由于添加的NP在以前的版本中不存在，因此以前存在的代码tokens不太可能与之相关联。但由于被删除的NP确实存在于以前的版本中，因此我们不能完全地声称一个在代码中的不同版本之间保持不变的token与删掉的NP没有关联。这将可能会导致更多的负标签噪声，除了固有存在的正标签噪声。如图3，nextBCI被自动标记为与被删除的NP“下一个字节码”无关，即使它可以说是关联的。因此，我们将这些示例从我们的主数据集中分离出来，并形成另一组我们称为删除数据集的示例。</p>
<center> <img src="https://uploadfiles.nowcoder.com/images/20220808/799168342_1659939277294/D2B5CA33BD970F64A6301FA75AE2EB22" width="270"></center></li>
</ul>
</li>
<li><p>数据处理</p>
<p>我们在提交中检查代码和注释的两个版本：提交之前和提交之后。使用spaCy，我们从两个版本的注释中提取NPs，并使用javalang库，对代码的两个版本进行tokenize。使用difflib库，我们计算了两个版本的注释中的NPs之间的差异，以及两个版本的tokenized代码序列之间的差异。这些差异的每一行变化都用正负号标记，如图3所示。</p>
<p>从不同的结果中，我们分别识别了之前和之后的注释和代码版本中唯一的NPs和code tokens，允许我们构建两对(NPs，相关的code tokens)。一个是删除的案例，删除部分的注释和代码只出现在先前的版本中。一个是添加的案例，添加部分的注释和代码只出现在新版本中。</p>
<p>如果提取的NPs或相关code tokens列表为空，我们丢弃这对。此外，我们丢弃由多个NP组成的对，以获得不模糊的训练数据，以确定哪些code tokens应该与哪个NP相关联。因此，最后的对形式是(NP，相关的code tokens)。注意，对于关联的code tokens中的任何token，如果它不是一个通用的Java类型(例如，int，String)，我们会用关联到相同文字字符串来处理code tokens中任何其他token。</p>
<p>然后，我们回到前后版本的代码(不包括Java关键字、操作符和符号，参照第2节)。我们将代码序列tokenize，并将任何不存在在关联code tokens中的tokens标记为不关联。按照这个过程，每个示例都由一个NP和一个被标记的code tokens序列组成。从以前版本（以前）提取的示例将添加到删除数据集中，从新版本（之后）提取的示例被添加到主（添加）数据集中。</p>
</li>
<li><p>数据过滤</p>
<p>虽然大型代码基地如Github和StackOverflow提供了大量数据，为源代码和自然语言任务获取海量高质量的并行数据仍是一个挑战，由于显著的噪声和代码重复等原因。先前的工作通过使用人工标记数据训练的分类器来过滤低质量的数据来解决这个问题。但是，手动获取数据是很困难的，我们选择用启发式，如之前的工作，我们施加约束来过滤掉噪声数据，包括重复、细碎的场景，和不相干的代码与注释更改组成的示例数据。</p>
<p>我们定义细碎场景为涉及到由几个都关联到这个NP的code tokens组成的单行方法的示例，还有那些用简单字符串匹配工具就可以解决关联的示例。</p>
<p>此外，在手动检查了大约200个示例的样本后，我们建立了启发式来最小化不相干代码注释更改的示例数量：</p>
<ol>
<li>那些有冗长的方法或大量代码更改，这些更改可能不都与注释相关。</li>
<li>与重新格式化、更正错误修复和简单改述的相关代码和注释变动的示例。</li>
<li>注释变化包括动词短语的示例，因为相关的代码变化可能与这些短语有关，而不是NP。</li>
</ol>
<p>此外，由于我们关注的是描述Java方法返回值的@return标记，因此我们消除了不包括返回类型更改或至少一个返回语句的代码更改示例。具体参数和每个启发式丢弃的示例数量见补充材料。</p>
<p>应用这种启发式方法大大减少了数据集的大小。然而，我们在手动检查200个例子并观察到显著的噪声后，确定这种过滤是必要的，并发现这与上述之前的工作一致，表明了在大的代码库中，如果没有积极的过滤和预处理，就无法从中学习。</p>
<p>经过过滤后，我们将主数据集划分为训练集、测试集和验证集，如表1所示。基于训练集，NP的中位数是2，四分位数范围(IQR，差异在25%和75%的百分位)1，code token的中位数25,IQR 21，相关的code token是10,IQR 13。我们报告了IQR，因为这些分布不是正态的。</p>
<center> <img src="https://uploadfiles.nowcoder.com/images/20220808/799168342_1659957315365/D2B5CA33BD970F64A6301FA75AE2EB22" width="270"></center>
</li>
<li><p>测试集</p>
<p>测试集中的117个示例是由一位有7年Java使用经验的作者进行注释的。在试验研究中，两个注释者在集中专注于注释的标准之前，共同检查了一个用于注释的方法&#x2F;注释对的样本集。用于识别相关的code tokens的标准包括：它是否由NP直接引用；它是与NP引用的实体对应的属性、类型或方法；它被设置为等于NP引用的实体；如果令牌被更改，则需要更新NP。有关注释的示例，请参见补充材料。为了评估注释的质量，我们询问了一个不是作者之一且有5年Java经验的研究生，注释286个代码标记（来自测试集中的25个示例），这些标记在嘈杂的监督下被标记为相关的。两组注释之间的Cohen’s kappa得分为0.713，表明一致性令人满意。</p>
</li>
</ul>
<h2 id="表示和特征"><a href="#表示和特征" class="headerlink" title="表示和特征"></a>表示和特征</h2><p>我们设计了一组特征，包括表面特征，单词表示，代码标记表示，余弦相似性、代码结构和java api。我们的模型利用通过连接这些特征而得到的1852维特征向量。</p>
<ul>
<li><p>surface feature 表面特征</p>
<p>我们合并了两个binary features，subtokens matching 和返回语句中的 presence，这些也被用于下节讨论的baseline。subtoken matching feature 表示一个候选code token完全匹配给定名词短语的组件，在给定token-level或者subtoken-level。subtokenization 是指java中常用的分裂驼峰式大小写，返回语句的presence feature表示候选code token是否出现返回语句还是完全匹配在返回语句出现的任何token。</p>
</li>
<li><p>单词和代码表示</p>
<p>为了在注释和代码中获得术语表示，我们预先训练了注释的character-level和word-level嵌入表示，代码的character-level、subtoken-level和token-level的嵌入表示。这些128维度的嵌入在大语料库上训练，由GitHub中提取的128,168个@return标签&#x2F;Java方法对组成。训练前的任务是使用单层、单向的SEQ2SEQ模型为Java方法生成@return注释。我们使用平均嵌入来获得NP和候选代码标记的表示。此外，为了提供一个有意义的上下文，我们对完整@return注释对应的嵌入以及候选令牌出现的同一行中标记对应的嵌入取平均值。</p>
</li>
<li><p>余弦相似性</p>
<p>最近的工作使用代码和自然语言描述组成的对的联合向量空间，表明一个代码体和它相关描述有相似的向量。由于@return注释的内容经常在代码中提到实体，不是建立一个联合的向量空间，而是通过计算关于java代码训练的嵌入向量表示，我们将NP投影到代码的相同向量空间。然后我们计算NP和候选code token的余弦相似性，在token-level，subtoken-level，character-level。类似的计算NP和候选code tokens出现的代码行的余弦相似性。</p>
</li>
<li><p>代码结构</p>
<p>抽象语法树捕捉给定代码体在树形式的语法结构，由Java语法定义。使用javalang的AST解析器，我们获得了该method对应的AST。为了表示候选code token相对于该method的整体结构属性，我们提取其父节点和祖父节点的节点类型，用one-hot编码来表示它们。这在method的更广泛的内容的候选code token作用，提供了更深的了解，通过传递详细的信息，如是否在方法调用中、变量声明、循环、参数、try&#x2F;catch block等等。</p>
</li>
<li><p>java api</p>
<p>我们用one-hot编码来表示与通用Java类型和java.util包（一个实用程序类的集合，如List，我们往往会经常使用）。我们假设这些特征可以揭示这些经常出现的tokens的显示形式。为了捕捉本地文本，我们还包括了与候选code token相邻的code token的java相关特性，例如它是通用的Java类型还是Java关键字之一。</p>
</li>
</ul>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>我们开发了代表不同的方式来解决我们提出的任务的两种模型：二进制分类和序列标记。我们还制定了多个基于规则的baseline。</p>
<ul>
<li><p>二分类器</p>
<p>给定一个code token序列和注释中的一个NP，我们独立地将每个标记分类为关联或不关联。我们的分类器是一个前馈神经网络，有4个全连接层和最后一个最终输出层。作为输入，网络接受一个与候选code token对应的特征向量（在前一节中讨论），并且模型输出对该token的二值预测。在实验中，我们还用了逻辑回归模型作为分类器，但它的表现不如神经网络。</p>
</li>
<li><p>序列标记</p>
<p>给定一个code token序列和注释中的一个NP，我们对代码序列的tokens是否与NP相关联进行一起分类。以这种方式构建问题背后的直觉是，对给定代码标记的分类通常依赖于对附近标记的分类。例如，在图3c中，表示next()函数的返回类型的int的token与指定的NP没有关联，而与opcode相邻的int的token被认为是关联的，因为opcode是关联的，而int是它的类型。</p>
<p>为了重新建立原始序列的连续顺序，我们将已删除的Java关键字和符号注入回序列中，并引入第三个类作为这些插入标记的黄金标签。具体来说，我们预测了这三个标签：关联的、非关联的和一个伪标签Java。请注意，我们在评估过程中忽略了这些令牌的分类，也就是说，如果在测试时预测了任何code token标记为伪标签，我们会自动将其分配为不关联（平均而言，这种情况的约为1%）。我们构建一个CRF模型，通过在前馈神经网络的前面加上神经CRF层，类似于二元分类器的结构，让网络接受一个由method中的所有tokens的特征向量组成的矩阵。在实验中，我们还用了非神经网络的CRF，采用sklearn-crfsuite，但它的表现不如神经网络。</p>
</li>
<li><p>模型参数</p>
<p>四个全连接层有512，384，256，126个units，每个被dropout的概率是0.2。如果在5个连续epochs（10epochs之后），F1 score没有改进，我们就终止训练。我们使用最高F1 score的模型。我们用tensorflow实现了这两种模型。</p>
</li>
<li><p>baselines</p>
<ul>
<li><p>Random</p>
<p>基于均匀分布的code token的随机分类。</p>
</li>
<li><p>weighted random</p>
<p>根据从训练集中观察到的相关类和非相关类的概率分别为42.8%和57.2%，对代码标记进行随机分类。</p>
</li>
<li><p>subtoken matching</p>
<p>将subtoken matching表面特征（在上一节中介绍）设置为true的任何token都被分类为关联，而所有其他token都被分类为不关联。请注意，永远不会有这种情况出现，所有相关的code token在token-level或subtoken-level与NP匹配。在过滤过程中，我们从数据集中删除了这些琐碎的例子，因为它们可以用简单的字符串匹配工具来解决，而不是本工作的重点。</p>
</li>
<li><p>presence in return statement</p>
<p>将返回语句表面特征（在上一节中讨论）设置为true的任何token都被分类为关联，所有其他token都被分类为未关联。</p>
</li>
</ul>
</li>
</ul>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>采用micro-level precision，recall，F1 metrics评估模型。在token-level上，测试集中有3592NP-code token pairs。所有报告的分数都是三次运行取平均。下面讨论在主训练集上训练的结果，将删除数据集纳入训练的结果，二值分类器和CRF模型使用的特征的消融研究结果。</p>
<ul>
<li><p>主训练集上训练结果</p>
<p>三个baselines和我们的模型的结果如表2所示。我们的分析主要基于注释测试集上的结果，为了完整性，我们展示了来自未注释集的结果。相对于未注释集的分数，模型通过注释集获得较低的精度分数和较高的召回分数。这是意料之中的，因为在注释过程中，关联有黄金标签的令牌数量减少了。</p>
<p>我们的两个模型的表现都大大优于baseline。从二进制分类器中获得的样本输出见补充材料。虽然CRF的召回率得分略高于二值分类器，但很明显，二值分类器总体上优于F1得分。这可能是由于CRF需要额外的参数来建模依赖关系，这可能无法准确设置，因为我们的实验设置中示例级数据数量有限。此外，虽然我们期望CRF比二值分类器更对上下文敏感，但我们确实将二值分类器结合了许多上下文特征(周围和邻近标记的嵌入、上下文与NP的相似性以及相邻标记的Java API知识)。我们发现有错误分析，CRF模型倾向于在Java关键字之后的令牌以及在稍后出现的方法中的令牌上犯错误。这表明CRF模型可能难以对更长范围的依赖关系和更长范围的序列进行推理。此外，与二进制分类设置相比，Java关键字出现在序列标记设置，因此CRF模型必须比二进制分类器推理更多的code tokens。</p>
</li>
<li><p>使用删除功能来增强训练</p>
<p>我们通过从删除数据集中分阶段添加数据来增加训练集。在这些新的补充数据集上训练二值分类器和CRF的结果如表3所示。对于二值分类器，添加500和867个被删除的示例似乎对F1有显著的提升，而对于CRF模型，添加任何数量的被删除的示例都会提高性能。这表明，我们的模型可以从我们认为比我们收集的主要训练集更有噪声的数据中学习。由于我们能够在添加的情况以及对应的删除的情况下找到价值，我们能够大大增加可以收集来训练执行我们提议的任务的模型的数据量的上限。考虑到为这项任务获得大量高质量的数据是多么困难，这尤其令人鼓舞。尽管我们从超过1,000个项目的所有提交的源代码文件中的方法中提取了示例，在过滤噪声后，我们只从添加的案例中获得了总共970个例子。通过包括已删除案例中的867个例子，我们将这个数字增加到1837个。虽然这仍然是一个相对较小的数字，但我们预计随着任务范围扩展到本文中关注的@return之外的其他评论，潜在的规模将大幅增加。</p>
</li>
<li><p>消融研究</p>
<p>我们对在主数据集上训练的二元分类器进行了消融研究，以分析我们所引入的特征的影响。我们消除了余弦相似性，嵌入，以及和java相关的特性。嵌入特性包括代码嵌入（即对应于候选代码标记的嵌入和方法行中的标记）和注释嵌入(即对应于NP和@retern注释的嵌入)。6根据表4所示的结果，相对于f1度量，所有这些特征都对完整模型的性能做出了积极的贡献。</p>
</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>之前的工作研究了一项任务，包括将对话系统中的名词短语接地到编程环境中(Li和Boyer 2015；Li和Boyer2016)。名词短语是从学生和导师之间的互动中提取出来的，而编程环境承载着学生的代码。他们的工作性质类似于共引用解析，因为目标是识别编程环境中被给定名词短语引用的实体。在对话中，学生和导师讨论与代码中特定实体相关的实现细节，这使得共同引用解析成为框架任务的适当方式。相比之下，他们的工作主题——伴随源代码的注释——通常描述高级功能，而不是实现细节。由于代码中的多个组件相互作用以组成功能，因此代码中可能存在由注释中的给定元素直接或间接引用的实体。由于它们的数据和实现无法公开获得，因此我们无法对这些任务和方法进行任何进一步的比较。</p>
<p>Fluri、W¨ursch和Gall（2007）研究了一个变体任务，即基于距离度量和其他简单启发式方法将单个源代码组件（如类、方法、语句）映射到行或块注释。相比之下，我们的方法在更细粒度的级别上处理代码和注释——我们在令牌级别上对代码进行建模，并在注释中考虑NPs。此外，在我们的框架下，多个代码标记可以映射到同一个NP，并且这些映射是从从更改中提取的数据中学习到的。</p>
<p>Liu等人（2018）介绍了一项任务，该任务涉及将单个提交消息中包含的不同更改意图链接到在提交中发生更改的软件项目中的源代码文件。虽然这需要将自然语言消息中的组件与源代码关联起来，就像我们提出的任务一样，但我们感兴趣的关联占据了更高的粒度。也就是说，我们关注评论中的NPs，而他们的工作是关注提交消息中的句子和子句，在我们的例子中，分类单位是每个单独的代码标记，而在他们的工作中是每个文件。此外，在它们的工作中构建的数据集提取已经更改并提交消息的源代码文件，这些文件是为每个提交新编写的。相反，对于我们的任务，我们收集了包含源代码和注释中的更改的例子，这些源代码和注释是共同发展的。</p>
<p>我们从Git版本控制系统中提取示例的过程与Faruqui等人（2018）基于维基百科的编辑历史建立一个维基百科编辑语料库的方法类似。他们从维基百科文章中的句子中连续文本的插入中提取样本，这些例子有望展示自然语言文本通常是如何被编辑的。相比之下，我们不仅仅限制插入或要求编辑是连续的。此外，我们努力收集一些演示两种模式如何一起编辑的例子。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本文中，我们制定了将Javadoc注释中的实体与Java源代码中的元素关联起来的任务。我们提出了一种新的方法来获得有噪声的监督，并提出了一组丰富的特性，旨在捕获代码、注释和它们之间的关系等方面。基于在一个手动标记的测试集上进行的评估，我们表明，在这样的噪声数据上训练的两个不同的模型可以显著优于多个基线。此外，我们通过展示增加有噪声训练数据的大小如何提高性能，证明了从有噪声数据中学习的潜力。我们还通过消融研究强调了我们的特征集的价值。</p>
<h1 id="Code-and-datasets"><a href="#Code-and-datasets" class="headerlink" title="Code and datasets"></a><a target="_blank" rel="noopener" href="https://github.com/panthap2/AssociatingNLCommentCodeEntities">Code and datasets</a></h1><p>env：py2.7，tnsorflow-gpu&#x3D;1.15.0<br>待改进：</p>
<ul>
<li>数据集太小</li>
<li>模型太简单，baseline太弱</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-06-12T06:43:54.000Z" title="2022/6/12 下午2:43:54">2022-06-12</time>发表</span><span class="level-item"><time dateTime="2022-06-20T15:54:01.742Z" title="2022/6/20 下午11:54:01">2022-06-20</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="level-item">11 分钟读完 (大约1598个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/12/%E5%A4%8D%E4%B9%A0%20transformer/">Review - Transformer</a></h1><div class="content"><p>回顾《 attention is all your need 》，transformer 的结构如下图所示，Inputs 包括 embedding 和 positional encodeing，将词嵌入结合位置信息；Encoder 包括 N 个堆叠的层，每个层中的多头注意力机制和前馈神经网络后都进行了残差和归一化连接；Decoder 包括 N 个堆叠的层，与 Encoder 不同的是，它还多了一层 masked multi-head attention；Output 包括简单的线性层和 softmax 。</p>
<p><img src="https://uploadfiles.nowcoder.com/images/20220620/799168342_1655707530470/CC13B77D33AB41DCE75324F9024F53C5" alt="模型整体结构图"></p>
<h2 id="Inputs"><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h2><p>embedding 将文本处理为向量，如word embedding。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        embedds = self.lut(x)</span><br><span class="line">        <span class="keyword">return</span> embedds * math.sqrt(self.d_model)</span><br><span class="line">        <span class="comment"># 这里在给词向量添加位置编码之前，扩大词向量的数值目的是让位置编码相对较小。</span></span><br><span class="line">        <span class="comment"># 这意味着向词向量添加位置编码时，词向量的原始含义不会丢失。</span></span><br></pre></td></tr></table></figure>

<p>positional ecoding 添加位置信息，采用正余弦可以避免句子长短不一时对位置带来的影响。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="comment"># 为防止当1000的幂作为分母导致的float溢出，对公式进行转换</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 奇数</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 偶数</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># self.pe[:, :x.size(1)]取到x的实际长度</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>可视化位置信息<br><img src="https://uploadfiles.nowcoder.com/images/20220620/799168342_1655740419415/D2B5CA33BD970F64A6301FA75AE2EB22"></p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>encoder由 N 层堆叠，将层复制 N 次。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask) </span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>构造掩码，这里掩码的作用是屏蔽空白区域，decoder中掩码还有屏蔽未来信息的作用。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>encoder层包括多头注意力层和前馈全连接层这两个子层，每个子层后面都用归一和残差连接。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        x_norm = self.norm(x + self.dropout(sublayer(x)))</span><br><span class="line">        <span class="comment"># 有的把x提出来加速收敛 x_norm = x + self.norm(self.dropout(sublayer(x))) </span></span><br><span class="line">        <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>
<p>规范化层</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, feature_size, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(feature_size))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(feature_size))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="comment"># 多注意力层</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># 前馈传播层</span></span><br><span class="line">        z = self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<p>attention层，采用多头注意力机制。</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20220620/799168342_1655728230104/D2B5CA33BD970F64A6301FA75AE2EB22" width="200">  
<img src="https://uploadfiles.nowcoder.com/images/20220620/799168342_1655728396309/D2B5CA33BD970F64A6301FA75AE2EB22" width="230">  
</center>
单头注意力中，QK矩阵内积求出相关性系数scores，判断是否使用掩码，对scores进行softmax，乘上V得到输出。

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p>多头注意力，设计多种Q均衡偏差，让词义有多种表达。给每个头分配等量的词特征，四个线性层中有三个分别对应QKV，最后一个是对应拼接后的。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">         </span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">  </span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<p>feed forward 层包括两个线性层和一个relu层。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>decoder 根据 encoder 的输出和上一次的预测结果，预测序列的下一个输出，由 N 个相同的层堆叠。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed    </span><br><span class="line">        self.tgt_embed = tgt_embed    </span><br><span class="line">        self.generator = generator    </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        memory = self.encode(src, src_mask)</span><br><span class="line">        res = self.decode(memory, src_mask, tgt, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        src_embedds = self.src_embed(src)</span><br><span class="line">        <span class="keyword">return</span> self.encoder(src_embedds, src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        target_embedds = self.tgt_embed(tgt)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(target_embedds, memory, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">上一页</a></div><div class="pagination-next"><a href="/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="🐏"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">🐏</p><p class="is-size-6 is-block">DubistmeinAugenstern</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>WuHan,China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Yang-Emily" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Yang-Emily"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/wuyangemily/"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Youtube" href="https://www.youtube.com/channel/UCSnojGpH9om-xzl-og2_AZQ/featured"><i class="fab fa-youtube"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="twitter" href="https://twitter.com/wuyangemily"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="http://wpa.qq.com/msgrd?v=3&amp;uin=1047772929&amp;site=qq&amp;menu=yes"><i class="fab fa-qq"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://leetcode-cn.com/u/wuyangemily/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Leetcode</span></span><span class="level-right"><span class="level-item tag">leetcode-cn.com</span></span></a></li><li><a class="level is-mobile" href="https://blog.csdn.net/qq_44729001" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">CSDN</span></span><span class="level-right"><span class="level-item tag">blog.csdn.net</span></span></a></li><li><a class="level is-mobile" href="https://www.kaggle.com/wuyangemily" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Kaggle</span></span><span class="level-right"><span class="level-item tag">www.kaggle.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%82/"><span class="level-start"><span class="level-item">杂</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度强化学习</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-22T06:43:54.000Z">2022-10-22</time></p><p class="title"><a href="/2022/10/22/pytorch-%E5%8F%AF%E8%A7%86%E5%8C%96/">pytorch - 可视化</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-20T06:43:54.000Z">2022-10-20</time></p><p class="title"><a href="/2022/10/20/pytorch-%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/">pytorch - 训练技巧</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-16T06:43:54.000Z">2022-10-16</time></p><p class="title"><a href="/2022/10/16/pytorch-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/">pytorch - 模型定义</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-12T06:43:54.000Z">2022-10-12</time></p><p class="title"><a href="/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/">pytorch - 各个组件和实践</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-10T06:43:54.000Z">2022-10-10</time></p><p class="title"><a href="/2022/10/10/pytorch-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">pytorch - 基础知识</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">十月 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">九月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">八月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">三月 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">十月 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DRL/"><span class="tag">DRL</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Draft/"><span class="tag">Draft</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Intelligence-Code/"><span class="tag">Intelligence Code</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">5</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="🐏&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Yang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Yang-Emily/Yang-Emily.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>