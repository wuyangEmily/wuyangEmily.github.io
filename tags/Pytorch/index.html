<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>标签: Pytorch - 🐏&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="🐏&#039;s Blog"><meta name="msapplication-TileImage" content="/img/logo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="🐏&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="🐏&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="🐏&#039;s Blog"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Yang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"🐏's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Yang"},"publisher":{"@type":"Organization","name":"🐏's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/logo.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 6.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="🐏&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Yang-Emily/Yang-Emily.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">Pytorch</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-23T06:43:54.000Z" title="2022/10/23 下午2:43:54">2022-10-23</time>发表</span><span class="level-item"><time dateTime="2022-10-23T08:10:43.559Z" title="2022/10/23 下午4:10:43">2022-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">18 分钟读完 (大约2763个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/23/pytorch-%E7%94%9F%E6%80%81%E9%83%A8%E7%BD%B2/">pytorch - 生态和部署</a></h1><div class="content"><h1 id="1-生态"><a href="#1-生态" class="headerlink" title="1 生态"></a>1 生态</h1><p>PyTorch生态在图像、视频、文本等领域中的发展。</p>
<h2 id="1-1-torchvision"><a href="#1-1-torchvision" class="headerlink" title="1.1 torchvision"></a>1.1 torchvision</h2><p>torchvision包含了在计算机视觉中常常用到的数据集，模型和图像处理的方式。</p>
<ol>
<li>torchvision.datasets *<br>torchvision.datasets主要包含了一些我们在计算机视觉中常见的数据集</li>
<li>torchvision.models *<br><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html">各种预训练好的模型</a></li>
<li>torchvision.tramsforms *<br><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">图像处理的方法</a></li>
<li>torchvision.io<br>在torchvision.io提供了视频、图片和文件的 IO 操作的功能，它们包括读取、写入、编解码处理操作。随着torchvision的发展，io也增加了更多底层的高效率的API。</li>
<li>torchvision.ops<br><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/ops.html">torchvision.ops</a> 为我们提供了许多计算机视觉的特定操作，包括但不仅限于NMS，RoIAlign（MASK R-CNN中应用的一种方法），RoIPool（Fast R-CNN中用到的一种方法）。在合适的时间使用可以大大降低我们的工作量，避免重复的造轮子。</li>
<li>torchvision.utils<br><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/utils.html">torchvision.utils</a> 为我们提供了一些可视化的方法，可以帮助我们将若干张图片拼接在一起、可视化检测和分割的效果。</li>
</ol>
<h2 id="1-2-PyTorchVideo"><a href="#1-2-PyTorchVideo" class="headerlink" title="1.2 PyTorchVideo"></a>1.2 PyTorchVideo</h2><p><a target="_blank" rel="noopener" href="https://pytorchvideo.readthedocs.io/en/latest/index.html">PyTorchVideo</a> 是一个专注于视频理解工作的深度学习库。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/list.png"></p>
<h2 id="1-3-torchtext"><a href="#1-3-torchtext" class="headerlink" title="1.3 torchtext"></a>1.3 torchtext</h2><p>用于自然语言处理（NLP）的工具包torchtext。方便的对文本进行预处理，例如截断补长、构建词表等。</p>
<ol>
<li>构建数据集<ul>
<li>Field及其使用<br> Field是torchtext中定义数据类型以及转换为张量的指令。torchtext 认为一个样本是由多个字段（文本字段，标签字段）组成，不同的字段可能会有不同的处理方式，所以才会有 Field 抽象。定义Field对象是为了明确如何处理不同类型的数据，但具体的处理则是在Dataset中完成的。 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenize = <span class="keyword">lambda</span> x: x.split()</span><br><span class="line">TEXT = data.Field(sequential=<span class="literal">True</span>, tokenize=tokenize, lower=<span class="literal">True</span>, fix_length=<span class="number">200</span>)</span><br><span class="line">LABEL = data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
 sequential设置数据是否是顺序表示的；<br> ​tokenize用于设置将字符串标记为顺序实例的函数;<br> ​lower设置是否将字符串全部转为小写；<br> ​fix_length设置此字段所有实例都将填充到一个固定的长度，方便后续处理；<br> ​use_vocab设置是否引入Vocab object，如果为False，则需要保证之后输入field中的data都是numerical的;<br> 构建Field完成后就可以进一步构建dataset了 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span>(<span class="params">csv_data, text_field, label_field, test=<span class="literal">False</span></span>):</span></span><br><span class="line">    fields = [(<span class="string">&quot;id&quot;</span>, <span class="literal">None</span>), <span class="comment"># we won&#x27;t be needing the id, so we pass in None as the field</span></span><br><span class="line">                (<span class="string">&quot;comment_text&quot;</span>, text_field), (<span class="string">&quot;toxic&quot;</span>, label_field)]       </span><br><span class="line">    examples = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> test:</span><br><span class="line">        <span class="comment"># 如果为测试集，则不加载label</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> tqdm(csv_data[<span class="string">&#x27;comment_text&#x27;</span>]):</span><br><span class="line">            examples.append(data.Example.fromlist([<span class="literal">None</span>, text, <span class="literal">None</span>], fields))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> text, label <span class="keyword">in</span> tqdm(<span class="built_in">zip</span>(csv_data[<span class="string">&#x27;comment_text&#x27;</span>], csv_data[<span class="string">&#x27;toxic&#x27;</span>])):</span><br><span class="line">            examples.append(data.Example.fromlist([<span class="literal">None</span>, text, label], fields))</span><br><span class="line">    <span class="keyword">return</span> examples, fields</span><br></pre></td></tr></table></figure>
 这里使用数据csv_data中有”comment_text”和”toxic”两列，分别对应text和label <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;train_toxic_comments.csv&#x27;</span>)</span><br><span class="line">valid_data = pd.read_csv(<span class="string">&#x27;valid_toxic_comments.csv&#x27;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">&quot;test_toxic_comments.csv&quot;</span>)</span><br><span class="line">TEXT = data.Field(sequential=<span class="literal">True</span>, tokenize=tokenize, lower=<span class="literal">True</span>)</span><br><span class="line">LABEL = data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到构建Dataset所需的examples和fields</span></span><br><span class="line">train_examples, train_fields = get_dataset(train_data, TEXT, LABEL)</span><br><span class="line">valid_examples, valid_fields = get_dataset(valid_data, TEXT, LABEL)</span><br><span class="line">test_examples, test_fields = get_dataset(test_data, TEXT, <span class="literal">None</span>, test=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 构建Dataset数据集</span></span><br><span class="line">train = data.Dataset(train_examples, train_fields)</span><br><span class="line">valid = data.Dataset(valid_examples, valid_fields)</span><br><span class="line">test = data.Dataset(test_examples, test_fields)</span><br></pre></td></tr></table></figure>
 可以看到，定义Field对象完成后，通过get_dataset函数可以读入数据的文本和标签，将二者（examples）连同field一起送到torchtext.data.Dataset类中，即可完成数据集的构建。使用以下命令可以看下读入的数据情况 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查keys是否正确</span></span><br><span class="line"><span class="built_in">print</span>(train[<span class="number">0</span>].__dict__.keys())</span><br><span class="line"><span class="built_in">print</span>(test[<span class="number">0</span>].__dict__.keys())</span><br><span class="line"><span class="comment"># 抽查内容是否正确</span></span><br><span class="line"><span class="built_in">print</span>(train[<span class="number">0</span>].comment_text)</span><br></pre></td></tr></table></figure></li>
<li>词汇表（vocab）<br> Word Embedding 的基本思想是收集一个比较大的语料库（尽量与所做的任务相关），在语料库中使用word2vec之类的方法构建词语到向量（或数字）的映射关系，之后将这一映射关系应用于当前的任务，将句子中的词语转为向量表示。在torchtext中可以使用Field自带的build_vocab函数完成词汇表构建。 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(train)</span><br></pre></td></tr></table></figure></li>
<li>数据迭代器<br> 相当于dataloader <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator, BucketIterator</span><br><span class="line"><span class="comment"># 若只针对训练集构造迭代器</span></span><br><span class="line"><span class="comment"># train_iter = data.BucketIterator(dataset=train, batch_size=8, shuffle=True, sort_within_batch=False, repeat=False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时对训练集和验证集进行迭代器的构建</span></span><br><span class="line">train_iter, val_iter = BucketIterator.splits(</span><br><span class="line">        (train, valid), <span class="comment"># 构建数据集所需的数据集</span></span><br><span class="line">        batch_sizes=(<span class="number">8</span>, <span class="number">8</span>),</span><br><span class="line">        device=-<span class="number">1</span>, <span class="comment"># 如果使用gpu，此处将-1更换为GPU的编号</span></span><br><span class="line">        sort_key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.comment_text), <span class="comment"># the BucketIterator needs to be told what function it should use to group the data.</span></span><br><span class="line">        sort_within_batch=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_iter = Iterator(test, batch_size=<span class="number">8</span>, device=-<span class="number">1</span>, sort=<span class="literal">False</span>, sort_within_batch=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li>
<li>使用自带数据集<br> <a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/datasets.html">若干常用的数据集</a></li>
</ul>
</li>
<li>评测指标（metric）<br>NLP中部分任务的评测不是通过准确率等指标完成的，比如机器翻译任务常用BLEU (bilingual evaluation understudy) score来评价预测文本和标签文本之间的相似程度。torchtext中可以直接调用torchtext.data.metrics.bleu_score来快速实现BLEU<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data.metrics <span class="keyword">import</span> bleu_score</span><br><span class="line">candidate_corpus = [[<span class="string">&#x27;My&#x27;</span>, <span class="string">&#x27;full&#x27;</span>, <span class="string">&#x27;pytorch&#x27;</span>, <span class="string">&#x27;test&#x27;</span>], [<span class="string">&#x27;Another&#x27;</span>, <span class="string">&#x27;Sentence&#x27;</span>]]</span><br><span class="line">references_corpus = [[[<span class="string">&#x27;My&#x27;</span>, <span class="string">&#x27;full&#x27;</span>, <span class="string">&#x27;pytorch&#x27;</span>, <span class="string">&#x27;test&#x27;</span>], [<span class="string">&#x27;Completely&#x27;</span>, <span class="string">&#x27;Different&#x27;</span>]], [[<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Match&#x27;</span>]]]</span><br><span class="line">bleu_score(candidate_corpus, references_corpus)</span><br></pre></td></tr></table></figure></li>
<li>其他<br>由于NLP常用的网络结构比较固定，torchtext并不像torchvision那样提供一系列常用的网络结构。模型主要通过torch.nn中的模块来实现，比如torch.nn.LSTM、torch.nn.RNN等。</li>
</ol>
<h1 id="2-模型部署"><a href="#2-模型部署" class="headerlink" title="2 模型部署"></a>2 模型部署</h1><p>将PyTorch训练好的模型转换为ONNX 格式，然后使用ONNX Runtime运行它进行推理。将得到的权重进行变换才能使我们的模型可以成功部署在上述设备上。<br><img src="https://niuzhikang.oss-cn-chengdu.aliyuncs.com/figures/202208052139305.jpg"></p>
<h2 id="2-1-使用ONNX进行部署并推理"><a href="#2-1-使用ONNX进行部署并推理" class="headerlink" title="2.1 使用ONNX进行部署并推理"></a>2.1 使用ONNX进行部署并推理</h2><p>ONNX Runtime 是由微软维护的一个跨平台机器学习推理加速器，它直接对接ONNX，可以直接读取.onnx文件并实现推理，不需要再把 .onnx 格式的文件转换成其他格式的文件。PyTorch借助ONNX Runtime也完成了部署的最后一公里，构建了 PyTorch –&gt; ONNX –&gt; ONNX Runtime 部署流水线，我们只需要将模型转换为 .onnx 文件，并在 ONNX Runtime 上运行模型即可。</p>
<h2 id="2-2-ONNX和ONNX-Runtime简介"><a href="#2-2-ONNX和ONNX-Runtime简介" class="headerlink" title="2.2 ONNX和ONNX Runtime简介"></a>2.2 ONNX和ONNX Runtime简介</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 激活虚拟环境</span></span><br><span class="line">conda activate env_name <span class="comment"># env_name换成环境名称</span></span><br><span class="line"><span class="comment"># 安装onnx</span></span><br><span class="line">pip install onnx </span><br><span class="line"><span class="comment"># 安装onnx runtime</span></span><br><span class="line">pip install onnxruntime <span class="comment"># 使用CPU进行推理</span></span><br><span class="line"><span class="comment"># pip install onnxruntime-gpu # 使用GPU进行推理</span></span><br></pre></td></tr></table></figure>

<h2 id="2-3-模型导出为ONNX"><a href="#2-3-模型导出为ONNX" class="headerlink" title="2.3 模型导出为ONNX"></a>2.3 模型导出为ONNX</h2><ol>
<li>模型转换为ONNX格式<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.onnx </span><br><span class="line"><span class="comment"># 转换的onnx格式的名称，文件后缀需为.onnx</span></span><br><span class="line">onnx_file_name = <span class="string">&quot;xxxxxx.onnx&quot;</span></span><br><span class="line"><span class="comment"># 我们需要转换的模型，将torch_model设置为自己的模型</span></span><br><span class="line">model = torch_model</span><br><span class="line"><span class="comment"># 加载权重，将model.pth转换为自己的模型权重</span></span><br><span class="line"><span class="comment"># 如果模型的权重是使用多卡训练出来，我们需要去除权重中多的module. 具体操作可以见5.4节</span></span><br><span class="line">model = model.load_state_dict(torch.load(<span class="string">&quot;model.pth&quot;</span>))</span><br><span class="line"><span class="comment"># 导出模型前，必须调用model.eval()或者model.train(False)</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># dummy_input就是一个输入的实例，仅提供输入shape、type等信息 </span></span><br><span class="line">batch_size = <span class="number">1</span> <span class="comment"># 随机的取值，当设置dynamic_axes后影响不大</span></span><br><span class="line">dummy_input = torch.randn(batch_size, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>, requires_grad=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># 这组输入对应的模型输出</span></span><br><span class="line">output = model(dummy_input)</span><br><span class="line"><span class="comment"># 导出模型</span></span><br><span class="line">torch.onnx.export(model,        <span class="comment"># 模型的名称</span></span><br><span class="line">                  dummy_input,   <span class="comment"># 一组实例化输入</span></span><br><span class="line">                  onnx_file_name,   <span class="comment"># 文件保存路径/名称</span></span><br><span class="line">                  export_params=<span class="literal">True</span>,        <span class="comment">#  如果指定为True或默认, 参数也会被导出. 如果你要导出一个没训练过的就设为 False.</span></span><br><span class="line">                  opset_version=<span class="number">10</span>,          <span class="comment"># ONNX 算子集的版本，当前已更新到15</span></span><br><span class="line">                  do_constant_folding=<span class="literal">True</span>,  <span class="comment"># 是否执行常量折叠优化</span></span><br><span class="line">                  input_names = [<span class="string">&#x27;input&#x27;</span>],   <span class="comment"># 输入模型的张量的名称</span></span><br><span class="line">                  output_names = [<span class="string">&#x27;output&#x27;</span>], <span class="comment"># 输出模型的张量的名称</span></span><br><span class="line">                  <span class="comment"># dynamic_axes将batch_size的维度指定为动态，</span></span><br><span class="line">                  <span class="comment"># 后续进行推理的数据可以与导出的dummy_input的batch_size不同</span></span><br><span class="line">                  dynamic_axes=&#123;<span class="string">&#x27;input&#x27;</span> : &#123;<span class="number">0</span> : <span class="string">&#x27;batch_size&#x27;</span>&#125;,    </span><br><span class="line">                                <span class="string">&#x27;output&#x27;</span> : &#123;<span class="number">0</span> : <span class="string">&#x27;batch_size&#x27;</span>&#125;&#125;)</span><br></pre></td></tr></table></figure></li>
<li>ONNX模型的检验<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="comment"># 我们可以使用异常处理的方法进行检验</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 当我们的模型不可用时，将会报出异常</span></span><br><span class="line">    onnx.checker.check_model(self.onnx_model)</span><br><span class="line"><span class="keyword">except</span> onnx.checker.ValidationError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model is invalid: %s&quot;</span>%e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 模型可用时，将不会报出异常，并会输出“The model is valid!”</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model is valid!&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>ONNX可视化<br><a target="_blank" rel="noopener" href="https://github.com/lutzroeder/netron">Netron</a></li>
</ol>
<h2 id="2-4-使用ONNX-Runtime进行推理"><a href="#2-4-使用ONNX-Runtime进行推理" class="headerlink" title="2.4 使用ONNX Runtime进行推理"></a>2.4 使用ONNX Runtime进行推理</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入onnxruntime</span></span><br><span class="line"><span class="keyword">import</span> onnxruntime</span><br><span class="line"><span class="comment"># 需要进行推理的onnx模型文件名称</span></span><br><span class="line">onnx_file_name = <span class="string">&quot;xxxxxx.onnx&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># onnxruntime.InferenceSession用于获取一个 ONNX Runtime 推理器</span></span><br><span class="line">ort_session = onnxruntime.InferenceSession(onnx_file_name)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建字典的输入数据，字典的key需要与我们构建onnx模型时的input_names相同</span></span><br><span class="line"><span class="comment"># 输入的input_img 也需要改变为ndarray格式</span></span><br><span class="line">ort_inputs = &#123;<span class="string">&#x27;input&#x27;</span>: input_img&#125; </span><br><span class="line"><span class="comment"># 我们更建议使用下面这种方法,因为避免了手动输入key</span></span><br><span class="line"><span class="comment"># ort_inputs = &#123;ort_session.get_inputs()[0].name:input_img&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># run是进行模型的推理，第一个参数为输出张量名的列表，一般情况可以设置为None</span></span><br><span class="line"><span class="comment"># 第二个参数为构建的输入值的字典</span></span><br><span class="line"><span class="comment"># 由于返回的结果被列表嵌套，因此我们需要进行[0]的索引</span></span><br><span class="line">ort_output = ort_session.run(<span class="literal">None</span>,ort_inputs)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># output = &#123;ort_session.get_outputs()[0].name&#125;</span></span><br><span class="line"><span class="comment"># ort_output = ort_session.run([output], ort_inputs)[0]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>PyTorch模型的输入为tensor，而ONNX的输入为array，因此我们需要对张量进行变换或者直接将数据读取为array格式，我们可以实现下面的方式进行张量到array的转化。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_numpy</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tensor.detach().cpu().numpy() <span class="keyword">if</span> tensor.requires_grad <span class="keyword">else</span> tensor.cpu().numpy()</span><br></pre></td></tr></table></figure></li>
<li>输入的array的shape应该和我们导出模型的dummy_input的shape相同，如果图片大小不一样，我们应该先进行resize操作。</li>
<li>run的结果是一个列表，我们需要进行索引操作才能获得array格式的结果。</li>
<li>在构建输入的字典时，我们需要注意字典的key应与导出ONNX格式设置的input_name相同，因此我们更建议使用上述的第二种方法构建输入的字典。</li>
</ul>
<h2 id="2-5-代码实战"><a href="#2-5-代码实战" class="headerlink" title="2.5 代码实战"></a>2.5 代码实战</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">pytorch- EXPORTING A MODEL FROM PYTORCH TO ONNX AND RUNNING IT USING ONNX RUNTIME</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E5%85%AB%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-22T06:43:54.000Z" title="2022/10/22 下午2:43:54">2022-10-22</time>发表</span><span class="level-item"><time dateTime="2022-10-21T17:12:18.246Z" title="2022/10/22 上午1:12:18">2022-10-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">9 分钟读完 (大约1366个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/22/pytorch-%E5%8F%AF%E8%A7%86%E5%8C%96/">pytorch - 可视化</a></h1><div class="content"><h1 id="可视化网络结构"><a href="#可视化网络结构" class="headerlink" title="可视化网络结构"></a>可视化网络结构</h1><ol>
<li>使用print函数打印模型基础信息<br>只能得出基础构件的信息，既不能显示出每一层的shape，也不能显示对应参数量的大小<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">model = models.resnet18()</span><br></pre></td></tr></table></figure></li>
<li>使用torchinfo可视化网络结构<br>输出结构化的更详细的信息，包括模块信息（每一层的类型、输出shape和参数量）、模型整体的参数量、模型大小、一次前向或者反向传播需要的内存大小等<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">resnet18 = models.resnet18() <span class="comment"># 实例化模型</span></span><br><span class="line">summary(resnet18, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)) <span class="comment"># 1：batch_size 3:图片的通道数 224: 图片的高宽</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="CNN可视化"><a href="#CNN可视化" class="headerlink" title="CNN可视化"></a>CNN可视化</h1><ol>
<li>CNN卷积核可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg11</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dict</span>(model.features.named_children()))</span><br><span class="line">conv1 = <span class="built_in">dict</span>(model.features.named_children())[<span class="string">&#x27;3&#x27;</span>]</span><br><span class="line">kernel_set = conv1.weight.detach()</span><br><span class="line">num = <span class="built_in">len</span>(conv1.weight.detach())</span><br><span class="line"><span class="built_in">print</span>(kernel_set.shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num):</span><br><span class="line">    i_kernel = kernel_set[i]</span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">17</span>))</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(i_kernel)) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> idx, filer <span class="keyword">in</span> <span class="built_in">enumerate</span>(i_kernel):</span><br><span class="line">            plt.subplot(<span class="number">9</span>, <span class="number">9</span>, idx+<span class="number">1</span>) </span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">            plt.imshow(filer[ :, :].detach(),cmap=<span class="string">&#x27;bwr&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li>CNN特征图可视化方法<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hook</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.module_name = []</span><br><span class="line">        self.features_in_hook = []</span><br><span class="line">        self.features_out_hook = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self,module, fea_in, fea_out</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;hooker working&quot;</span>, self)</span><br><span class="line">        self.module_name.append(module.__class__)</span><br><span class="line">        self.features_in_hook.append(fea_in)</span><br><span class="line">        self.features_out_hook.append(fea_out)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_feature</span>(<span class="params">model, idx, inputs</span>):</span></span><br><span class="line">    hh = Hook()</span><br><span class="line">    model.features[idx].register_forward_hook(hh)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># forward_model(model,False)</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    _ = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(hh.module_name)</span><br><span class="line">    <span class="built_in">print</span>((hh.features_in_hook[<span class="number">0</span>][<span class="number">0</span>].shape))</span><br><span class="line">    <span class="built_in">print</span>((hh.features_out_hook[<span class="number">0</span>].shape))</span><br><span class="line">    </span><br><span class="line">    out1 = hh.features_out_hook[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    total_ft  = out1.shape[<span class="number">1</span>]</span><br><span class="line">    first_item = out1[<span class="number">0</span>].cpu().clone()    </span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">17</span>))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ftidx <span class="keyword">in</span> <span class="built_in">range</span>(total_ft):</span><br><span class="line">        <span class="keyword">if</span> ftidx &gt; <span class="number">99</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ft = first_item[ftidx]</span><br><span class="line">        plt.subplot(<span class="number">10</span>, <span class="number">10</span>, ftidx+<span class="number">1</span>) </span><br><span class="line">        </span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        <span class="comment">#plt.imshow(ft[ :, :].detach(),cmap=&#x27;gray&#x27;)</span></span><br><span class="line">        plt.imshow(ft[ :, :].detach())</span><br></pre></td></tr></table></figure></li>
<li>CNN class activation map可视化方法<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg11,resnet18,resnet101,resnext101_32x8d</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=<span class="literal">True</span>)</span><br><span class="line">img_path = <span class="string">&#x27;./dog.png&#x27;</span></span><br><span class="line"><span class="comment"># resize操作是为了和传入神经网络训练图片大小一致</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path).resize((<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 需要将原始图片转为np.float32格式并且在0-1之间 </span></span><br><span class="line">rgb_img = np.float32(img)/<span class="number">255</span></span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_grad_cam <span class="keyword">import</span> GradCAM,ScoreCAM,GradCAMPlusPlus,AblationCAM,XGradCAM,EigenCAM,FullGrad</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.model_targets <span class="keyword">import</span> ClassifierOutputTarget</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.image <span class="keyword">import</span> show_cam_on_image</span><br><span class="line"></span><br><span class="line">target_layers = [model.features[-<span class="number">1</span>]]</span><br><span class="line"><span class="comment"># 选取合适的类激活图，但是ScoreCAM和AblationCAM需要batch_size</span></span><br><span class="line">cam = GradCAM(model=model,target_layers=target_layers)</span><br><span class="line">targets = [ClassifierOutputTarget(preds)]   </span><br><span class="line"><span class="comment"># 上方preds需要设定，比如ImageNet有1000类，这里可以设为200</span></span><br><span class="line">grayscale_cam = cam(input_tensor=img_tensor, targets=targets)</span><br><span class="line">grayscale_cam = grayscale_cam[<span class="number">0</span>, :]</span><br><span class="line">cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(cam_img))</span><br><span class="line">Image.fromarray(cam_img)</span><br></pre></td></tr></table></figure></li>
<li>使用FlashTorch快速实现CNN可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download example images</span></span><br><span class="line"><span class="comment"># !mkdir -p images</span></span><br><span class="line"><span class="comment"># !wget -nv \</span></span><br><span class="line"><span class="comment">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/great_grey_owl.jpg \</span></span><br><span class="line"><span class="comment">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/peacock.jpg   \</span></span><br><span class="line"><span class="comment">#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg    \</span></span><br><span class="line"><span class="comment">#    -P /content/images</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> flashtorch.utils <span class="keyword">import</span> apply_transforms, load_image</span><br><span class="line"><span class="keyword">from</span> flashtorch.saliency <span class="keyword">import</span> Backprop</span><br><span class="line"></span><br><span class="line">model = models.alexnet(pretrained=<span class="literal">True</span>)</span><br><span class="line">backprop = Backprop(model)</span><br><span class="line"></span><br><span class="line">image = load_image(<span class="string">&#x27;/content/images/great_grey_owl.jpg&#x27;</span>)</span><br><span class="line">owl = apply_transforms(image)</span><br><span class="line"></span><br><span class="line">target_class = <span class="number">24</span></span><br><span class="line">backprop.visualize(owl, target_class, guided=<span class="literal">True</span>, use_gpu=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> flashtorch.activmax <span class="keyword">import</span> GradientAscent</span><br><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">g_ascent = GradientAscent(model.features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># specify layer and filter info</span></span><br><span class="line">conv5_1 = model.features[<span class="number">24</span>]</span><br><span class="line">conv5_1_filters = [<span class="number">45</span>, <span class="number">271</span>, <span class="number">363</span>, <span class="number">489</span>]</span><br><span class="line"></span><br><span class="line">g_ascent.visualize(conv5_1, conv5_1_filters, title=<span class="string">&quot;VGG16: conv5_1&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="使用TensorBoard可视化训练过程"><a href="#使用TensorBoard可视化训练过程" class="headerlink" title="使用TensorBoard可视化训练过程"></a>使用TensorBoard可视化训练过程</h1><ol>
<li>TensorBoard安装</li>
<li>TensorBoard可视化的基本逻辑</li>
<li>TensorBoard的配置与启动</li>
<li>TensorBoard模型结构可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">32</span>,kernel_size = <span class="number">3</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size = <span class="number">2</span>,stride = <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>,out_channels=<span class="number">64</span>,kernel_size = <span class="number">5</span>)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line">writer.add_graph(model, input_to_model = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li>TensorBoard图像可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_test = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">train_data = datasets.CIFAR10(<span class="string">&quot;.&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_train)</span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&quot;.&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform_test)</span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">images, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 仅查看一张图片</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images[0]&#x27;</span>, images[<span class="number">0</span>])</span><br><span class="line">writer.close()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将多张图片拼接成一张图片，中间用黑色网格分割</span></span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;image_grid&#x27;</span>, img_grid)</span><br><span class="line">writer.close()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将多张图片直接写入</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line">writer.add_images(<span class="string">&quot;images&quot;</span>,images,global_step = <span class="number">0</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li>TensorBoard连续变量可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    x = i</span><br><span class="line">    y = x**<span class="number">2</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;x&quot;</span>, x, i) <span class="comment">#日志中记录x在第step i 的值</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y&quot;</span>, y, i) <span class="comment">#日志中记录y在第step i 的值</span></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">writer1 = SummaryWriter(<span class="string">&#x27;./pytorch_tb/x&#x27;</span>)</span><br><span class="line">writer2 = SummaryWriter(<span class="string">&#x27;./pytorch_tb/y&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    x = i</span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    writer1.add_scalar(<span class="string">&quot;same&quot;</span>, x, i) <span class="comment">#日志中记录x在第step i 的值</span></span><br><span class="line">    writer2.add_scalar(<span class="string">&quot;same&quot;</span>, y, i) <span class="comment">#日志中记录y在第step i 的值</span></span><br><span class="line">writer1.close()</span><br><span class="line">writer2.close()</span><br></pre></td></tr></table></figure></li>
<li>TensorBoard参数分布可视化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建正态分布的张量模拟参数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span>(<span class="params">mean, std</span>):</span></span><br><span class="line">    t = std * torch.randn((<span class="number">100</span>, <span class="number">20</span>)) + mean</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"> </span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./pytorch_tb/&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> step, mean <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>)):</span><br><span class="line">    w = norm(mean, <span class="number">1</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&quot;w&quot;</span>, w, step)</span><br><span class="line">    writer.flush()</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></li>
<li>服务器端使用TensorBoard</li>
<li>总结<br>TensorBoard的基本逻辑就是文件的读写逻辑，写入想要可视化的数据，然后TensorBoard自己会读出来。<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%83%E7%AB%A0/index.html">深入浅出PyTorch</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-20T06:43:54.000Z" title="2022/10/20 下午2:43:54">2022-10-20</time>发表</span><span class="level-item"><time dateTime="2022-10-19T16:59:07.090Z" title="2022/10/20 上午12:59:07">2022-10-20</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">21 分钟读完 (大约3084个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/20/pytorch-%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/">pytorch - 训练技巧</a></h1><div class="content"><h1 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h1><ol>
<li>以函数定义<br>简单直接<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_loss</span>(<span class="params">output, target</span>):</span></span><br><span class="line">    loss = torch.mean((output - target)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></li>
<li>以类定义<br>更加常用，继承自nn.Module，可以当成神经网络的一层，使用tensor可以自动求导<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiceLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,weight=<span class="literal">None</span>,size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DiceLoss,self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,inputs,targets,smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        intersection = (inputs * targets).<span class="built_in">sum</span>()                   </span><br><span class="line">        dice = (<span class="number">2.</span>*intersection + smooth)/(inputs.<span class="built_in">sum</span>() + targets.<span class="built_in">sum</span>() + smooth)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - dice</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方法    </span></span><br><span class="line">criterion = DiceLoss()</span><br><span class="line">loss = criterion(<span class="built_in">input</span>,targets)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiceBCELoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DiceBCELoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        intersection = (inputs * targets).<span class="built_in">sum</span>()                     </span><br><span class="line">        dice_loss = <span class="number">1</span> - (<span class="number">2.</span>*intersection + smooth)/(inputs.<span class="built_in">sum</span>() + targets.<span class="built_in">sum</span>() + smooth)  </span><br><span class="line">        BCE = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        Dice_BCE = BCE + dice_loss</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Dice_BCE</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IoULoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(IoULoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        intersection = (inputs * targets).<span class="built_in">sum</span>()</span><br><span class="line">        total = (inputs + targets).<span class="built_in">sum</span>()</span><br><span class="line">        union = total - intersection </span><br><span class="line">        </span><br><span class="line">        IoU = (intersection + smooth)/(union + smooth)</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - IoU</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ALPHA = <span class="number">0.8</span></span><br><span class="line">GAMMA = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FocalLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">        inputs = F.sigmoid(inputs)       </span><br><span class="line">        inputs = inputs.view(-<span class="number">1</span>)</span><br><span class="line">        targets = targets.view(-<span class="number">1</span>)</span><br><span class="line">        BCE = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        BCE_EXP = torch.exp(-BCE)</span><br><span class="line">        focal_loss = alpha * (<span class="number">1</span>-BCE_EXP)**gamma * BCE</span><br><span class="line">                       </span><br><span class="line">        <span class="keyword">return</span> focal_loss</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="动态调整学习率"><a href="#动态调整学习率" class="headerlink" title="动态调整学习率"></a>动态调整学习率</h1><ol>
<li><p>使用scheduler<br>PyTorch在torch.optim.lr_scheduler封装好了一些动态调整学习率的方法。</p>
<ul>
<li>lr_scheduler.LambdaLR</li>
<li>lr_scheduler.MultiplicativeLR</li>
<li>lr_scheduler.StepLR</li>
<li>lr_scheduler.MultiStepLR</li>
<li>lr_scheduler.ExponentialLR</li>
<li>lr_scheduler.CosineAnnealingLR</li>
<li>lr_scheduler.ReduceLROnPlateau</li>
<li>lr_scheduler.CyclicLR</li>
<li>lr_scheduler.OneCycleLR</li>
<li>lr_scheduler.CosineAnnealingWarmRestarts</li>
</ul>
<p> 将scheduler.step()放在optimizer.step()后面进行使用。<br> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择一种优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(...) </span><br><span class="line"><span class="comment"># 选择上面提到的一种或多种动态调整学习率的方法</span></span><br><span class="line">scheduler1 = torch.optim.lr_scheduler.... </span><br><span class="line">scheduler2 = torch.optim.lr_scheduler....</span><br><span class="line">...</span><br><span class="line">schedulern = torch.optim.lr_scheduler....</span><br><span class="line"><span class="comment"># 进行训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 需要在优化器参数更新之后再动态调整学习率</span></span><br><span class="line">    scheduler1.step() </span><br><span class="line">    ...</span><br><span class="line">    schedulern.step()</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>自定义scheduler</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span></span><br><span class="line">    lr = args.lr * (<span class="number">0.1</span> ** (epoch // <span class="number">30</span>)) <span class="comment"># 学习率每30轮下降为原来的1/10</span></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = <span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    adjust_learning_rate(optimizer,epoch)        </span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="模型微调-torchvision"><a href="#模型微调-torchvision" class="headerlink" title="模型微调-torchvision"></a>模型微调-torchvision</h1><ol>
<li>模型微调-torchvision<ul>
<li>在源数据集上预训练一个源模型</li>
<li>创建一个新的目标模型，复制源模型除了输出层外的所有结构，其参数学习到了源数据集的知识，假设其同样适用于目标数据集，且源模型输出层和源数据集标签密切相关，所以目标模型不采用它。</li>
<li>为目标层添加目标数据集类别个数的输出层，随机初始化模型参数。</li>
<li>在目标数据集上训练目标模型，从头训练输出层，其他层参数微调。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/finetune.png"></li>
</ul>
</li>
<li>使用已有模型结构<ul>
<li>实例化网络 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18()</span><br><span class="line"><span class="comment"># resnet18 = models.resnet18(pretrained=False)  等价于与上面的表达式</span></span><br><span class="line">alexnet = models.alexnet()</span><br><span class="line">vgg16 = models.vgg16()</span><br><span class="line">squeezenet = models.squeezenet1_0()</span><br><span class="line">densenet = models.densenet161()</span><br><span class="line">inception = models.inception_v3()</span><br><span class="line">googlenet = models.googlenet()</span><br><span class="line">shufflenet = models.shufflenet_v2_x1_0()</span><br><span class="line">mobilenet_v2 = models.mobilenet_v2()</span><br><span class="line">mobilenet_v3_large = models.mobilenet_v3_large()</span><br><span class="line">mobilenet_v3_small = models.mobilenet_v3_small()</span><br><span class="line">resnext50_32x4d = models.resnext50_32x4d()</span><br><span class="line">wide_resnet50_2 = models.wide_resnet50_2()</span><br><span class="line">mnasnet = models.mnasnet1_0()</span><br></pre></td></tr></table></figure></li>
<li>传递pretrained参数 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">alexnet = models.alexnet(pretrained=<span class="literal">True</span>)</span><br><span class="line">squeezenet = models.squeezenet1_0(pretrained=<span class="literal">True</span>)</span><br><span class="line">vgg16 = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">densenet = models.densenet161(pretrained=<span class="literal">True</span>)</span><br><span class="line">inception = models.inception_v3(pretrained=<span class="literal">True</span>)</span><br><span class="line">googlenet = models.googlenet(pretrained=<span class="literal">True</span>)</span><br><span class="line">shufflenet = models.shufflenet_v2_x1_0(pretrained=<span class="literal">True</span>)</span><br><span class="line">mobilenet_v2 = models.mobilenet_v2(pretrained=<span class="literal">True</span>)</span><br><span class="line">mobilenet_v3_large = models.mobilenet_v3_large(pretrained=<span class="literal">True</span>)</span><br><span class="line">mobilenet_v3_small = models.mobilenet_v3_small(pretrained=<span class="literal">True</span>)</span><br><span class="line">resnext50_32x4d = models.resnext50_32x4d(pretrained=<span class="literal">True</span>)</span><br><span class="line">wide_resnet50_2 = models.wide_resnet50_2(pretrained=<span class="literal">True</span>)</span><br><span class="line">mnasnet = models.mnasnet1_0(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
 pytorch模型扩展为.pt .pth，模型权重一旦被下载下次就不需要加载，可以将自己的权重下载下来放到同文件夹下，然后再将参数加载网络。如果中途强行停止下载的话，一定要去对应路径下将权重文件删除干净，要不然可能会报错。 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.model = models.resnet50(pretrained=<span class="literal">False</span>)</span><br><span class="line">self.model.load_state_dict(torch.load(<span class="string">&#x27;./model/resnet50-19c8e357.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>训练特定层<br>如果我们正在提取特征并且只想为新初始化的层计算梯度，其他参数不进行改变。那我们就需要通过设置requires_grad &#x3D; False来冻结部分层。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_parameter_requires_grad</span>(<span class="params">model, feature_extracting</span>):</span></span><br><span class="line">    <span class="keyword">if</span> feature_extracting:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="comment"># 冻结参数的梯度</span></span><br><span class="line">feature_extract = <span class="literal">True</span></span><br><span class="line">model = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">set_parameter_requires_grad(model, feature_extract)</span><br><span class="line"><span class="comment"># 修改模型</span></span><br><span class="line">num_ftrs = model.fc.in_features</span><br><span class="line">model.fc = nn.Linear(in_features=num_ftrs, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
之后在训练过程中，model仍会进行梯度回传，但是参数更新则只会发生在fc层。</li>
</ol>
<h1 id="模型微调-timm"><a href="#模型微调-timm" class="headerlink" title="模型微调 - timm"></a>模型微调 - timm</h1><p>torchvision的扩充版本</p>
<ol>
<li>查看预训练模型种类<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timm</span><br><span class="line">avail_pretrained_models = timm.list_models(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">len</span>(avail_pretrained_models)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模糊查询</span></span><br><span class="line">all_densnet_models = timm.list_models(<span class="string">&quot;*densenet*&quot;</span>)</span><br><span class="line">all_densnet_models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型参数</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,num_classes=<span class="number">10</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line">model.default_cfg</span><br></pre></td></tr></table></figure></li>
<li>使用和修改预训练模型<br>通过timm.create_model()的方法来进行模型的创建，传入参数pretrained&#x3D;True，来使用预训练模型。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line">x = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">output = model(x)</span><br><span class="line">output.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某一层模型参数</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">dict</span>(model.named_children())[<span class="string">&#x27;conv1&#x27;</span>].parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改模型</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,num_classes=<span class="number">10</span>,pretrained=<span class="literal">True</span>)</span><br><span class="line">x = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">output = model(x)</span><br><span class="line">output.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变输入通道数</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,num_classes=<span class="number">10</span>,pretrained=<span class="literal">True</span>,in_chans=<span class="number">1</span>)</span><br><span class="line">x = torch.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">output = model(x)</span><br></pre></td></tr></table></figure></li>
<li>模型保存<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(),<span class="string">&#x27;./checkpoint/timm_model.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;./checkpoint/timm_model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="半精度训练"><a href="#半精度训练" class="headerlink" title="半精度训练"></a>半精度训练</h1><p>GPU的性能主要分为两部分：算力和显存，前者决定了显卡计算的速度，后者则决定了显卡可以同时放入多少数据用于计算。在可以使用的显存数量一定的情况下，每次训练能够加载的数据更多（也就是batch size更大），则也可以提高训练效率。另外，有时候数据本身也比较大（比如3D图像、视频等），显存较小的情况下可能甚至batch size为1的情况都无法实现。因此，合理使用显存也就显得十分重要。</p>
<p>我们观察PyTorch默认的浮点数存储方式用的是torch.float32，小数点后位数更多固然能保证数据的精确性，但绝大多数场景其实并不需要这么精确，只保留一半的信息也不会影响结果，也就是使用torch.float16格式。由于数位减了一半，因此被称为“半精度”。显然半精度能够减少显存占用，使得显卡可以同时加载更多数据进行计算。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/float16.jpg"></p>
<ol>
<li>半精度训练的设置<br>import autocast<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br></pre></td></tr></table></figure>
在模型定义中，使用python的装饰器方法，用autocast装饰模型中的forward函数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@autocast()   </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
在训练过程中，只需在将数据输入模型及其之后的部分放入“with autocast():“即可：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> train_loader:</span><br><span class="line">x = x.cuda()</span><br><span class="line"><span class="keyword">with</span> autocast():</span><br><span class="line">       output = model(x)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="数据增强-imgaug"><a href="#数据增强-imgaug" class="headerlink" title="数据增强-imgaug"></a>数据增强-imgaug</h1><ol>
<li>单张图片处理<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">import</span> imgaug <span class="keyword">as</span> ia</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片的读取</span></span><br><span class="line">img = imageio.imread(<span class="string">&quot;./Lenna.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Image进行读取</span></span><br><span class="line"><span class="comment"># img = Image.open(&quot;./Lenna.jpg&quot;)</span></span><br><span class="line"><span class="comment"># image = np.array(img)</span></span><br><span class="line"><span class="comment"># ia.imshow(image)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化图片</span></span><br><span class="line">ia.imshow(img)</span><br></pre></td></tr></table></figure>
imgaug包含了许多从Augmenter继承的数据增强的操作<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imgaug <span class="keyword">import</span> augmenters <span class="keyword">as</span> iaa</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子</span></span><br><span class="line">ia.seed(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化方法</span></span><br><span class="line">rotate = iaa.Affine(rotate=(-<span class="number">4</span>,<span class="number">45</span>))</span><br><span class="line">img_aug = rotate(image=img)</span><br><span class="line">ia.imshow(img_aug)</span><br></pre></td></tr></table></figure>
对一张图片做多种数据增强处理<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">iaa.Sequential(children=<span class="literal">None</span>, <span class="comment"># Augmenter集合</span></span><br><span class="line">               random_order=<span class="literal">False</span>, <span class="comment"># 是否对每个batch使用不同顺序的Augmenter list</span></span><br><span class="line">               name=<span class="literal">None</span>,</span><br><span class="line">               deterministic=<span class="literal">False</span>,</span><br><span class="line">               random_state=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 构建处理序列</span></span><br><span class="line">aug_seq = iaa.Sequential([</span><br><span class="line">    iaa.Affine(rotate=(-<span class="number">25</span>,<span class="number">25</span>)),</span><br><span class="line">    iaa.AdditiveGaussianNoise(scale=(<span class="number">10</span>,<span class="number">60</span>)),</span><br><span class="line">    iaa.Crop(percent=(<span class="number">0</span>,<span class="number">0.2</span>))</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 对图片进行处理，image不可以省略，也不能写成images</span></span><br><span class="line">image_aug = aug_seq(image=img)</span><br><span class="line">ia.imshow(image_aug)</span><br></pre></td></tr></table></figure></li>
<li>对批次图片进行处理<br>对批次的图片以同一种方式处理<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">images = [img,img,img,img,]</span><br><span class="line">images_aug = rotate(images=images)</span><br><span class="line">ia.imshow(np.hstack(images_aug))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对批次图片进行多种增强</span></span><br><span class="line">aug_seq = iaa.Sequential([</span><br><span class="line">    iaa.Affine(rotate=(-<span class="number">25</span>, <span class="number">25</span>)),</span><br><span class="line">    iaa.AdditiveGaussianNoise(scale=(<span class="number">10</span>, <span class="number">60</span>)),</span><br><span class="line">    iaa.Crop(percent=(<span class="number">0</span>, <span class="number">0.2</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 传入时需要指明是images参数</span></span><br><span class="line">images_aug = aug_seq.augment_images(images = images)</span><br><span class="line"><span class="comment">#images_aug = aug_seq(images = images) </span></span><br><span class="line">ia.imshow(np.hstack(images_aug))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对批次的图片分部分处理</span></span><br><span class="line">iaa.Sometimes(p=<span class="number">0.5</span>,  <span class="comment"># 代表划分比例</span></span><br><span class="line">              then_list=<span class="literal">None</span>,  <span class="comment"># Augmenter集合。p概率的图片进行变换的Augmenters。</span></span><br><span class="line">              else_list=<span class="literal">None</span>,  <span class="comment">#1-p概率的图片会被进行变换的Augmenters。注意变换的图片应用的Augmenter只能是then_list或者else_list中的一个。</span></span><br><span class="line">              name=<span class="literal">None</span>,</span><br><span class="line">              deterministic=<span class="literal">False</span>,</span><br><span class="line">              random_state=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不同大小的图片进行处理</span></span><br><span class="line"><span class="comment"># 构建pipline</span></span><br><span class="line">seq = iaa.Sequential([</span><br><span class="line">    iaa.CropAndPad(percent=(-<span class="number">0.2</span>, <span class="number">0.2</span>), pad_mode=<span class="string">&quot;edge&quot;</span>),  <span class="comment"># crop and pad images</span></span><br><span class="line">    iaa.AddToHueAndSaturation((-<span class="number">60</span>, <span class="number">60</span>)),  <span class="comment"># change their color</span></span><br><span class="line">    iaa.ElasticTransformation(alpha=<span class="number">90</span>, sigma=<span class="number">9</span>),  <span class="comment"># water-like effect</span></span><br><span class="line">    iaa.Cutout()  <span class="comment"># replace one squared area within the image by a constant intensity value</span></span><br><span class="line">], random_order=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载不同大小的图片</span></span><br><span class="line">images_different_sizes = [</span><br><span class="line">    imageio.imread(<span class="string">&quot;https://upload.wikimedia.org/wikipedia/commons/e/ed/BRACHYLAGUS_IDAHOENSIS.jpg&quot;</span>),</span><br><span class="line">    imageio.imread(<span class="string">&quot;https://upload.wikimedia.org/wikipedia/commons/c/c9/Southern_swamp_rabbit_baby.jpg&quot;</span>),</span><br><span class="line">    imageio.imread(<span class="string">&quot;https://upload.wikimedia.org/wikipedia/commons/9/9f/Lower_Keys_marsh_rabbit.jpg&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图片进行增强</span></span><br><span class="line">images_aug = seq(images=images_different_sizes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Image 0 (input shape: %s, output shape: %s)&quot;</span> % (images_different_sizes[<span class="number">0</span>].shape, images_aug[<span class="number">0</span>].shape))</span><br><span class="line">ia.imshow(np.hstack([images_different_sizes[<span class="number">0</span>], images_aug[<span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Image 1 (input shape: %s, output shape: %s)&quot;</span> % (images_different_sizes[<span class="number">1</span>].shape, images_aug[<span class="number">1</span>].shape))</span><br><span class="line">ia.imshow(np.hstack([images_different_sizes[<span class="number">1</span>], images_aug[<span class="number">1</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Image 2 (input shape: %s, output shape: %s)&quot;</span> % (images_different_sizes[<span class="number">2</span>].shape, images_aug[<span class="number">2</span>].shape))</span><br><span class="line">ia.imshow(np.hstack([images_different_sizes[<span class="number">2</span>], images_aug[<span class="number">2</span>]]))</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="使用argparse进行调参"><a href="#使用argparse进行调参" class="headerlink" title="使用argparse进行调参"></a>使用argparse进行调参</h1><p>直接在命令行中就可以向程序中传入参数。我们可以使用python file.py来运行python文件。而argparse的作用就是将命令行传入的其他参数进行解析、保存和使用。在使用argparse后，我们在命令行输入的参数就可以以这种形式python file.py –lr 1e-4 –batch_size 32来完成对常见超参数的设置。</p>
<h2 id="argparse的使用"><a href="#argparse的使用" class="headerlink" title="argparse的使用"></a>argparse的使用</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># demo.py</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建ArgumentParser()对象</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加参数</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-o&#x27;</span>, <span class="string">&#x27;--output&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, </span><br><span class="line">    <span class="built_in">help</span>=<span class="string">&quot;shows output&quot;</span>)</span><br><span class="line"><span class="comment"># action = `store_true` 会将output参数记录为True</span></span><br><span class="line"><span class="comment"># type 规定了参数的格式</span></span><br><span class="line"><span class="comment"># default 规定了默认值</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">3e-5</span>, <span class="built_in">help</span>=<span class="string">&#x27;select the learning rate, default=1e-3&#x27;</span>) </span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;input batch size&#x27;</span>)  </span><br><span class="line"><span class="comment"># 使用parse_args()解析函数</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.output:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;This is some output&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;learning rate:<span class="subst">&#123;args.lr&#125;</span> &quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输入python demo.py –lr 3e-4 –batch_size 32，得到：</p>
<pre><code>This is some output
learning rate: 3e-4
</code></pre>
<h2 id="更加高效使用argparse修改超参数"><a href="#更加高效使用argparse修改超参数" class="headerlink" title="更加高效使用argparse修改超参数"></a>更加高效使用argparse修改超参数</h2><p>为了使代码更加简洁和模块化，我一般会将有关超参数的操作写在config.py，然后在train.py或者其他文件导入就可以。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_options</span>(<span class="params">parser=argparse.ArgumentParser(<span class="params"></span>)</span>):</span>  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--workers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>,  </span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of data loading workers, you had better put it &#x27;</span>  </span><br><span class="line">                              <span class="string">&#x27;4 times of your gpu&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>, <span class="built_in">help</span>=<span class="string">&#x27;input batch size, default=64&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--niter&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>, <span class="built_in">help</span>=<span class="string">&#x27;number of epochs to train for, default=10&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">3e-5</span>, <span class="built_in">help</span>=<span class="string">&#x27;select the learning rate, default=1e-3&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">118</span>, <span class="built_in">help</span>=<span class="string">&quot;random seed&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;enables cuda&#x27;</span>)  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--checkpoint_path&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,default=<span class="string">&#x27;&#x27;</span>,  </span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Path to load a previous trained model if not empty (default empty)&#x27;</span>)  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output&#x27;</span>,action=<span class="string">&#x27;store_true&#x27;</span>,default=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&quot;shows output&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    opt = parser.parse_args()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> opt.output:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;num_workers: <span class="subst">&#123;opt.workers&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;batch_size: <span class="subst">&#123;opt.batch_size&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epochs (niters) : <span class="subst">&#123;opt.niter&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;learning rate : <span class="subst">&#123;opt.lr&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;manual_seed: <span class="subst">&#123;opt.seed&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;cuda enable: <span class="subst">&#123;opt.cuda&#125;</span>&#x27;</span>)  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;checkpoint_path: <span class="subst">&#123;opt.checkpoint_path&#125;</span>&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> opt  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  </span><br><span class="line">    opt = get_options()</span><br></pre></td></tr></table></figure>
<p>随后在train.py等其他文件，我们就可以使用下面的这样的结构来调用参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要库</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"></span><br><span class="line">opt = config.get_options()</span><br><span class="line"></span><br><span class="line">manual_seed = opt.seed</span><br><span class="line">num_workers = opt.workers</span><br><span class="line">batch_size = opt.batch_size</span><br><span class="line">lr = opt.lr</span><br><span class="line">niters = opt.niters</span><br><span class="line">checkpoint_path = opt.checkpoint_path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机数的设置，保证复现结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seed</span>(<span class="params">seed</span>):</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	set_seed(manual_seed)</span><br><span class="line">	<span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(niters):</span><br><span class="line">		train(model,lr,batch_size,num_workers,checkpoint_path)</span><br><span class="line">		val(model,lr,batch_size,num_workers,checkpoint_path)</span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E5%85%AD%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-16T06:43:54.000Z" title="2022/10/16 下午2:43:54">2022-10-16</time>发表</span><span class="level-item"><time dateTime="2022-10-16T12:34:44.177Z" title="2022/10/16 下午8:34:44">2022-10-16</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">12 分钟读完 (大约1837个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/16/pytorch-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/">pytorch - 模型定义</a></h1><div class="content"><h1 id="模型定义的方式"><a href="#模型定义的方式" class="headerlink" title="模型定义的方式"></a>模型定义的方式</h1><p>基于nn.Module，可以通过Sequential，ModuleList和ModuleDict三种方式定义PyTorch模型。</p>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>将模型的层按序排列起来，按顺序读取，不用写forward，但丧失灵活性</p>
<ol>
<li>Sequential<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Sequential: Direct list</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">net1 = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>), </span><br><span class="line">        )</span><br><span class="line"><span class="built_in">print</span>(net1)</span><br></pre></td></tr></table></figure>

<pre><code> Sequential(
   (0): Linear(in_features=784, out_features=256, bias=True)
   (1): ReLU()
   (2): Linear(in_features=256, out_features=10, bias=True)
 )
</code></pre>
</li>
<li>Ordered Dict<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">net2 = nn.Sequential(collections.OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(<span class="number">784</span>, <span class="number">256</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">          ]))</span><br><span class="line"><span class="built_in">print</span>(net2)</span><br></pre></td></tr></table></figure>

<pre><code> Sequential(
   (fc1): Linear(in_features=784, out_features=256, bias=True)
   (relu1): ReLU()
   (fc2): Linear(in_features=256, out_features=10, bias=True)
 )
</code></pre>
</li>
</ol>
<h2 id="ModuleList"><a href="#ModuleList" class="headerlink" title="ModuleList"></a>ModuleList</h2><p>ModuleList 接收一个子模块（或层，需属于nn.Module类）的列表作为输入，类似List那样进行append和extend操作。同时，子模块或层的权重也会自动添加到网络中来。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net3 = nn.ModuleList([nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU()])</span><br><span class="line">net3.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>)) <span class="comment"># # 类似List的append操作</span></span><br><span class="line"><span class="built_in">print</span>(net3[-<span class="number">1</span>])  <span class="comment"># 类似List的索引访问</span></span><br><span class="line"><span class="built_in">print</span>(net3)</span><br></pre></td></tr></table></figure>

<p>ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起。把modellist写到初始化，再定义forward函数明确传输顺序。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net3</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.modulelist = nn.ModuleList([nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU()])</span><br><span class="line">        self.modulelist.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.modulelist:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net3_ = Net3()</span><br><span class="line">out3_ = net3_(a)</span><br><span class="line"><span class="built_in">print</span>(out3_.shape)</span><br></pre></td></tr></table></figure>

<h2 id="ModuleDict"><a href="#ModuleDict" class="headerlink" title="ModuleDict"></a>ModuleDict</h2><p>ModuleDict和ModuleList的作用类似，只是ModuleDict能够更方便地为神经网络的层添加名称。同样地，ModuleDict并没有定义一个网络，它只是将不同的模块储存在一起，要定义forward。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleDict(&#123;</span><br><span class="line">    <span class="string">&#x27;linear&#x27;</span>: nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    <span class="string">&#x27;act&#x27;</span>: nn.ReLU(),</span><br><span class="line">&#125;)</span><br><span class="line">net[<span class="string">&#x27;output&#x27;</span>] = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="string">&#x27;linear&#x27;</span>]) <span class="comment"># 访问</span></span><br><span class="line"><span class="built_in">print</span>(net.output)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<h1 id="利用模型块快速搭建复杂网络"><a href="#利用模型块快速搭建复杂网络" class="headerlink" title="利用模型块快速搭建复杂网络"></a>利用模型块快速搭建复杂网络</h1><p>当模型有很多层的时候，其中很多重复出现的结构可以定义为一个模块，便利模型构建。<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/5.2.1unet.png"><br>如U-Net所示，模型左右对称，每个子层内部有两次卷积，左侧下采样连接，右侧上采样连接，每层模型块和上下模型块连接，同层的左右模型块连接。</p>
<ol>
<li>双次卷积<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleConv</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;(convolution =&gt; [BN] =&gt; ReLU) * 2&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, mid_channels=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> mid_channels:</span><br><span class="line">            mid_channels = out_channels</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, mid_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(mid_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(mid_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.double_conv(x)</span><br></pre></td></tr></table></figure></li>
<li>下采样<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Down</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Downscaling with maxpool then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.maxpool_conv(x)</span><br></pre></td></tr></table></figure></li>
<li>上采样<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Up</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Upscaling then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if bilinear, use the normal convolutions to reduce the number of channels</span></span><br><span class="line">        <span class="keyword">if</span> bilinear: <span class="comment"># 插值</span></span><br><span class="line">            self.up = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">            self.conv = DoubleConv(in_channels, out_channels, in_channels // <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.up = nn.ConvTranspose2d(in_channels, in_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">            self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x1, x2</span>):</span></span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        <span class="comment"># input is CHW</span></span><br><span class="line">        diffY = x2.size()[<span class="number">2</span>] - x1.size()[<span class="number">2</span>]</span><br><span class="line">        diffX = x2.size()[<span class="number">3</span>] - x1.size()[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // <span class="number">2</span>, diffX - diffX // <span class="number">2</span>,</span><br><span class="line">                        diffY // <span class="number">2</span>, diffY - diffY // <span class="number">2</span>])</span><br><span class="line">        <span class="comment"># if you have padding issues, see</span></span><br><span class="line">        <span class="comment"># https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a</span></span><br><span class="line">        <span class="comment"># https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd</span></span><br><span class="line">        x = torch.cat([x2, x1], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 连接左侧的数据再卷积</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure></li>
<li>输出<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutConv</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure></li>
<li>组装<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span> // factor)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">unet = UNet(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">unet</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="模型修改"><a href="#模型修改" class="headerlink" title="模型修改"></a>模型修改</h1><p>当有一个现成的模型需要对结构进行修改使用时，我们可以在已有模型上修改。</p>
<ol>
<li>修改模型层<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">unet1 = copy.deepcopy(unet)</span><br><span class="line">unet1.outc</span><br></pre></td></tr></table></figure>
先复制，然后修改outc<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">out_unet1 = unet1(b)</span><br><span class="line"><span class="built_in">print</span>(out_unet1.shape)</span><br></pre></td></tr></table></figure>
要把输出Chanel变成5，重新实例化outc<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unet1.outc = OutConv(<span class="number">64</span>, <span class="number">5</span>)</span><br><span class="line">unet1.outc</span><br><span class="line">out_unet1 = unet1(b)</span><br><span class="line"><span class="built_in">print</span>(out_unet1.shape)</span><br></pre></td></tr></table></figure></li>
<li>添加额外输入<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet2, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span> // factor)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, add_variable</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        x = x + add_variable   <span class="comment">#修改点</span></span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">unet2 = UNet2(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">c = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">out_unet2 = unet2(b, c)</span><br><span class="line"><span class="built_in">print</span>(out_unet2.shape)</span><br></pre></td></tr></table></figure>
或用torch.cat实现了tensor的拼接，如x &#x3D; torch.cat((self.dropout(self.relu(x)), add_variable.unsqueeze(1)),1)。</li>
<li>添加额外输出<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet3</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet3, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span> // factor)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits, x5  <span class="comment"># 修改点</span></span><br><span class="line">unet3 = UNet3(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">c = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">out_unet3, mid_out = unet3(b)</span><br><span class="line"><span class="built_in">print</span>(out_unet3.shape, mid_out.shape)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="模型保存和读取"><a href="#模型保存和读取" class="headerlink" title="模型保存和读取"></a>模型保存和读取</h1><p>单卡&#x2F;多卡，整个&#x2F;部分模型, unet.state_dict()查看模型权重，保存的模型格式： pt pth pkl。</p>
<ol>
<li><p>CPU或单卡：保存&amp;读取整个模型</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(unet, <span class="string">&quot;./unet_example.pth&quot;</span>)</span><br><span class="line">loaded_unet = torch.load(<span class="string">&quot;./unet_example.pth&quot;</span>)</span><br><span class="line">loaded_unet.state_dict()</span><br></pre></td></tr></table></figure></li>
<li><p>CPU或单卡：保存&amp;读取模型权重</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(unet.state_dict(), <span class="string">&quot;./unet_weight_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_weights = torch.load(<span class="string">&quot;./unet_weight_example.pth&quot;</span>)</span><br><span class="line">unet.load_state_dict(loaded_unet_weights) <span class="comment"># 用已经定义好的模型结构加载变量</span></span><br><span class="line">unet.state_dict()</span><br></pre></td></tr></table></figure></li>
<li><p>多卡：保存&amp;读取整个模型。注意模型层名称前多了module<br>不建议，因为保存模型的GPU_id等信息和读取后训练环境可能不同，尤其是要把保存的模型交给另一用户使用的情况</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;2,3&#x27;</span></span><br><span class="line">unet_mul = copy.deepcopy(unet)</span><br><span class="line">unet_mul = nn.DataParallel(unet_mul).cuda()</span><br><span class="line">torch.save(unet_mul, <span class="string">&quot;./unet_mul_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_mul = torch.load(<span class="string">&quot;./unet_mul_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_mul</span><br></pre></td></tr></table></figure>
</li>
<li><p>多卡：保存&amp;读取模型权重。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(unet_mul.state_dict(), <span class="string">&quot;./unet_weight_mul_example.pth&quot;</span>)</span><br><span class="line">loaded_unet_weights_mul = torch.load(<span class="string">&quot;./unet_weight_mul_example.pth&quot;</span>)</span><br><span class="line">unet_mul.load_state_dict(loaded_unet_weights_mul)</span><br><span class="line">unet_mul = nn.DataParallel(unet_mul).cuda()</span><br><span class="line">unet_mul.state_dict()</span><br></pre></td></tr></table></figure>
<p>另外，如果保存的是整个模型，也建议采用提取权重的方式构建新的模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unet_mul.state_dict = loaded_unet_mul.state_dict</span><br><span class="line">unet_mul = nn.DataParallel(unet_mul).cuda()</span><br><span class="line">unet_mul.state_dict()</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%94%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-12T06:43:54.000Z" title="2022/10/12 下午2:43:54">2022-10-12</time>发表</span><span class="level-item"><time dateTime="2022-10-12T15:33:48.906Z" title="2022/10/12 下午11:33:48">2022-10-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">1 小时读完 (大约11579个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/">pytorch - 各个组件和实践</a></h1><div class="content"><p>本章介绍pytorch进行深度学习模型训练的各个组件和实践，层层搭建神经网络模型。</p>
<h1 id="神经网络学习机制"><a href="#神经网络学习机制" class="headerlink" title="神经网络学习机制"></a>神经网络学习机制</h1><center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665563909211/D2B5CA33BD970F64A6301FA75AE2EB22" width="250">  
</center>

<ul>
<li>数据预处理<br>完成一项机器学习任务时的步骤，首先需要对数据进行预处理，其中重要的步骤包括数据格式的统一和必要的数据变换，同时划分训练集和测试集。</li>
<li>模型设计<br>选择模型。</li>
<li>损失函数和优化方案设计<br>设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失函数和优化器）。</li>
<li>前向传播<br>用模型去拟合训练集数据，</li>
<li>反向传播</li>
<li>更新参数</li>
<li>模型表现<br>在验证集&#x2F;测试集上计算模型表现。</li>
</ul>
<h1 id="深度学习在实现上的特殊性"><a href="#深度学习在实现上的特殊性" class="headerlink" title="深度学习在实现上的特殊性"></a>深度学习在实现上的特殊性</h1><ul>
<li>样本量大，需要分批加载<br>由于深度学习所需的样本量很大，一次加载全部数据运行可能会超出内存容量而无法实现；同时还有批（batch）训练等提高模型表现的策略，需要每次训练读取固定数量的样本送入模型中训练。</li>
<li>逐层、模块化搭建网络（卷积层、全连接层、LSTM等）<br>深度神经网络往往需要“逐层”搭建，或者预先定义好可以实现特定功能的模块，再把这些模块组装起来。</li>
<li>多样化的损失函数和优化器设计<br>由于模型设定的灵活性，因此损失函数和优化器要能够保证反向传播能够在用户自行定义的模型结构上实现</li>
<li>GPU的使用<br>需要把模型和数据“放到”GPU上去做运算，同时还需要保证损失函数和优化器能够在GPU上工作。如果使用多张GPU进行训练，还需要考虑模型和数据分配、整合的问题。</li>
<li>各个模块之间的配合<br>深度学习中训练和验证过程最大的特点在于读入数据是按批的，每次读入一个批次的数据，放入GPU中训练，然后将损失函数反向传播回网络最前面的层，同时使用优化器调整网络参数。这里会涉及到各个模块配合的问题。训练&#x2F;验证后还需要根据设定好的指标计算模型表现。</li>
</ul>
<h1 id="pytorch深度学习模块"><a href="#pytorch深度学习模块" class="headerlink" title="pytorch深度学习模块"></a>pytorch深度学习模块</h1><p>将PyTorch完成深度学习的步骤拆解为几个主要模块，实际使用根据自身需求修改对应模块即可，深度学习-&gt;搭积木。</p>
<h2 id="一、-基本配置"><a href="#一、-基本配置" class="headerlink" title="一、 基本配置"></a>一、 基本配置</h2><p>首先导入必要的包</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optimizer</span><br></pre></td></tr></table></figure>
<p>配置训练环境和超参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置GPU，这里有两种方式</span></span><br><span class="line"><span class="comment">## 方案一：使用os.environ，后续用.cuda()</span></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0&#x27;</span></span><br><span class="line"><span class="comment"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:1&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置其他超参数，如batch_size, num_workers, learning rate, 以及总的epochs</span></span><br><span class="line">batch_size = <span class="number">256</span> <span class="comment"># 每次训练读入的数据量</span></span><br><span class="line">num_workers = <span class="number">4</span>   <span class="comment"># 有多少线程来读入数据，对于Windows用户，这里应设置为0，否则会出现多线程错误</span></span><br><span class="line">lr = <span class="number">1e-4</span> <span class="comment"># 参数更新的步长</span></span><br><span class="line">epochs = <span class="number">20</span> <span class="comment"># 训练多少轮</span></span><br></pre></td></tr></table></figure>
<h2 id="二、-数据读入"><a href="#二、-数据读入" class="headerlink" title="二、 数据读入"></a>二、 数据读入</h2><p>有两种方式：</p>
<ul>
<li>下载并使用PyTorch提供的内置数据集<br>只适用于常见的数据集，如MNIST，CIFAR10等，PyTorch官方提供了数据下载。这种方式往往适用于快速测试方法（比如测试下某个idea在MNIST数据集上是否有效）<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先设置数据变换</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">28</span> </span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),   <span class="comment"># 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要</span></span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">train_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br><span class="line">test_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br></pre></td></tr></table></figure></li>
<li>从网站下载以csv格式存储的数据，读入并转成预期的格式<br>需要自己构建Dataset，这对于PyTorch应用于自己的工作中十分重要,同时，还需要对数据进行必要的变换，比如说需要将图片统一为一致的大小，以便后续能够输入网络训练；需要将数据格式转为Tensor类，等等。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式二：读入csv格式的数据，自行构建Dataset类</span></span><br><span class="line"><span class="comment"># csv数据下载链接：https://www.kaggle.com/zalando-research/fashionmnist</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FMDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.df = df</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.images = df.iloc[:,<span class="number">1</span>:].values.astype(np.uint8)</span><br><span class="line">        self.labels = df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.images)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        image = self.images[idx].reshape(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 1是单一通道</span></span><br><span class="line">        label = <span class="built_in">int</span>(self.labels[idx])</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image = torch.tensor(image/<span class="number">255.</span>, dtype=torch.<span class="built_in">float</span>) <span class="comment"># image/255 把数值归一化</span></span><br><span class="line">        label = torch.tensor(label, dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">train_df = pd.read_csv(<span class="string">&quot;./fashion-mnist_train.csv&quot;</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">&quot;./fashion-mnist_test.csv&quot;</span>)</span><br><span class="line">train_data = FMDataset(train_df, data_transform)</span><br><span class="line">test_data = FMDataset(test_df, data_transform)</span><br></pre></td></tr></table></figure>
PyTorch数据读入是通过Dataset+DataLoader的方式完成的，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：</li>
<li>_<em>init</em>_: 用于向类中传入外部参数，同时定义样本集</li>
<li>_<em>getitem</em>_: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练&#x2F;验证所需的数据</li>
<li>_<em>len</em>_: 用于返回数据集的样本数<br>在构建训练和测试数据集完成后，需要定义DataLoader类，以便在训练和测试时加载数据:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
读入后，我们可以做一些数据可视化操作，主要是验证我们读入的数据是否正确:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image, label = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="built_in">print</span>(image.shape, label.shape)</span><br><span class="line">plt.imshow(image[<span class="number">0</span>][<span class="number">0</span>], cmap=<span class="string">&quot;gray&quot;</span>)</span><br></pre></td></tr></table></figure>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665566323623/D2B5CA33BD970F64A6301FA75AE2EB22" width="250">  
</center></li>
</ul>
<h2 id="三、-模型构建"><a href="#三、-模型构建" class="headerlink" title="三、 模型构建"></a>三、 模型构建</h2><p>我们这里的任务是对10个类别的“时装”图像进行分类，FashionMNIST数据集中包含已经预先划分好的训练集和测试集，其中训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为32*32pixel，分属10个类别。由于任务较为简单，这里我们<strong>手搭一个CNN</strong>，而不考虑当下各种模型的复杂结构，模型构建完成后，将模型放到GPU上用于训练。<br>Module 类是nn模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__() </span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># x = nn.functional.normalize(x)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">model = model.cuda()</span><br><span class="line"><span class="comment"># model = nn.DataParallel(model).cuda()   # 多卡训练时的写法，之后的课程中会进一步讲解</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.nn.Conv2d(
  in_channels, 
  out_channels, 
  kernel_size, 
  stride=1, 
  padding=0, 
  dilation=1, 
  groups=1, 
  bias=True, 
  padding_mode=&#39;zeros&#39;, 
  device=None, 
  dtype=None)

torch.nn.MaxPool2d(
  kernel_size, 
  stride=None, 
  padding=0, 
  dilation=1, 
  return_indices=False, 
  ceil_mode=False)
</code></pre>
<p>$d_{out} &#x3D;(d_{in}−dilation∗(kernelsize−1)−1+2∗padding)&#x2F;stride+1)$<br><strong>下面再举一个其他模型MLP：</strong><br>继承Module类构造多层感知机,这里定义的MLP类重载了Module类的init函数和forward函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 backward 函数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)   </span><br></pre></td></tr></table></figure>
<p>我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 call 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">2</span>,<span class="number">784</span>)</span><br><span class="line">net = MLP()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p>注意，这里并没有将 Module 类命名为 Layer (层)或者 Model (模型)之类的名字，这是因为该类是一个可供⾃由组建的部件。它的子类既可以是⼀个层(如PyTorch提供的 Linear 类)，⼜可以是一个模型(如这里定义的 MLP 类)，或者是模型的⼀个部分。<br><strong>下面介绍一些神经网络中常见的层：</strong><br>深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层、卷积层、池化层与循环层等等。虽然PyTorch提供了⼤量常用的层，但有时候我们依然希望⾃定义层。</p>
<ol>
<li>不含模型参数的层<br>下⾯构造的 <strong>MyLayer</strong> 类通过继承 Module 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 forward 函数里。这个层里不含模型参数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyLayer, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()  </span><br><span class="line"><span class="comment"># 测试，实例化该层，然后做前向计算        </span></span><br><span class="line">layer = MyLayer()</span><br><span class="line">layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br></pre></td></tr></table></figure></li>
<li>含模型参数的层<br>自定义含模型参数的自定义层，其中的模型参数可以通过训练学出。<strong>Parameter</strong> 类其实是 Tensor 的子类，如果一个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ <strong>ParameterList</strong> 和 <strong>ParameterDict</strong> 分别定义参数的列表和字典。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyListDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i]) <span class="comment"># torch.mm矩阵相乘，两个二维张量相乘</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = MyListDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line">net = MyDictDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。</li>
<li>二维卷积层<br>二维卷积层将输入和<strong>卷积核</strong>做互相关运算，并加上一个<strong>标量偏差</strong>来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积运算（二维互相关）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span> </span><br><span class="line">    h, w = K.shape</span><br><span class="line">    X, K = X.<span class="built_in">float</span>(), K.<span class="built_in">float</span>()</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。给定一个高和宽为8的输入，我们发现输出的高和宽也是8。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span></span><br><span class="line">    <span class="comment"># (1, 1)代表批量大小和通道数</span></span><br><span class="line">    X = X.view((<span class="number">1</span>, <span class="number">1</span>) + X.shape) <span class="comment"># 加上两个维度</span></span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.view(Y.shape[<span class="number">2</span>:]) <span class="comment"># 排除不关心的前两维:批量和通道</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 ( 为大于1的整数)。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1，-5+2*2=-3+2*1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></li>
<li>池化层<br>池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。下面把池化层的前向计算实现在pool2d函数里。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
我们可以使用torch.nn包来构建神经网络。我们已经介绍了autograd包，nn包则依赖于autograd包来定义模型并对它们求导。一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。</li>
<li>LeNet模型示例<center> 
<img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.1.png" width="480">  
</center>
这是一个简单的前馈神经网络 (feed-forward network）（LeNet）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。一个神经网络的典型训练过程如下：
定义包含一些可学习参数(或者叫权重）的神经网络
在输入数据集上迭代
通过网络处理输入
计算 loss (输出和正确答案的距离）
将梯度反向传播给网络的参数
更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入图像channel：1；输出channel：6；5x5卷积核</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 2x2 Max pooling</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是方阵,则可以只使用一个数字进行定义</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 除去批处理维度的其他所有维度</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<pre><code> Net(
   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
   (fc1): Linear(in_features=400, out_features=120, bias=True)
   (fc2): Linear(in_features=120, out_features=84, bias=True)
   (fc3): Linear(in_features=84, out_features=10, bias=True)
 )
</code></pre>
</li>
</ol>
<p>我们只需要定义 forward 函数，backward函数会在使用autograd时自动定义，backward函数用来计算导数。我们可以在 forward 函数中使用任何针对张量的操作和计算。一个模型的可学习参数可以通过net.parameters()返回。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1的权重</span></span><br></pre></td></tr></table></figure>
<p>尝试随机输入</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="comment"># 清零所有参数的梯度缓存，然后进行随机梯度的反向传播：</span></span><br><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>torch.nn只支持小批量处理 (mini-batches）。整个 torch.nn 包只支持小批量样本的输入，不支持单个样本的输入。比如，nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width 如果是一个单独的样本，只需要使用input.unsqueeze(0) 来添加一个“假的”批大小维度。<br>torch.Tensor - 一个多维数组，支持诸如backward()等的自动求导操作，同时也保存了张量的梯度。<br>nn.Module - 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。<br>nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。<br>autograd.Function - 实现了自动求导前向和反向传播的定义，每个Tensor至少创建一个Function节点，该节点连接到创建Tensor的函数并对其历史进行编码。</p>
<ol>
<li>AlexNet模型示例<center> 
<img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.4.2.png" width="480">  
</center></li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">net = AlexNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<pre><code>AlexNet(
  (conv): Sequential(
    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU()
    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU()
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc): Sequential(
    (0): Linear(in_features=6400, out_features=4096, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
</code></pre>
<h2 id="四、-模型初始化"><a href="#四、-模型初始化" class="headerlink" title="四、 模型初始化"></a>四、 模型初始化</h2><ol>
<li>torch.nn.init使用<br>通常使用isinstance来进行判断模块<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">linear = nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">isinstance</span>(conv,nn.Conv2d)</span><br><span class="line"><span class="built_in">isinstance</span>(linear,nn.Conv2d)</span><br></pre></td></tr></table></figure>
查看不同初始化参数<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看随机初始化的conv参数</span></span><br><span class="line">conv.weight.data</span><br><span class="line"><span class="comment"># 查看linear的参数</span></span><br><span class="line">linear.weight.data</span><br></pre></td></tr></table></figure>
对不同类型层进行初始化<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对conv进行kaiming初始化</span></span><br><span class="line">torch.nn.init.kaiming_normal_(conv.weight.data)</span><br><span class="line">conv.weight.data</span><br><span class="line"><span class="comment"># 对linear进行常数初始化</span></span><br><span class="line">torch.nn.init.constant_(linear.weight.data,<span class="number">0.3</span>)</span><br><span class="line">linear.weight.data</span><br></pre></td></tr></table></figure></li>
<li>初始化函数的封装<br>人们常常将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">		<span class="comment"># 判断是否属于Conv2d</span></span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">			torch.nn.init.xavier_normal_(m.weight.data)</span><br><span class="line">			<span class="comment"># 判断是否有偏置</span></span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				torch.nn.init.constant_(m.bias.data,<span class="number">0.3</span>)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">			torch.nn.init.normal_(m.weight.data, <span class="number">0.1</span>)</span><br><span class="line">			<span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">				torch.nn.init.zeros_(m.bias.data)</span><br><span class="line">		<span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">			m.weight.data.fill_(<span class="number">1</span>) 		 </span><br><span class="line">			m.bias.data.zeros_()	</span><br></pre></td></tr></table></figure>
这段代码流程是遍历当前模型的每一层，然后判断各层属于什么类型，然后根据不同类型层，设定不同的权值初始化方法。我们可以通过下面的例程进行一个简短的演示：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型的定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)</span><br><span class="line"></span><br><span class="line">mlp = MLP()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(mlp.parameters()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------初始化-------&quot;</span>)</span><br><span class="line"></span><br><span class="line">initialize_weights(mlp)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(mlp.parameters()))</span><br></pre></td></tr></table></figure>

<pre><code> [Parameter containing:
 tensor([[[[ 0.2103, -0.1679,  0.1757],
           [-0.0647, -0.0136, -0.0410],
           [ 0.1371, -0.1738, -0.0850]]]], requires_grad=True), Parameter containing:
 tensor([0.2507], requires_grad=True), Parameter containing:
 tensor([[ 0.2790, -0.1247,  0.2762,  0.1149, -0.2121, -0.3022, -0.1859,  0.2983,
         -0.0757, -0.2868]], requires_grad=True), Parameter containing:
 tensor([-0.0905], requires_grad=True)]
 &quot;-------初始化-------&quot;
 [Parameter containing:
 tensor([[[[-0.3196, -0.0204, -0.5784],
           [ 0.2660,  0.2242, -0.4198],
           [-0.0952,  0.6033, -0.8108]]]], requires_grad=True),
 Parameter containing:
 tensor([0.3000], requires_grad=True),
 Parameter containing:
 tensor([[ 0.7542,  0.5796,  2.2963, -0.1814, -0.9627,  1.9044,  0.4763,  1.2077,
           0.8583,  1.9494]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]
</code></pre>
</li>
</ol>
<h2 id="五、-损失函数"><a href="#五、-损失函数" class="headerlink" title="五、 损失函数"></a>五、 损失函数</h2><p>这里使用torch.nn模块自带的CrossEntropy损失，PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss，这里需要确保label是从0开始的，同时模型不加softmax层（使用logits计算）,这也说明了PyTorch训练中各个部分不是独立的，需要通盘考虑。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<h3 id="1-二分类交叉熵损失函数"><a href="#1-二分类交叉熵损失函数" class="headerlink" title="1. 二分类交叉熵损失函数"></a>1. 二分类交叉熵损失函数</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BCELoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。<br><strong>主要参数</strong>：<br><code>weight</code>:每个类别的loss设置权值<br><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。<br><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。<br>计算公式如下：</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580819001/D2B5CA33BD970F64A6301FA75AE2EB22" width="290">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;BCELoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>BCELoss损失函数的计算结果为 tensor(0.5732, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
</code></pre>
<h3 id="2-交叉熵损失函数"><a href="#2-交叉熵损失函数" class="headerlink" title="2. 交叉熵损失函数"></a>2. 交叉熵损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=-<span class="number">100</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算交叉熵函数<br><strong>主要参数</strong>：<br><code>weight</code>:每个类别的loss设置权值。<br><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。<br><code>ignore_index</code>:忽略某个类的损失函数。<br><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。<br>计算公式如下：<br>$<br>\operatorname{loss}(x, \text { class })&#x3D;-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)&#x3D;-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.0115, grad_fn=&lt;NllLossBackward&gt;)
</code></pre>
<h3 id="3-L1损失函数"><a href="#3-L1损失函数" class="headerlink" title="3. L1损失函数"></a>3. L1损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.L1Loss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算输出<code>y</code>和真实标签<code>target</code>之间的差值的绝对值。<code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。mean：加权平均，返回标量。如果选择<code>none</code>，那么返回的结果是和输入元素相同尺寸的。默认计算方式是求平均。<br><strong>计算公式如下：</strong><br>$<br>L_{n} &#x3D; |x_{n}-y_{n}|<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.L1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;L1损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>L1损失函数的计算结果为 tensor(1.5729, grad_fn=&lt;L1LossBackward&gt;)
</code></pre>
<h3 id="4-MSE损失函数"><a href="#4-MSE损失函数" class="headerlink" title="4. MSE损失函数"></a>4. MSE损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MSELoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算输出<code>y</code>和真实标签<code>target</code>之差的平方。</p>
<p>和<code>L1Loss</code>一样，<code>MSELoss</code>损失函数中，<code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。默认计算方式是求平均。</p>
<p><strong>计算公式如下：</strong></p>
<p>$<br>l_{n}&#x3D;\left(x_{n}-y_{n}\right)^{2}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MSE损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MSE损失函数的计算结果为 tensor(1.6968, grad_fn=&lt;MseLossBackward&gt;)
</code></pre>
<h3 id="5-平滑L1-Smooth-L1-损失函数"><a href="#5-平滑L1-Smooth-L1-损失函数" class="headerlink" title="5. 平滑L1 (Smooth L1)损失函数"></a>5. 平滑L1 (Smooth L1)损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.SmoothL1Loss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, beta=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> L1的平滑输出，其功能是减轻离群点带来的影响</p>
<p><code>reduction</code>参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。sum：所有元素求和，返回标量。默认计算方式是求平均。</p>
<p><strong>提醒：</strong> 之后的损失函数中，关于<code>reduction</code> 这个参数依旧会存在。所以，之后就不再单独说明。</p>
<p><strong>计算公式如下：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\frac{1}{n} \sum_{i&#x3D;1}^{n} z_{i}<br>$<br>其中，</p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665580768639/D2B5CA33BD970F64A6301FA75AE2EB22" width="290">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.SmoothL1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SmoothL1Loss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>SmoothL1Loss损失函数的计算结果为 tensor(0.7808, grad_fn=&lt;SmoothL1LossBackward&gt;)
</code></pre>
<p><strong>平滑L1与L1的对比</strong></p>
<p>这里我们通过可视化两种损失函数曲线来对比平滑L1和L1两种损失函数的区别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.linspace(-<span class="number">10</span>, <span class="number">10</span>, steps=<span class="number">5000</span>)</span><br><span class="line">target = torch.zeros_like(inputs)</span><br><span class="line"></span><br><span class="line">loss_f_smooth = nn.SmoothL1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_smooth = loss_f_smooth(inputs, target)</span><br><span class="line">loss_f_l1 = nn.L1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_l1 = loss_f_l1(inputs,target)</span><br><span class="line"></span><br><span class="line">plt.plot(inputs.numpy(), loss_smooth.numpy(), label=<span class="string">&#x27;Smooth L1 Loss&#x27;</span>)</span><br><span class="line">plt.plot(inputs.numpy(), loss_l1, label=<span class="string">&#x27;L1 loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x_i - y_i&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss value&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.5.2.png"><br>可以看出，对于<code>smoothL1</code>来说，在 0 这个尖端处，过渡更为平滑。</p>
<h3 id="6-目标泊松分布的负对数似然损失"><a href="#6-目标泊松分布的负对数似然损失" class="headerlink" title="6. 目标泊松分布的负对数似然损失"></a>6. 目标泊松分布的负对数似然损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.PoissonNLLLoss(log_input=<span class="literal">True</span>, full=<span class="literal">False</span>, size_average=<span class="literal">None</span>, eps=<span class="number">1e-08</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 泊松分布的负对数似然损失函数<br><strong>主要参数：</strong><br><code>log_input</code>：输入是否为对数形式，决定计算公式。<br><code>full</code>：计算所有 loss，默认为 False。<br><code>eps</code>：修正项，避免 input 为 0 时，log(input) 为 nan 的情况。<br><strong>数学公式：</strong></p>
<ul>
<li>当参数<code>log_input=True</code>：<br>$<br>\operatorname{loss}\left(x_{n}, y_{n}\right)&#x3D;e^{x_{n}}-x_{n} \cdot y_{n}<br>$</li>
<li>当参数<code>log_input=False</code>：<br>  $<br>  \operatorname{loss}\left(x_{n}, y_{n}\right)&#x3D;x_{n}-y_{n} \cdot \log \left(x_{n}+\text { eps }\right)<br>  $</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.PoissonNLLLoss()</span><br><span class="line">log_input = torch.randn(<span class="number">5</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">output = loss(log_input, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;PoissonNLLLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>PoissonNLLLoss损失函数的计算结果为 tensor(0.7358, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="7-KL散度"><a href="#7-KL散度" class="headerlink" title="7. KL散度"></a>7. KL散度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.KLDivLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, log_target=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 <code>none</code>&#x2F;<code>sum</code>&#x2F;<code>mean</code>&#x2F;<code>batchmean</code>。</p>
<pre><code>none：逐个元素计算。
sum：所有元素求和，返回标量。
mean：加权平均，返回标量。
batchmean：batchsize 维度求平均值。
</code></pre>
<p><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582006862/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.05</span>], [<span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.2</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">loss = nn.KLDivLoss()</span><br><span class="line">output = loss(inputs,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;KLDivLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>KLDivLoss损失函数的计算结果为 tensor(-0.3335)
</code></pre>
<h3 id="8-MarginRankingLoss"><a href="#8-MarginRankingLoss" class="headerlink" title="8. MarginRankingLoss"></a>8. MarginRankingLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MarginRankingLoss(margin=<span class="number">0.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算两个向量之间的相似度，用于排序任务。该方法用于计算两组数据之间的差异。<br><strong>主要参数:</strong><br><code>margin</code>：边界值，$x_{1}$ 与$x_{2}$ 之间的差异值。<br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x 1, x 2, y)&#x3D;\max (0,-y *(x 1-x 2)+\operatorname{margin})<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MarginRankingLoss()</span><br><span class="line">input1 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">input2 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>).sign()</span><br><span class="line">output = loss(input1, input2, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MarginRankingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MarginRankingLoss损失函数的计算结果为 tensor(0.7740, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="9-多标签边界损失函数"><a href="#9-多标签边界损失函数" class="headerlink" title="9. 多标签边界损失函数"></a>9. 多标签边界损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiLabelMarginLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对于多标签分类问题计算损失函数。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i j} \frac{\max (0,1-x[y[j]]-x[i])}{x \cdot \operatorname{size}(0)}<br>$<br>$<br>\begin{array}{l}<br>\text { 其中, } i&#x3D;0, \ldots, x \cdot \operatorname{size}(0), j&#x3D;0, \ldots, y \cdot \operatorname{size}(0), \text { 对于所有的 } i \text { 和 } j \text {, 都有 } y[j] \geq 0 \text { 并且 }\<br>i \neq y[j]<br>\end{array}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MultiLabelMarginLoss()</span><br><span class="line">x = torch.FloatTensor([[<span class="number">0.9</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]])</span><br><span class="line"><span class="comment"># for target y, only consider labels 3 and 0, not after label -1</span></span><br><span class="line">y = torch.LongTensor([[<span class="number">3</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]])<span class="comment"># 真实的分类是，第3类和第0类</span></span><br><span class="line">output = loss(x, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MultiLabelMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MultiLabelMarginLoss损失函数的计算结果为 tensor(0.4500)
</code></pre>
<h3 id="10-二分类损失函数"><a href="#10-二分类损失函数" class="headerlink" title="10. 二分类损失函数"></a>10. 二分类损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.SoftMarginLoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)torch.nn.(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算二分类的 logistic 损失。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\sum_{i} \frac{\log (1+\exp (-y[i] \cdot x[i]))}{x \cdot \operatorname{nelement}()}<br>$<br>$<br><br>\text { 其中, } x . \text { nelement() 为输入 } x \text { 中的样本个数。注意这里 } y \text { 也有 } 1 \text { 和 }-1 \text { 两种模式。 }<br><br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]])  <span class="comment"># 两个样本，两个神经元</span></span><br><span class="line">target = torch.tensor([[-<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签</span></span><br><span class="line"></span><br><span class="line">loss_f = nn.SoftMarginLoss()</span><br><span class="line">output = loss_f(inputs, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SoftMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>SoftMarginLoss损失函数的计算结果为 tensor(0.6764)
</code></pre>
<h3 id="11-多分类的折页损失"><a href="#11-多分类的折页损失" class="headerlink" title="11. 多分类的折页损失"></a>11. 多分类的折页损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiMarginLoss(p=<span class="number">1</span>, margin=<span class="number">1.0</span>, weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算多分类的折页损失<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>p：</code>可选 1 或 2。<br><code>weight</code>：各类别的 loss 设置权值。<br><code>margin</code>：边界值<br><strong>计算公式：</strong><br>$<br>\operatorname{loss}(x, y)&#x3D;\frac{\sum_{i} \max (0, \operatorname{margin}-x[y]+x[i])^{p}}{x \cdot \operatorname{size}(0)}<br>$<br>$<br>\begin{array}{l}<br>\text { 其中, } x \in{0, \ldots, x \cdot \operatorname{size}(0)-1}, y \in{0, \ldots, y \cdot \operatorname{size}(0)-1} \text {, 并且对于所有的 } i \text { 和 } j \text {, }\<br>\text { 都有 } 0 \leq y[j] \leq x \cdot \operatorname{size}(0)-1, \text { 以及 } i \neq y[j] \text { 。 }<br>\end{array}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]]) </span><br><span class="line">target = torch.tensor([<span class="number">0</span>, <span class="number">1</span>], dtype=torch.long) </span><br><span class="line"></span><br><span class="line">loss_f = nn.MultiMarginLoss()</span><br><span class="line">output = loss_f(inputs, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MultiMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>MultiMarginLoss损失函数的计算结果为 tensor(0.6000)
</code></pre>
<h3 id="12-三元组损失"><a href="#12-三元组损失" class="headerlink" title="12. 三元组损失"></a>12. 三元组损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">2.0</span>, eps=<span class="number">1e-06</span>, swap=<span class="literal">False</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 计算三元组损失。<br><strong>三元组:</strong> 这是一种数据的存储或者使用格式。&lt;实体1，关系，实体2&gt;。在项目中，也可以表示为&lt; <code>anchor</code>, <code>positive examples</code> , <code>negative examples</code>&gt;<br>在这个损失函数中，我们希望去<code>anchor</code>的距离更接近<code>positive examples</code>，而远离<code>negative examples </code><br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>p：</code>可选 1 或 2。<br><code>margin</code>：边界值<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582222120/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">triplet_loss = nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">2</span>)</span><br><span class="line">anchor = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">positive = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">negative = torch.randn(<span class="number">100</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">output = triplet_loss(anchor, positive, negative)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TripletMarginLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>TripletMarginLoss损失函数的计算结果为 tensor(1.1667, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h3 id="13-HingEmbeddingLoss"><a href="#13-HingEmbeddingLoss" class="headerlink" title="13. HingEmbeddingLoss"></a>13. HingEmbeddingLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.HingeEmbeddingLoss(margin=<span class="number">1.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对输出的embedding结果做Hing损失计算<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>margin</code>：边界值<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582284050/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<p><strong>注意事项：</strong> 输入x应为两个输入之差的绝对值。<br>可以这样理解，让个输出的是正例yn&#x3D;1,那么loss就是x，如果输出的是负例y&#x3D;-1，那么输出的loss就是要做一个比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_f = nn.HingeEmbeddingLoss()</span><br><span class="line">inputs = torch.tensor([[<span class="number">1.</span>, <span class="number">0.8</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line">output = loss_f(inputs,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;HingEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>HingEmbeddingLoss损失函数的计算结果为 tensor(0.7667)
</code></pre>
<h3 id="14-余弦相似度"><a href="#14-余弦相似度" class="headerlink" title="14. 余弦相似度"></a>14. 余弦相似度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CosineEmbeddingLoss(margin=<span class="number">0.0</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 对两个向量做余弦相似度<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>margin</code>：可取值[-1,1] ，推荐为[0,0.5] 。<br><strong>计算公式：</strong></p>
<center> 
<img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665582351089/D2B5CA33BD970F64A6301FA75AE2EB22" width="350">  
</center>

<p>这个损失函数应该是最广为人知的。对于两个向量，做余弦相似度。将余弦相似度作为一个距离的计算方式，如果两个向量的距离近，则损失函数值小，反之亦然。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss_f = nn.CosineEmbeddingLoss()</span><br><span class="line">inputs_1 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>], [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>]])</span><br><span class="line">inputs_2 = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">target = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">output = loss_f(inputs_1,inputs_2,target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;CosineEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)</span><br></pre></td></tr></table></figure>

<pre><code>CosineEmbeddingLoss损失函数的计算结果为 tensor(0.5000)
</code></pre>
<h3 id="15-CTC损失函数"><a href="#15-CTC损失函数" class="headerlink" title="15.CTC损失函数"></a>15.CTC损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CTCLoss(blank=<span class="number">0</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, zero_infinity=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong> 用于解决时序类数据的分类<br>计算连续时间序列和目标序列之间的损失。CTCLoss对输入和目标的可能排列的概率进行求和，产生一个损失值，这个损失值对每个输入节点来说是可分的。输入与目标的对齐方式被假定为 “多对一”，这就限制了目标序列的长度，使其必须是≤输入长度。<br><strong>主要参数:</strong><br><code>reduction</code>：计算模式，可为 none&#x2F;sum&#x2F;mean。<br><code>blank</code>：blank label。<br><code>zero_infinity</code>：无穷大的值或梯度值为 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Target are to be padded</span></span><br><span class="line">T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line">S = <span class="number">30</span>      <span class="comment"># Target sequence length of longest target in batch (padding length)</span></span><br><span class="line">S_min = <span class="number">10</span>  <span class="comment"># Minimum target length, for demonstration purposes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">target = torch.randint(low=<span class="number">1</span>, high=C, size=(N, S), dtype=torch.long)</span><br><span class="line"></span><br><span class="line">input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line">target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span><br><span class="line">ctc_loss = nn.CTCLoss()</span><br><span class="line">loss = ctc_loss(<span class="built_in">input</span>, target, input_lengths, target_lengths)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target are to be un-padded</span></span><br><span class="line">T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line">input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">target_lengths = torch.randint(low=<span class="number">1</span>, high=T, size=(N,), dtype=torch.long)</span><br><span class="line">target = torch.randint(low=<span class="number">1</span>, high=C, size=(<span class="built_in">sum</span>(target_lengths),), dtype=torch.long)</span><br><span class="line">ctc_loss = nn.CTCLoss()</span><br><span class="line">loss = ctc_loss(<span class="built_in">input</span>, target, input_lengths, target_lengths)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;CTCLoss损失函数的计算结果为&#x27;</span>,loss)</span><br></pre></td></tr></table></figure>

<pre><code>CTCLoss损失函数的计算结果为 tensor(16.0885, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h2 id="六、-优化器"><a href="#六、-优化器" class="headerlink" title="六、 优化器"></a>六、 优化器</h2><p>这里使用Adam优化器</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<p>Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面提供了十种优化器。</p>
<ul>
<li>torch.optim.ASGD</li>
<li>torch.optim.Adadelta</li>
<li>torch.optim.Adagrad</li>
<li>torch.optim.Adam</li>
<li>torch.optim.AdamW</li>
<li>torch.optim.Adamax</li>
<li>torch.optim.LBFGS</li>
<li>torch.optim.RMSprop</li>
<li>torch.optim.Rprop</li>
<li>torch.optim.SGD</li>
<li>torch.optim.SparseAdam</li>
</ul>
<p>而以上这些优化算法均继承于<code>Optimizer</code>，下面我们先来看下所有优化器的基类<code>Optimizer</code>。定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, params, defaults</span>):</span>        </span><br><span class="line">        self.defaults = defaults</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">        self.param_groups = []</span><br></pre></td></tr></table></figure>

<p><strong><code>Optimizer</code>有三个属性：</strong></p>
<ul>
<li><code>defaults</code>：存储的是优化器的超参数，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>state</code>：参数的缓存，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;, &#123;<span class="title">tensor</span>(<span class="params">[[ <span class="number">0.3864</span>, -<span class="number">0.0131</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">        [-<span class="number">0.1911</span>, -<span class="number">0.4511</span>]], requires_grad=<span class="literal">True</span></span>):</span> &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>param_groups</code>：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov，例子如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.1022</span>, -<span class="number">1.6890</span>],[-<span class="number">1.5116</span>, -<span class="number">1.7846</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;]</span><br></pre></td></tr></table></figure>

<p><strong><code>Optimizer</code>还有以下的方法：</strong></p>
<ul>
<li><code>zero_grad()</code>：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self, set_to_none: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment">#梯度不为空</span></span><br><span class="line">                <span class="keyword">if</span> set_to_none: </span><br><span class="line">                    p.grad = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> p.grad.grad_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        p.grad.detach_()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        p.grad.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">                    p.grad.zero_()<span class="comment"># 梯度设置为0</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>step()</code>：执行一步梯度更新，参数更新</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, closure</span>):</span> </span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<ul>
<li><code>add_param_group()</code>：添加参数组</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_param_group</span>(<span class="params">self, param_group</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(param_group, <span class="built_in">dict</span>), <span class="string">&quot;param group must be a dict&quot;</span></span><br><span class="line"><span class="comment"># 检查类型是否为tensor</span></span><br><span class="line">    params = param_group[<span class="string">&#x27;params&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(params, torch.Tensor):</span><br><span class="line">        param_group[<span class="string">&#x27;params&#x27;</span>] = [params]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(params, <span class="built_in">set</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;optimizer parameters need to be organized in ordered collections, but &#x27;</span></span><br><span class="line">                        <span class="string">&#x27;the ordering of tensors in sets will change between runs. Please use a list instead.&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        param_group[<span class="string">&#x27;params&#x27;</span>] = <span class="built_in">list</span>(params)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> param_group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(param, torch.Tensor):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">&quot;optimizer can only optimize Tensors, &quot;</span></span><br><span class="line">                            <span class="string">&quot;but one of the params is &quot;</span> + torch.typename(param))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> param.is_leaf:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;can&#x27;t optimize a non-leaf Tensor&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, default <span class="keyword">in</span> self.defaults.items():</span><br><span class="line">        <span class="keyword">if</span> default <span class="keyword">is</span> required <span class="keyword">and</span> name <span class="keyword">not</span> <span class="keyword">in</span> param_group:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;parameter group didn&#x27;t specify a value of required optimization parameter &quot;</span> +</span><br><span class="line">                             name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            param_group.setdefault(name, default)</span><br><span class="line"></span><br><span class="line">    params = param_group[<span class="string">&#x27;params&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(params) != <span class="built_in">len</span>(<span class="built_in">set</span>(params)):</span><br><span class="line">        warnings.warn(<span class="string">&quot;optimizer contains a parameter group with duplicate parameters; &quot;</span></span><br><span class="line">                      <span class="string">&quot;in future, this will cause an error; &quot;</span></span><br><span class="line">                      <span class="string">&quot;see github.com/pytorch/pytorch/issues/40967 for more information&quot;</span>, stacklevel=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 上面好像都在进行一些类的检测，报Warning和Error</span></span><br><span class="line">    param_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        param_set.update(<span class="built_in">set</span>(group[<span class="string">&#x27;params&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> param_set.isdisjoint(<span class="built_in">set</span>(param_group[<span class="string">&#x27;params&#x27;</span>])):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;some parameters appear in more than one parameter group&quot;</span>)</span><br><span class="line"><span class="comment"># 添加参数</span></span><br><span class="line">    self.param_groups.append(param_group)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>load_state_dict()</code> ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_state_dict</span>(<span class="params">self, state_dict</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Loads the optimizer state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        state_dict (dict): optimizer state. Should be an object returned</span></span><br><span class="line"><span class="string">            from a call to :meth:`state_dict`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># deepcopy, to be consistent with module API</span></span><br><span class="line">    state_dict = deepcopy(state_dict)</span><br><span class="line">    <span class="comment"># Validate the state_dict</span></span><br><span class="line">    groups = self.param_groups</span><br><span class="line">    saved_groups = state_dict[<span class="string">&#x27;param_groups&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(groups) != <span class="built_in">len</span>(saved_groups):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;loaded state dict has a different number of &quot;</span></span><br><span class="line">                         <span class="string">&quot;parameter groups&quot;</span>)</span><br><span class="line">    param_lens = (<span class="built_in">len</span>(g[<span class="string">&#x27;params&#x27;</span>]) <span class="keyword">for</span> g <span class="keyword">in</span> groups)</span><br><span class="line">    saved_lens = (<span class="built_in">len</span>(g[<span class="string">&#x27;params&#x27;</span>]) <span class="keyword">for</span> g <span class="keyword">in</span> saved_groups)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">any</span>(p_len != s_len <span class="keyword">for</span> p_len, s_len <span class="keyword">in</span> <span class="built_in">zip</span>(param_lens, saved_lens)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;loaded state dict contains a parameter group &quot;</span></span><br><span class="line">                         <span class="string">&quot;that doesn&#x27;t match the size of optimizer&#x27;s group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the state</span></span><br><span class="line">    id_map = &#123;old_id: p <span class="keyword">for</span> old_id, p <span class="keyword">in</span></span><br><span class="line">              <span class="built_in">zip</span>(chain.from_iterable((g[<span class="string">&#x27;params&#x27;</span>] <span class="keyword">for</span> g <span class="keyword">in</span> saved_groups)),</span><br><span class="line">                  chain.from_iterable((g[<span class="string">&#x27;params&#x27;</span>] <span class="keyword">for</span> g <span class="keyword">in</span> groups)))&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cast</span>(<span class="params">param, value</span>):</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Make a deep copy of value, casting all tensors to device of param.&quot;&quot;&quot;</span></span><br><span class="line">   		.....</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Copy state assigned to params (and cast tensors to appropriate types).</span></span><br><span class="line">    <span class="comment"># State that is not assigned to params is copied as is (needed for</span></span><br><span class="line">    <span class="comment"># backward compatibility).</span></span><br><span class="line">    state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict[<span class="string">&#x27;state&#x27;</span>].items():</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> id_map:</span><br><span class="line">            param = id_map[k]</span><br><span class="line">            state[param] = cast(param, v)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            state[k] = v</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameter groups, setting their &#x27;params&#x27; value</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_group</span>(<span class="params">group, new_group</span>):</span></span><br><span class="line">       ...</span><br><span class="line">    param_groups = [</span><br><span class="line">        update_group(g, ng) <span class="keyword">for</span> g, ng <span class="keyword">in</span> <span class="built_in">zip</span>(groups, saved_groups)]</span><br><span class="line">    self.__setstate__(&#123;<span class="string">&#x27;state&#x27;</span>: state, <span class="string">&#x27;param_groups&#x27;</span>: param_groups&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>state_dict()</code>：获取优化器当前状态信息字典</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Returns the state of the optimizer as a :class:`dict`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It contains two entries:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * state - a dict holding current optimization state. Its content</span></span><br><span class="line"><span class="string">        differs between optimizer classes.</span></span><br><span class="line"><span class="string">    * param_groups - a dict containing all parameter groups</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Save order indices instead of Tensors</span></span><br><span class="line">    param_mappings = &#123;&#125;</span><br><span class="line">    start_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pack_group</span>(<span class="params">group</span>):</span></span><br><span class="line">		......</span><br><span class="line">    param_groups = [pack_group(g) <span class="keyword">for</span> g <span class="keyword">in</span> self.param_groups]</span><br><span class="line">    <span class="comment"># Remap state to use order indices as keys</span></span><br><span class="line">    packed_state = &#123;(param_mappings[<span class="built_in">id</span>(k)] <span class="keyword">if</span> <span class="built_in">isinstance</span>(k, torch.Tensor) <span class="keyword">else</span> k): v</span><br><span class="line">                    <span class="keyword">for</span> k, v <span class="keyword">in</span> self.state.items()&#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;state&#x27;</span>: packed_state,</span><br><span class="line">        <span class="string">&#x27;param_groups&#x27;</span>: param_groups,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置权重，服从正态分布  --&gt; 2 x 2</span></span><br><span class="line">weight = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 设置梯度为全1矩阵  --&gt; 2 x 2</span></span><br><span class="line">weight.grad = torch.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 输出现有的weight和data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The data of weight before step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight before step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 实例化优化器</span></span><br><span class="line">optimizer = torch.optim.SGD([weight], lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># 进行一步操作</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment"># 查看进行一步后的值，梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The data of weight after step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight after step:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 权重清零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="comment"># 检验权重是否为0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad of weight after optimizer.zero_grad():\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.grad))</span><br><span class="line"><span class="comment"># 输出参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;optimizer.params_group is \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br><span class="line"><span class="comment"># 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight in optimizer:&#123;&#125;\nweight in weight:&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;params&#x27;</span>][<span class="number">0</span>]), <span class="built_in">id</span>(weight)))</span><br><span class="line"><span class="comment"># 添加参数：weight2</span></span><br><span class="line">weight2 = torch.randn((<span class="number">3</span>, <span class="number">3</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">optimizer.add_param_group(&#123;<span class="string">&quot;params&quot;</span>: weight2, <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>&#125;)</span><br><span class="line"><span class="comment"># 查看现有的参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;optimizer.param_groups is\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br><span class="line"><span class="comment"># 查看当前状态信息</span></span><br><span class="line">opt_state_dict = optimizer.state_dict()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;state_dict before step:\n&quot;</span>, opt_state_dict)</span><br><span class="line"><span class="comment"># 进行5次step操作</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="comment"># 输出现有状态信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;state_dict after step:\n&quot;</span>, optimizer.state_dict())</span><br><span class="line"><span class="comment"># 保存参数信息</span></span><br><span class="line">torch.save(optimizer.state_dict(),os.path.join(<span class="string">r&quot;D:\pythonProject\Attention_Unet&quot;</span>, <span class="string">&quot;optimizer_state_dict.pkl&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;----------done-----------&quot;</span>)</span><br><span class="line"><span class="comment"># 加载参数信息</span></span><br><span class="line">state_dict = torch.load(<span class="string">r&quot;D:\pythonProject\Attention_Unet\optimizer_state_dict.pkl&quot;</span>) <span class="comment"># 需要修改为你自己的路径</span></span><br><span class="line">optimizer.load_state_dict(state_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;load state_dict successfully\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(state_dict))</span><br><span class="line"><span class="comment"># 输出最后属性信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.defaults))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.state))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.param_groups))</span><br></pre></td></tr></table></figure>

<h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行更新前的数据，梯度</span></span><br><span class="line">The data of weight before step:</span><br><span class="line">tensor([[-<span class="number">0.3077</span>, -<span class="number">0.1808</span>],</span><br><span class="line">        [-<span class="number">0.7462</span>, -<span class="number">1.5556</span>]])</span><br><span class="line">The grad of weight before step:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 进行更新后的数据，梯度</span></span><br><span class="line">The data of weight after step:</span><br><span class="line">tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]])</span><br><span class="line">The grad of weight after step:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 进行梯度清零的梯度</span></span><br><span class="line">The grad of weight after optimizer.zero_grad():</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"><span class="comment"># 输出信息</span></span><br><span class="line">optimizer.params_group <span class="keyword">is</span> </span><br><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 证明了优化器的和weight的储存是在一个地方，Python基于值管理</span></span><br><span class="line">weight <span class="keyword">in</span> optimizer:<span class="number">1841923407424</span></span><br><span class="line">weight <span class="keyword">in</span> weight:<span class="number">1841923407424</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出参数</span></span><br><span class="line">optimizer.param_groups <span class="keyword">is</span></span><br><span class="line">[&#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">0.4077</span>, -<span class="number">0.2808</span>],</span><br><span class="line">        [-<span class="number">0.8462</span>, -<span class="number">1.6556</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;, &#123;<span class="string">&#x27;params&#x27;</span>: [tensor([[ <span class="number">0.4539</span>, -<span class="number">2.1901</span>, -<span class="number">0.6662</span>],</span><br><span class="line">        [ <span class="number">0.6630</span>, -<span class="number">1.5178</span>, -<span class="number">0.8708</span>],</span><br><span class="line">        [-<span class="number">2.0222</span>,  <span class="number">1.4573</span>,  <span class="number">0.8657</span>]], requires_grad=<span class="literal">True</span>)], <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行更新前的参数查看，用state_dict</span></span><br><span class="line">state_dict before step:</span><br><span class="line"> &#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"><span class="comment"># 进行更新后的参数查看，用state_dict</span></span><br><span class="line">state_dict after step:</span><br><span class="line"> &#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储信息完毕</span></span><br><span class="line">----------done-----------</span><br><span class="line"><span class="comment"># 加载参数信息成功</span></span><br><span class="line">load state_dict successfully</span><br><span class="line"><span class="comment"># 加载参数信息</span></span><br><span class="line">&#123;<span class="string">&#x27;state&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;, <span class="string">&#x27;param_groups&#x27;</span>: [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">0</span>]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">1</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># defaults的属性输出</span></span><br><span class="line">&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># state属性输出</span></span><br><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;, &#123;<span class="title">tensor</span>(<span class="params">[[-<span class="number">1.3031</span>, -<span class="number">1.1761</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">        [-<span class="number">1.7415</span>, -<span class="number">2.5510</span>]], requires_grad=<span class="literal">True</span></span>):</span> &#123;<span class="string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="number">0.0052</span>, <span class="number">0.0052</span>],</span><br><span class="line">        [<span class="number">0.0052</span>, <span class="number">0.0052</span>]])&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># param_groups属性输出</span></span><br><span class="line">[&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [tensor([[-<span class="number">1.3031</span>, -<span class="number">1.1761</span>],</span><br><span class="line">        [-<span class="number">1.7415</span>, -<span class="number">2.5510</span>]], requires_grad=<span class="literal">True</span>)]&#125;, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0001</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;params&#x27;</span>: [tensor([[ <span class="number">0.4539</span>, -<span class="number">2.1901</span>, -<span class="number">0.6662</span>],</span><br><span class="line">        [ <span class="number">0.6630</span>, -<span class="number">1.5178</span>, -<span class="number">0.8708</span>],</span><br><span class="line">        [-<span class="number">2.0222</span>,  <span class="number">1.4573</span>,  <span class="number">0.8657</span>]], requires_grad=<span class="literal">True</span>)]&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ol>
<li><p>每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Moddule</span>):</span></span><br><span class="line">    ···</span><br><span class="line">net = Net()</span><br><span class="line">optim = torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class="line">optim.step()</span><br></pre></td></tr></table></figure>
</li>
<li><p>optimizer在一个神经网络的epoch中需要实现下面两个步骤：</p>
<ol>
<li>梯度置零</li>
<li>梯度更新<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-5</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">	...</span><br><span class="line">	optimizer.zero_grad()  <span class="comment">#梯度置零</span></span><br><span class="line">	loss = ...             <span class="comment">#计算loss</span></span><br><span class="line">	loss.backward()        <span class="comment">#BP反向传播</span></span><br><span class="line">	optimizer.step()       <span class="comment">#梯度更新</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>给网络不同的层赋予不同的优化器参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"></span><br><span class="line">net = resnet18()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>:net.fc.parameters()&#125;,<span class="comment">#fc的lr使用默认的1e-5</span></span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>:net.layer4[<span class="number">0</span>].conv1.parameters(),<span class="string">&#x27;lr&#x27;</span>:<span class="number">1e-2</span>&#125;],lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用param_groups查看属性</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>为了更好的了解优化器，对PyTorch中的优化器进行了一个小测试</p>
<p><strong>数据生成</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 升维操作</span></span><br><span class="line">x = torch.unsqueeze(a, dim=<span class="number">1</span>)</span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * torch.normal(torch.zeros(x.size()))</span><br></pre></td></tr></table></figure>

<p><strong>数据分布曲线</strong>：<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.1.png"></p>
<p><strong>网络结构</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line">        self.predict = nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.hidden(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下面这部分是测试图，纵坐标代表Loss，横坐标代表的是Step：<br><img src="https://datawhalechina.github.io/thorough-pytorch/_images/3.6.2.png"><br>在上面的图片上，曲线下降的趋势和对应的steps代表了在这轮数据，模型下的收敛速度</p>
<p><strong>注意:</strong></p>
<p>优化器的选择是需要根据模型进行改变的，不存在绝对的好坏之分，我们需要多进行一些测试。<br>后续会添加SparseAdam，LBFGS这两个优化器的可视化结果</p>
<h2 id="七、-训练与评估"><a href="#七、-训练与评估" class="headerlink" title="七、 训练与评估"></a>七、 训练与评估</h2><p>关注两者的主要区别：<br>模型状态设置<br>是否需要初始化优化器<br>是否需要将loss传回到网络<br>是否需要每步更新optimizer<br>此外，对于测试或验证过程，可以计算分类准确率</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        data, label = data.cuda(), label.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data) <span class="comment"># 前向传播</span></span><br><span class="line">        loss = criterion(output, label)</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 优化器更新权重</span></span><br><span class="line">        train_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    train_loss = train_loss/<span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span>(<span class="params">epoch</span>):</span>       </span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 测试和训练不一样</span></span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    gt_labels = []</span><br><span class="line">    pred_labels = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 不计算梯度</span></span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, label = data.cuda(), label.cuda()</span><br><span class="line">            output = model(data)</span><br><span class="line">            preds = torch.argmax(output, <span class="number">1</span>) <span class="comment"># 得到预测的结果是哪一类</span></span><br><span class="line">            gt_labels.append(label.cpu().data.numpy()) <span class="comment"># 拼接起来</span></span><br><span class="line">            pred_labels.append(preds.cpu().data.numpy())</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            val_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    val_loss = val_loss/<span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)</span><br><span class="line">    acc = np.<span class="built_in">sum</span>(gt_labels==pred_labels)/<span class="built_in">len</span>(pred_labels) <span class="comment"># pre和label相等的次数除上总数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tValidation Loss: &#123;:.6f&#125;, Accuracy: &#123;:6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, val_loss, acc))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    train(epoch)</span><br><span class="line">    val(epoch)</span><br></pre></td></tr></table></figure>
<p><img src="https://uploadfiles.nowcoder.com/images/20221012/799168342_1665568130565/D2B5CA33BD970F64A6301FA75AE2EB22"></p>
<h2 id="八、-可视化"><a href="#八、-可视化" class="headerlink" title="八、 可视化"></a>八、 可视化</h2><p>见后续专题</p>
<h2 id="九、-保存模型"><a href="#九、-保存模型" class="headerlink" title="九、 保存模型"></a>九、 保存模型</h2><p>训练完成后，可以使用torch.save保存模型参数或者整个模型，也可以在训练过程中保存模型:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">save_path = <span class="string">&quot;./FahionModel.pkl&quot;</span></span><br><span class="line">torch.save(model, save_path)</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%89%E7%AB%A0/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html">深入浅出PyTorch</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-10T06:43:54.000Z" title="2022/10/10 下午2:43:54">2022-10-10</time>发表</span><span class="level-item"><time dateTime="2022-10-16T12:31:05.794Z" title="2022/10/16 下午8:31:05">2022-10-16</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Pytorch/">Pytorch</a></span><span class="level-item">15 分钟读完 (大约2297个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/10/pytorch-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">pytorch - 基础知识</a></h1><div class="content"><p>由于张量是对数据的描述，在神经网络中通常将数据以张量的形式表示，如零维张量表示标量，一维张量表示向量，二维表示矩阵，三维张量表示图像，四维表示视频，这章首先介绍张量，然后介绍它的运算，再是核心包autograd自动微分。</p>
<h1 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h1><h2 id="张量的简介"><a href="#张量的简介" class="headerlink" title="张量的简介"></a>张量的简介</h2><p>pytorch的torch.Tensor和Numpy的多维数组非常相似，由于tensor提供GPU的自动求梯度和计算，它更适合深度学习。</p>
<ol>
<li><p>用dtype指定类型创建tensor，detype有tensor.float&#x2F;long等</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(<span class="number">1</span>,dtype=torch.int8)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用指定类型随机初始化</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.IntTensor(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>tensor和numpy array 互相转化，torch.tensor创建的张量是不共享内存的，但torch.from_numpy()和torch.as_tensor()从numpy array创建得到的张量和原数据是共享内存的，修改numpy array会导致对应tensor的改变</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">tensor = torch.tensor(array)</span><br><span class="line">array2tensor = torch.from_numpy(array)</span><br><span class="line">tensor2array = tensor.numpy()</span><br><span class="line"><span class="comment"># 修改array，对应的tensor也会改变</span></span><br><span class="line">array[<span class="number">0</span>,<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(array2tensor)</span><br></pre></td></tr></table></figure></li>
<li><p>从已存在的tensor创建</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">x = x.new_ones(<span class="number">4</span>, <span class="number">3</span>, dtype=torch.double) </span><br><span class="line"><span class="comment"># 创建一个新的全1矩阵tensor，返回的tensor默认具有相同的torch.dtype和torch.device</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># 重置数据类型</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 结果会有一样的size</span></span><br></pre></td></tr></table></figure></li>
<li><p>创建tensor的函数<br>  基础构造 torch.tensor([1,2,3,4])<br>  随机初始化 torch.rand(2,3) [0,1)的均匀分布， torch.randn(2,3) N(0,1)的正态分布，randperm(10) 随机排列<br>  正态分布 torch.normal(2,3) 均值为2标准差为3的正态分布<br>  全1矩阵 torch.ones(2,3)<br>  全0矩阵 torch.zeros(2,3)<br>  对角单位阵 torch.eye(2,3)<br>  有序序列 torch.arange(2,10,2) 从2到10，步长2<br>  均分序列 torch.linspace(2,10,2) 从2到10，均分成2份</p>
</li>
<li><p>查看tensor的维度</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">k = torch.tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(k.shape) </span><br><span class="line"><span class="built_in">print</span>(k.size())</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h2><ol>
<li><p>加法</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">k = torch.rand(<span class="number">2</span>, <span class="number">3</span>) </span><br><span class="line">l = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(k+l)</span><br><span class="line"></span><br><span class="line">torch.add(k,l)</span><br><span class="line"></span><br><span class="line">k.add(l) <span class="comment"># 原值修改</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>索引操作<br>与原数据内存共享，用clone()不会修改</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 取第二列</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>]) </span><br><span class="line"></span><br><span class="line">y = x[<span class="number">0</span>,:]</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :]) <span class="comment"># 源tensor也被改了了</span></span><br><span class="line"></span><br><span class="line">y -= <span class="number">1</span></span><br><span class="line">a = y.clone()</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :])</span><br></pre></td></tr></table></figure>
</li>
<li><p>维度变换</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p>view()共享内存，仅是更改了对张量的观察角度，而reshape()不共享内存，但原始tensor如果不连续，它会返回原值的copy，所以推荐先用clone创建副本再view，用clone能记录到计算图中，梯度回传到副本时也会传到源tensor</p>
<p> 扩展&#x2F;压缩tensor</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">o = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(o)</span><br><span class="line">r = o.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"><span class="built_in">print</span>(r.shape)</span><br><span class="line"></span><br><span class="line">s = r.squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"><span class="built_in">print</span>(s.shape)</span><br></pre></td></tr></table></figure></li>
<li><p>取值操作</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x)) </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x.item()))</span><br></pre></td></tr></table></figure>
<p>其他操作见<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">官方文档</a></p>
</li>
</ol>
<h2 id="张量的广播机制"><a href="#张量的广播机制" class="headerlink" title="张量的广播机制"></a>张量的广播机制</h2><p>两个形状不同的Tensor按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个Tensor形状相同后，再按元素运算。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">p = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line">q = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(q)</span><br><span class="line"><span class="built_in">print</span>(p + q)</span><br></pre></td></tr></table></figure>

<h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><h2 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h2><p>它是torch.tensor的核心类，设置它的属性requires_grad&#x3D;True来追踪张量，完成计算后调用backward()来自动计算所有梯度，导数会自动累积到grad，由链式法则可以计算导数，torch.autograd就是计算雅可比矩阵乘积的。</p>
<ul>
<li>requires_grad<br>如果没有指定的话，默认输入的这个标志是 False。</li>
<li>grad_fn<br>每个张量都有一个grad_fn属性，该属性引用了创建Tensor自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是None)。Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 缺失情况下默认 requires_grad = False</span></span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>下面举个例子说明梯度计算过程</p>
<ol>
<li>创建一个张量并设置requires_grad&#x3D;True用来追踪其计算历史<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">y = x**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line"></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br></pre></td></tr></table></figure></li>
<li>现在开始进行反向传播，因为out是一个标量，因此out.backward()和 out.backward(torch.tensor(1.)) 等价。由于grad在反向传播是累加的，所以在backward之前要清零.grad.data.zero_()。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再来反向传播⼀一次，注意grad是累加的</span></span><br><span class="line">out2 = x.<span class="built_in">sum</span>()</span><br><span class="line">out2.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">out3 = x.<span class="built_in">sum</span>()</span><br><span class="line">x.grad.data.zero_()</span><br><span class="line">out3.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>
雅可比向量积，.data.norm()它对张量y每个元素进行平方，然后对它们求和，最后取平方根，这些操作计算就是所谓的L2或欧几里德范数。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>
当y不再是标量，torch.autograd不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给backward：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>
若不需要计算梯度，阻止autograd跟踪设置.requires_grad&#x3D;True的张量的历史记录<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>
若想修改tensor的数值，又不希望被autograd记录(即不会影响反向传播)， 那么我们可以对tensor.data进行操作<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.data) <span class="comment"># 还是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(x.data.requires_grad) <span class="comment"># 但是已经是独立于计算图之外,返回false</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># 更改data的值也会影响tensor的值,x为100</span></span><br><span class="line"><span class="built_in">print</span>(x.grad) <span class="comment"># 不会受到值改变的影响</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="并行计算简介"><a href="#并行计算简介" class="headerlink" title="并行计算简介"></a>并行计算简介</h1><p>PyTorch可以在编写完模型之后，让多个GPU来参与训练，减少训练时间。CUDA是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是CUDA语言来实现的。而pytorch编写深度学习代码使用的CUDA是表示开始要求我们的模型或者数据开始使用GPU了。当我们使用了.cuda()时，其功能是让我们的模型或者数据从CPU迁移到GPU(0)当中，通过GPU开始计算。</p>
<p>注：数据在GPU和CPU之间进行传递时会比较耗时，我们应当尽量避免数据的切换。GPU运算很快，但是在使用简单的操作时，我们应该尽量使用CPU去完成。当我们的服务器上有多个GPU，我们应该指明我们使用的GPU是哪一块，如果我们不设置的话，tensor.cuda()方法会默认将tensor保存到第一块GPU上，等价于tensor.cuda(0)，这将会导致爆出out of memory的错误。我们可以通过以下两种方式继续设置。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#设置在文件最开始部分</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICE&quot;</span>] = <span class="string">&quot;2&quot;</span> <span class="comment"># 设置默认的显卡</span></span><br><span class="line">CUDA_VISBLE_DEVICE=<span class="number">0</span>,<span class="number">1</span> python train.py <span class="comment"># 使用0，1两块GPU</span></span><br></pre></td></tr></table></figure>

<p>常见的并行方法：</p>
<ul>
<li><p>网络结构分布到不同的设备中(Network partitioning)<br>将一个模型的各个部分拆分，然后将不同的部分放入到GPU来做不同任务的计算。这里遇到的问题就是，不同模型组件在不同的GPU上时，GPU之间的传输就很重要，对于GPU之间的通信是一个考验。但是GPU的通信在这种密集任务中很难办到，所以这个方式慢慢淡出了视野。</p>
</li>
<li><p>同一层的任务分布到不同数据中(Layer-wise partitioning)<br>同一层的模型做一个拆分，让不同的GPU去训练同一层模型的部分任务。这样可以保证在不同组件之间传输的问题，但是在我们需要大量的训练，同步任务加重的情况下，会出现和第一种方式一样的问题。</p>
</li>
<li><p>不同的数据分布到不同的设备中，执行相同的任务(Data parallelism)<br>它的逻辑是，不再拆分模型，训练的时候模型都是一整个模型。但是我将输入的数据拆分。所谓的拆分数据就是，同一个模型在不同GPU中训练一部分数据，然后再分别计算一部分数据之后，只需要将输出的数据做一个汇总，然后再反传，这种方式可以解决之前模式遇到的通讯问题，是现在主流方式。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%8C%E7%AB%A0/index.html">深入浅出PyTorch</a></p>
</li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="🐏"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">🐏</p><p class="is-size-6 is-block">DubistmeinAugenstern</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>WuHan,China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Yang-Emily" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Yang-Emily"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/wuyangemily/"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Youtube" href="https://www.youtube.com/channel/UCSnojGpH9om-xzl-og2_AZQ/featured"><i class="fab fa-youtube"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="twitter" href="https://twitter.com/wuyangemily"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="http://wpa.qq.com/msgrd?v=3&amp;uin=1047772929&amp;site=qq&amp;menu=yes"><i class="fab fa-qq"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://leetcode-cn.com/u/wuyangemily/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Leetcode</span></span><span class="level-right"><span class="level-item tag">leetcode-cn.com</span></span></a></li><li><a class="level is-mobile" href="https://blog.csdn.net/qq_44729001" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">CSDN</span></span><span class="level-right"><span class="level-item tag">blog.csdn.net</span></span></a></li><li><a class="level is-mobile" href="https://www.kaggle.com/wuyangemily" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Kaggle</span></span><span class="level-right"><span class="level-item tag">www.kaggle.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%82/"><span class="level-start"><span class="level-item">杂</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度强化学习</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-23T06:43:54.000Z">2022-10-23</time></p><p class="title"><a href="/2022/10/23/pytorch-%E7%94%9F%E6%80%81%E9%83%A8%E7%BD%B2/">pytorch - 生态和部署</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-22T06:43:54.000Z">2022-10-22</time></p><p class="title"><a href="/2022/10/22/pytorch-%E5%8F%AF%E8%A7%86%E5%8C%96/">pytorch - 可视化</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-20T06:43:54.000Z">2022-10-20</time></p><p class="title"><a href="/2022/10/20/pytorch-%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/">pytorch - 训练技巧</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-16T06:43:54.000Z">2022-10-16</time></p><p class="title"><a href="/2022/10/16/pytorch-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/">pytorch - 模型定义</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-12T06:43:54.000Z">2022-10-12</time></p><p class="title"><a href="/2022/10/12/pytorch-%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E5%92%8C%E5%AE%9E%E8%B7%B5/">pytorch - 各个组件和实践</a></p><p class="categories"><a href="/categories/Pytorch/">Pytorch</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">十月 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">九月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">八月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">三月 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">十月 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DRL/"><span class="tag">DRL</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Draft/"><span class="tag">Draft</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Intelligence-Code/"><span class="tag">Intelligence Code</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">6</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="🐏&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Yang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Yang-Emily/Yang-Emily.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>